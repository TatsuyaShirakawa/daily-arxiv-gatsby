---
title: Hot Papers 2020-10-26
date: 2020-10-27T09:10:20.Z
template: "post"
draft: false
slug: "hot-papers-2020-10-26"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-10-26"
socialImage: "/media/flying-marine.jpg"

---

# 1. A Survey on Recent Approaches for Natural Language Processing in  Low-Resource Scenarios

Michael A. Hedderich, Lukas Lange, Heike Adel, Jannik Str√∂tgen, Dietrich Klakow

- retweets: 198, favorites: 82 (10/27/2020 09:10:20)

- links: [abs](https://arxiv.org/abs/2010.12309) | [pdf](https://arxiv.org/pdf/2010.12309)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Current developments in natural language processing offer challenges and opportunities for low-resource languages and domains. Deep neural networks are known for requiring large amounts of training data which might not be available in resource-lean scenarios. However, there is also a growing body of works to improve the performance in low-resource settings. Motivated by fundamental changes towards neural models and the currently popular pre-train and fine-tune paradigm, we give an overview of promising approaches for low-resource natural language processing. After a discussion about the definition of low-resource scenarios and the different dimensions of data availability, we then examine methods that enable learning when training data is sparse. This includes mechanisms to create additional labeled data like data augmentation and distant supervision as well as transfer learning settings that reduce the need for target supervision. The survey closes with a brief look into methods suggested in non-NLP machine learning communities, which might be beneficial for NLP in low-resource scenarios

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I was looking for something like this: a survey of approaches for NLP in low-resource scenarios. From distant supervision to data augmentation to meta-learning. You will find it useful if you are working with low-resource languages.<br><br>by Hedderich et al.<a href="https://t.co/WIAMKaAdED">https://t.co/WIAMKaAdED</a> <a href="https://t.co/dIl94XQQwx">pic.twitter.com/dIl94XQQwx</a></p>&mdash; elvis (@omarsar0) <a href="https://twitter.com/omarsar0/status/1320700022514733057?ref_src=twsrc%5Etfw">October 26, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Self-Learning Transformations for Improving Gaze and Head Redirection

Yufeng Zheng, Seonwook Park, Xucong Zhang, Shalini De Mello, Otmar Hilliges

- retweets: 144, favorites: 66 (10/27/2020 09:10:20)

- links: [abs](https://arxiv.org/abs/2010.12307) | [pdf](https://arxiv.org/pdf/2010.12307)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Many computer vision tasks rely on labeled data. Rapid progress in generative modeling has led to the ability to synthesize photorealistic images. However, controlling specific aspects of the generation process such that the data can be used for supervision of downstream tasks remains challenging. In this paper we propose a novel generative model for images of faces, that is capable of producing high-quality images under fine-grained control over eye gaze and head orientation angles. This requires the disentangling of many appearance related factors including gaze and head orientation but also lighting, hue etc. We propose a novel architecture which learns to discover, disentangle and encode these extraneous variations in a self-learned manner. We further show that explicitly disentangling task-irrelevant factors results in more accurate modelling of gaze and head orientation. A novel evaluation scheme shows that our method improves upon the state-of-the-art in redirection accuracy and disentanglement between gaze direction and head orientation changes. Furthermore, we show that in the presence of limited amounts of real-world training data, our method allows for improvements in the downstream task of semi-supervised cross-dataset gaze estimation. Please check our project page at: https://ait.ethz.ch/projects/2020/STED-gaze/

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Self-Learning Transformations for Improving Gaze and Head Redirection<br>pdf: <a href="https://t.co/Vn609zh7bQ">https://t.co/Vn609zh7bQ</a><br>abs: <a href="https://t.co/ISrcIFWiKg">https://t.co/ISrcIFWiKg</a><br>project page: <a href="https://t.co/dXF7vlXptP">https://t.co/dXF7vlXptP</a> <a href="https://t.co/uzQPgx09pm">pic.twitter.com/uzQPgx09pm</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1320584070124965888?ref_src=twsrc%5Etfw">October 26, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Lightweight Generative Adversarial Networks for Text-Guided Image  Manipulation

Bowen Li, Xiaojuan Qi, Philip H. S. Torr, Thomas Lukasiewicz

- retweets: 90, favorites: 50 (10/27/2020 09:10:21)

- links: [abs](https://arxiv.org/abs/2010.12136) | [pdf](https://arxiv.org/pdf/2010.12136)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

We propose a novel lightweight generative adversarial network for efficient image manipulation using natural language descriptions. To achieve this, a new word-level discriminator is proposed, which provides the generator with fine-grained training feedback at word-level, to facilitate training a lightweight generator that has a small number of parameters, but can still correctly focus on specific visual attributes of an image, and then edit them without affecting other contents that are not described in the text. Furthermore, thanks to the explicit training signal related to each word, the discriminator can also be simplified to have a lightweight structure. Compared with the state of the art, our method has a much smaller number of parameters, but still achieves a competitive manipulation performance. Extensive experimental results demonstrate that our method can better disentangle different visual attributes, then correctly map them to corresponding semantic words, and thus achieve a more accurate image modification using natural language descriptions.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Lightweight Generative Adversarial Networks for Text-Guided Image Manipulation<br>pdf: <a href="https://t.co/UNp2ywLEYW">https://t.co/UNp2ywLEYW</a><br>abs: <a href="https://t.co/JTHRkCo4VS">https://t.co/JTHRkCo4VS</a> <a href="https://t.co/4KR1C090PM">pic.twitter.com/4KR1C090PM</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1320568462385831939?ref_src=twsrc%5Etfw">October 26, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Dynamics and Domain Randomized Gait Modulation with Bezier Curves for  Sim-to-Real Legged Locomotion

Maurice Rahme, Ian Abraham, Matthew L. Elwin, Todd D. Murphey

- retweets: 42, favorites: 63 (10/27/2020 09:10:21)

- links: [abs](https://arxiv.org/abs/2010.12070) | [pdf](https://arxiv.org/pdf/2010.12070)
- [cs.RO](https://arxiv.org/list/cs.RO/recent)

We present a sim-to-real framework that uses dynamics and domain randomized offline reinforcement learning to enhance open-loop gaits for legged robots, allowing them to traverse uneven terrain without sensing foot impacts. Our approach, D$^2$-Randomized Gait Modulation with Bezier Curves (D$^2$-GMBC), uses augmented random search with randomized dynamics and terrain to train, in simulation, a policy that modifies the parameters and output of an open-loop Bezier curve gait generator for quadrupedal robots. The policy, using only inertial measurements, enables the robot to traverse unknown rough terrain, even when the robot's physical parameters do not match the open-loop model.   We compare the resulting policy to hand-tuned Bezier Curve gaits and to policies trained without randomization, both in simulation and on a real quadrupedal robot. With D$^2$-GMBC, across a variety of experiments on unobserved and unknown uneven terrain, the robot walks significantly farther than with either hand-tuned gaits or gaits learned without domain randomization. Additionally, using D$^2$-GMBC, the robot can walk laterally and rotate while on the rough terrain, even though it was trained only for forward walking.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Dynamics and Domain Randomized Gait Modulation with Bezier Curves for Sim-to-Real Legged Locomotion<a href="https://t.co/jz6epXOsvR">https://t.co/jz6epXOsvR</a> <a href="https://t.co/7pj4KcrIAB">pic.twitter.com/7pj4KcrIAB</a></p>&mdash; sim2real (@sim2realAIorg) <a href="https://twitter.com/sim2realAIorg/status/1320601048961052672?ref_src=twsrc%5Etfw">October 26, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Factor Graph Grammars

David Chiang, Darcey Riley

- retweets: 56, favorites: 45 (10/27/2020 09:10:21)

- links: [abs](https://arxiv.org/abs/2010.12048) | [pdf](https://arxiv.org/pdf/2010.12048)
- [cs.LG](https://arxiv.org/list/cs.LG/recent)

We propose the use of hyperedge replacement graph grammars for factor graphs, or factor graph grammars (FGGs) for short. FGGs generate sets of factor graphs and can describe a more general class of models than plate notation, dynamic graphical models, case-factor diagrams, and sum-product networks can. Moreover, inference can be done on FGGs without enumerating all the generated factor graphs. For finite variable domains (but possibly infinite sets of graphs), a generalization of variable elimination to FGGs allows exact and tractable inference in many situations. For finite sets of graphs (but possibly infinite variable domains), a FGG can be converted to a single factor graph amenable to standard inference techniques.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">This seems like a good time to mention that the preprint of our NeurIPS paper, &quot;Factor Graph Grammars&quot;, is now available!<a href="https://t.co/fKbjsBd2Jx">https://t.co/fKbjsBd2Jx</a></p>&mdash; Darcey Riley (@DarceyNLP) <a href="https://twitter.com/DarceyNLP/status/1320590781221134336?ref_src=twsrc%5Etfw">October 26, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. Noise2Same: Optimizing A Self-Supervised Bound for Image Denoising

Yaochen Xie, Zhengyang Wang, Shuiwang Ji

- retweets: 42, favorites: 45 (10/27/2020 09:10:21)

- links: [abs](https://arxiv.org/abs/2010.11971) | [pdf](https://arxiv.org/pdf/2010.11971)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Self-supervised frameworks that learn denoising models with merely individual noisy images have shown strong capability and promising performance in various image denoising tasks. Existing self-supervised denoising frameworks are mostly built upon the same theoretical foundation, where the denoising models are required to be J-invariant. However, our analyses indicate that the current theory and the J-invariance may lead to denoising models with reduced performance. In this work, we introduce Noise2Same, a novel self-supervised denoising framework. In Noise2Same, a new self-supervised loss is proposed by deriving a self-supervised upper bound of the typical supervised loss. In particular, Noise2Same requires neither J-invariance nor extra information about the noise model and can be used in a wider range of denoising applications. We analyze our proposed Noise2Same both theoretically and experimentally. The experimental results show that our Noise2Same remarkably outperforms previous self-supervised denoising methods in terms of denoising performance and training efficiency. Our code is available at https://github.com/divelab/Noise2Same.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Noise2Same: Optimizing A Self-Supervised Bound for Image Denoising<br>pdf: <a href="https://t.co/LTXbdkzHT6">https://t.co/LTXbdkzHT6</a><br>abs: <a href="https://t.co/NRgrxxAQHQ">https://t.co/NRgrxxAQHQ</a><br>github: <a href="https://t.co/IsKKV4lIoB">https://t.co/IsKKV4lIoB</a> <a href="https://t.co/Y4psaACc9G">pic.twitter.com/Y4psaACc9G</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1320542777407668227?ref_src=twsrc%5Etfw">October 26, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. The nanoPU: Redesigning the CPU-Network Interface to Minimize RPC Tail  Latency

Stephen Ibanez, Alex Mallery, Serhat Arslan, Theo Jepsen, Muhammad Shahbaz, Nick McKeown, Changhoon Kim

- retweets: 42, favorites: 44 (10/27/2020 09:10:21)

- links: [abs](https://arxiv.org/abs/2010.12114) | [pdf](https://arxiv.org/pdf/2010.12114)
- [cs.AR](https://arxiv.org/list/cs.AR/recent) | [cs.NI](https://arxiv.org/list/cs.NI/recent)

The nanoPU is a new networking-optimized CPU designed to minimize tail latency for RPCs. By bypassing the cache and memory hierarchy, the nanoPU directly places arriving messages into the CPU register file. The wire-to-wire latency through the application is just 65ns, about 13x faster than the current state-of-the-art. The nanoPU moves key functions from software to hardware: reliable network transport, congestion control, core selection, and thread scheduling. It also supports a unique feature to bound the tail latency experienced by high-priority applications. Our prototype nanoPU is based on a modified RISC-V CPU; we evaluate its performance using cycle-accurate simulations of 324 cores on AWS FPGAs, including real applications (MICA and chain replication).

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">New on ArXiV today...  The nanoPU: Redesigning the CPU-Network Interface to Minimize RPC Tail Latency: <a href="https://t.co/E1csKpKILo">https://t.co/E1csKpKILo</a></p>&mdash; billions of spooky packets üéÉ (@justinesherry) <a href="https://twitter.com/justinesherry/status/1320719855755108355?ref_src=twsrc%5Etfw">October 26, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. Toward Expressive Singing Voice Correction: On Perceptual Validity of  Evaluation Metrics for Vocal Melody Extraction

Yin-Jyun Luo, Yuen-Jen Lin, Li Su

- retweets: 56, favorites: 23 (10/27/2020 09:10:21)

- links: [abs](https://arxiv.org/abs/2010.12196) | [pdf](https://arxiv.org/pdf/2010.12196)
- [eess.AS](https://arxiv.org/list/eess.AS/recent) | [cs.SD](https://arxiv.org/list/cs.SD/recent)

Singing voice correction (SVC) is an appealing application for amateur singers. Commercial products automate SVC by snapping pitch contours to equal-tempered scales, which could lead to deadpan modifications. Together with the neglect of rhythmic errors, extensive manual corrections are still necessary. In this paper, we present a streamlined system to automate expressive SVC for both pitch and rhythmic errors. Particularly, we extend a previous work by integrating advanced techniques for singing voice separation (SVS) and vocal melody extraction. SVC is achieved by temporally aligning the source-target pair, followed by replacing pitch and rhythm of the source with those of the target. We evaluate the framework by a comparative study for melody extraction which involves both subjective and objective evaluations, whereby we investigate perceptual validity of the standard metrics through the lens of SVC. The results suggest that the high pitch accuracy obtained by the metrics does not signify good perceptual scores.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Toward Expressive Singing Voice Correction: On Perceptual Validity of Evaluation Metrics for Vocal Melody Extraction<br>pdf: <a href="https://t.co/aRZgYkOnsQ">https://t.co/aRZgYkOnsQ</a><br>abs: <a href="https://t.co/HGLJi2xH42">https://t.co/HGLJi2xH42</a><br>project page: <a href="https://t.co/LEFRvxIrCB">https://t.co/LEFRvxIrCB</a> <a href="https://t.co/bKq6J43C2c">pic.twitter.com/bKq6J43C2c</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1320538418049404928?ref_src=twsrc%5Etfw">October 26, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. Language Models are Open Knowledge Graphs

Chenguang Wang, Xiao Liu, Dawn Song

- retweets: 37, favorites: 42 (10/27/2020 09:10:21)

- links: [abs](https://arxiv.org/abs/2010.11967) | [pdf](https://arxiv.org/pdf/2010.11967)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

This paper shows how to construct knowledge graphs (KGs) from pre-trained language models (e.g., BERT, GPT-2/3), without human supervision. Popular KGs (e.g, Wikidata, NELL) are built in either a supervised or semi-supervised manner, requiring humans to create knowledge. Recent deep language models automatically acquire knowledge from large-scale corpora via pre-training. The stored knowledge has enabled the language models to improve downstream NLP tasks, e.g., answering questions, and writing code and articles. In this paper, we propose an unsupervised method to cast the knowledge contained within language models into KGs. We show that KGs are constructed with a single forward pass of the pre-trained language models (without fine-tuning) over the corpora. We demonstrate the quality of the constructed KGs by comparing to two KGs (Wikidata, TAC KBP) created by humans. Our KGs also provide open factual knowledge that is new in the existing KGs. Our code and KGs will be made publicly available.




# 10. Reinforcement Learning with Combinatorial Actions: An Application to  Vehicle Routing

Arthur Delarue, Ross Anderson, Christian Tjandraatmadja

- retweets: 28, favorites: 47 (10/27/2020 09:10:21)

- links: [abs](https://arxiv.org/abs/2010.12001) | [pdf](https://arxiv.org/pdf/2010.12001)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [math.OC](https://arxiv.org/list/math.OC/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Value-function-based methods have long played an important role in reinforcement learning. However, finding the best next action given a value function of arbitrary complexity is nontrivial when the action space is too large for enumeration. We develop a framework for value-function-based deep reinforcement learning with a combinatorial action space, in which the action selection problem is explicitly formulated as a mixed-integer optimization problem. As a motivating example, we present an application of this framework to the capacitated vehicle routing problem (CVRP), a combinatorial optimization problem in which a set of locations must be covered by a single vehicle with limited capacity. On each instance, we model an action as the construction of a single route, and consider a deterministic policy which is improved through a simple policy iteration algorithm. Our approach is competitive with other reinforcement learning methods and achieves an average gap of 1.7% with state-of-the-art OR methods on standard library instances of medium size.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Two for two on <a href="https://twitter.com/NeurIPSConf?ref_src=twsrc%5Etfw">@NeurIPSConf</a> acceptances for tf.opt papers! Congrats everyone! <br><br>Reinforcement learning + MIP for vehicle routing <a href="https://t.co/Teqiygzsir">https://t.co/Teqiygzsir</a> <br><br>and <br><br>Convex relaxations for verification of <a href="https://twitter.com/hashtag/DeepLearning?src=hash&amp;ref_src=twsrc%5Etfw">#DeepLearning</a> networks <a href="https://t.co/1zPjiC8Obr">https://t.co/1zPjiC8Obr</a><a href="https://twitter.com/hashtag/orms?src=hash&amp;ref_src=twsrc%5Etfw">#orms</a> <a href="https://twitter.com/hashtag/GoogleOR?src=hash&amp;ref_src=twsrc%5Etfw">#GoogleOR</a> <a href="https://twitter.com/hashtag/MonarchsOfMIP?src=hash&amp;ref_src=twsrc%5Etfw">#MonarchsOfMIP</a></p>&mdash; Juan Pablo Vielma (@J_P_Vielma) <a href="https://twitter.com/J_P_Vielma/status/1320755653993005056?ref_src=twsrc%5Etfw">October 26, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 11. Show and Speak: Directly Synthesize Spoken Description of Images

Xinsheng Wang, Siyuan Feng, Jihua Zhu, Mark Hasegawa-Johnson, Odette Scharenborg

- retweets: 26, favorites: 27 (10/27/2020 09:10:22)

- links: [abs](https://arxiv.org/abs/2010.12267) | [pdf](https://arxiv.org/pdf/2010.12267)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.CL](https://arxiv.org/list/cs.CL/recent)

This paper proposes a new model, referred to as the show and speak (SAS) model that, for the first time, is able to directly synthesize spoken descriptions of images, bypassing the need for any text or phonemes. The basic structure of SAS is an encoder-decoder architecture that takes an image as input and predicts the spectrogram of speech that describes this image. The final speech audio is obtained from the predicted spectrogram via WaveNet. Extensive experiments on the public benchmark database Flickr8k demonstrate that the proposed SAS is able to synthesize natural spoken descriptions for images, indicating that synthesizing spoken descriptions for images while bypassing text and phonemes is feasible.




# 12. Sequence-to-sequence Singing Voice Synthesis with Perceptual Entropy  Loss

Jiatong Shi, Shuai Guo, Nan Huo, Yuekai Zhang, Qin Jin

- retweets: 30, favorites: 22 (10/27/2020 09:10:22)

- links: [abs](https://arxiv.org/abs/2010.12024) | [pdf](https://arxiv.org/pdf/2010.12024)
- [eess.AS](https://arxiv.org/list/eess.AS/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.SD](https://arxiv.org/list/cs.SD/recent)

The neural network (NN) based singing voice synthesis (SVS) systems require sufficient data to train well. However, due to high data acquisition and annotation cost, we often encounter data limitation problem in building SVS systems. The NN based models are prone to over-fitting due to data scarcity. In this work, we propose a Perceptual Entropy (PE) loss derived from a psycho-acoustic hearing model to regularize the network. With a one-hour open-source singing voice database, we explore the impact of the PE loss on various mainstream sequence-to-sequence models, including the RNN-based model, transformer-based model, and conformer-based model. Our experiments show that the PE loss can mitigate the over-fitting problem and significantly improve the synthesized singing quality reflected in objective and subjective evaluations. Furthermore, incorporating the PE loss in model training is shown to help the F0-contour and high-frequency-band spectrum prediction.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Sequence-to-sequence Singing Voice Synthesis with Perceptual Entropy Loss<br>pdf: <a href="https://t.co/46c6m7bgov">https://t.co/46c6m7bgov</a><br>abs: <a href="https://t.co/kxCNEFwNyI">https://t.co/kxCNEFwNyI</a> <a href="https://t.co/OY3dcOTBJu">pic.twitter.com/OY3dcOTBJu</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1320528670470082560?ref_src=twsrc%5Etfw">October 26, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 13. Adaptive Gradient Quantization for Data-Parallel SGD

Fartash Faghri, Iman Tabrizian, Ilia Markov, Dan Alistarh, Daniel Roy, Ali Ramezani-Kebrya

- retweets: 30, favorites: 21 (10/27/2020 09:10:22)

- links: [abs](https://arxiv.org/abs/2010.12460) | [pdf](https://arxiv.org/pdf/2010.12460)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Many communication-efficient variants of SGD use gradient quantization schemes. These schemes are often heuristic and fixed over the course of training. We empirically observe that the statistics of gradients of deep models change during the training. Motivated by this observation, we introduce two adaptive quantization schemes, ALQ and AMQ. In both schemes, processors update their compression schemes in parallel by efficiently computing sufficient statistics of a parametric distribution. We improve the validation accuracy by almost 2% on CIFAR-10 and 1% on ImageNet in challenging low-cost communication setups. Our adaptive methods are also significantly more robust to the choice of hyperparameters.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/NeurIPS2020?src=hash&amp;ref_src=twsrc%5Etfw">#NeurIPS2020</a> paper &quot;Adaptive Gradient Quantization for Data-Parallel SGD&quot; reducing communication cost by 70% without sacrificing accuracy.<br><br>Paper: <a href="https://t.co/YPo2s4pz1C">https://t.co/YPo2s4pz1C</a><br>Code: <a href="https://t.co/fNgNaeNj0O">https://t.co/fNgNaeNj0O</a><br><br>w/ <a href="https://twitter.com/ITabrizian?ref_src=twsrc%5Etfw">@ITabrizian</a> (equal c.), Ilia Markov, Dan Alistarh, <a href="https://twitter.com/roydanroy?ref_src=twsrc%5Etfw">@roydanroy</a>, <a href="https://twitter.com/aramezanik?ref_src=twsrc%5Etfw">@aramezanik</a> <a href="https://t.co/KugggAYUdc">pic.twitter.com/KugggAYUdc</a></p>&mdash; Fartash Faghri (@FartashFg) <a href="https://twitter.com/FartashFg/status/1320731238681006083?ref_src=twsrc%5Etfw">October 26, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



