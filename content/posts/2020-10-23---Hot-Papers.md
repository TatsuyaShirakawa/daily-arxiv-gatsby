---
title: Hot Papers 2020-10-23
date: 2020-10-24T10:54:39.Z
template: "post"
draft: false
slug: "hot-papers-2020-10-23"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-10-23"
socialImage: "/media/flying-marine.jpg"

---

# 1. Castle in the Sky: Dynamic Sky Replacement and Harmonization in Videos

Zhengxia Zou

- retweets: 10176, favorites: 0 (10/24/2020 10:54:39)

- links: [abs](https://arxiv.org/abs/2010.11800) | [pdf](https://arxiv.org/pdf/2010.11800)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

This paper proposes a vision-based method for video sky replacement and harmonization, which can automatically generate realistic and dramatic sky backgrounds in videos with controllable styles. Different from previous sky editing methods that either focus on static photos or require inertial measurement units integrated in smartphones on shooting videos, our method is purely vision-based, without any requirements on the capturing devices, and can be well applied to either online or offline processing scenarios. Our method runs in real-time and is free of user interactions. We decompose this artistic creation process into a couple of proxy tasks including sky matting, motion estimation, and image blending. Experiments are conducted on videos diversely captured in the wild by handheld smartphones and dash cameras, and show high fidelity and good generalization of our method in both visual quality and lighting/motion dynamics. Our code and animated results are available at \url{https://jiupinjia.github.io/skyar/}.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Castle in the Sky: Dynamic Sky Replacement and Harmonization in Videos<br>pdf: <a href="https://t.co/DZnFyK2V7G">https://t.co/DZnFyK2V7G</a><br>abs: <a href="https://t.co/MqWd8LQz7R">https://t.co/MqWd8LQz7R</a><br>project page: <a href="https://t.co/8ph0k5TjYd">https://t.co/8ph0k5TjYd</a><br>github: <a href="https://t.co/JNYZYJ3bi2">https://t.co/JNYZYJ3bi2</a> <a href="https://t.co/YxjeCs6uc6">pic.twitter.com/YxjeCs6uc6</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1319470965143777280?ref_src=twsrc%5Etfw">October 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. mT5: A massively multilingual pre-trained text-to-text transformer

Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel

- retweets: 8034, favorites: 103 (10/24/2020 10:54:39)

- links: [abs](https://arxiv.org/abs/2010.11934) | [pdf](https://arxiv.org/pdf/2010.11934)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

The recent "Text-to-Text Transfer Transformer" (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We describe the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. All of the code and model checkpoints used in this work are publicly available.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">We are releasing mT5: A massively-multilingual version of T5 that supports over üíØ languages! mT5 was pre-trained on a multilingual version of C4 and achieves SoTA on many cross-lingual NLP tasks.<br><br>üìúPre-print: <a href="https://t.co/lfQd9CeF7b">https://t.co/lfQd9CeF7b</a><br>üíæCode/models: <a href="https://t.co/1MauV6etQY">https://t.co/1MauV6etQY</a> <a href="https://t.co/LUJIFaMxF5">pic.twitter.com/LUJIFaMxF5</a></p>&mdash; Adam Roberts (@ada_rob) <a href="https://twitter.com/ada_rob/status/1319478585191768065?ref_src=twsrc%5Etfw">October 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Learning Invariances in Neural Networks

Gregory Benton, Marc Finzi, Pavel Izmailov, Andrew Gordon Wilson

- retweets: 2768, favorites: 273 (10/24/2020 10:54:39)

- links: [abs](https://arxiv.org/abs/2010.11882) | [pdf](https://arxiv.org/pdf/2010.11882)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Invariances to translations have imbued convolutional neural networks with powerful generalization properties. However, we often do not know a priori what invariances are present in the data, or to what extent a model should be invariant to a given symmetry group. We show how to \emph{learn} invariances and equivariances by parameterizing a distribution over augmentations and optimizing the training loss simultaneously with respect to the network parameters and augmentation parameters. With this simple procedure we can recover the correct set and extent of invariances on image classification, regression, segmentation, and molecular property prediction from a large space of augmentations, on training data alone.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Translation equivariance has imbued CNNs with powerful generalization abilities. Our <a href="https://twitter.com/hashtag/NeurIPS2020?src=hash&amp;ref_src=twsrc%5Etfw">#NeurIPS2020</a> paper shows how to *learn* symmetries -- rotations, translations, scalings, shears -- from training data alone! <a href="https://t.co/ur8sseuGRk">https://t.co/ur8sseuGRk</a><br>w/ <a href="https://twitter.com/g_benton_?ref_src=twsrc%5Etfw">@g_benton_</a>, <a href="https://twitter.com/Pavel_Izmailov?ref_src=twsrc%5Etfw">@Pavel_Izmailov</a>, <a href="https://twitter.com/m_finzi?ref_src=twsrc%5Etfw">@m_finzi</a>. 1/9 <a href="https://t.co/Bf2DGEItX7">pic.twitter.com/Bf2DGEItX7</a></p>&mdash; Andrew Gordon Wilson (@andrewgwils) <a href="https://twitter.com/andrewgwils/status/1319631580982280199?ref_src=twsrc%5Etfw">October 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. An Image is Worth 16x16 Words: Transformers for Image Recognition at  Scale

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby

- retweets: 1606, favorites: 532 (10/24/2020 10:54:40)

- links: [abs](https://arxiv.org/abs/2010.11929) | [pdf](https://arxiv.org/pdf/2010.11929)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.

<blockquote class="twitter-tweet"><p lang="ca" dir="ltr">Vision Transformer&#39;s codes and arxiv papers are now available. <br><br>codes: <a href="https://t.co/563rU7dDXo">https://t.co/563rU7dDXo</a><br>arxiv: <a href="https://t.co/N03Blafnl0">https://t.co/N03Blafnl0</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1319452656893448194?ref_src=twsrc%5Etfw">October 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">„Åä„ÄÅViT„Åç„Åü<br>arXiv: <a href="https://t.co/Uli5bm5rTV">https://t.co/Uli5bm5rTV</a><br>GitHub: <a href="https://t.co/pJmL4SOy6U">https://t.co/pJmL4SOy6U</a></p>&mdash; Kazuyuki Miyazawa (@kzykmyzw) <a href="https://twitter.com/kzykmyzw/status/1319449960434126849?ref_src=twsrc%5Etfw">October 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Check out our new work, &quot;Vision Transformer&quot; for image recognition at scale. So many cool findings... <a href="https://t.co/rvrvSYuCKN">https://t.co/rvrvSYuCKN</a>.<a href="https://t.co/tklTjAJFA5">https://t.co/tklTjAJFA5</a></p>&mdash; Mostafa Dehghani (@m__dehghani) <a href="https://twitter.com/m__dehghani/status/1319619302778175490?ref_src=twsrc%5Etfw">October 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="da" dir="ltr">Vision Transformer models &amp; code. Have fun! üòÉ<a href="https://t.co/Cb9VyCMoy2">https://t.co/Cb9VyCMoy2</a><a href="https://t.co/bRGFT5bBEJ">https://t.co/bRGFT5bBEJ</a></p>&mdash; Neil Houlsby (@neilhoulsby) <a href="https://twitter.com/neilhoulsby/status/1319647511229988866?ref_src=twsrc%5Etfw">October 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Batch Exploration with Examples for Scalable Robotic Reinforcement  Learning

Annie S. Chen, HyunJi Nam, Suraj Nair, Chelsea Finn

- retweets: 1373, favorites: 256 (10/24/2020 10:54:40)

- links: [abs](https://arxiv.org/abs/2010.11917) | [pdf](https://arxiv.org/pdf/2010.11917)
- [cs.RO](https://arxiv.org/list/cs.RO/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Learning from diverse offline datasets is a promising path towards learning general purpose robotic agents. However, a core challenge in this paradigm lies in collecting large amounts of meaningful data, while not depending on a human in the loop for data collection. One way to address this challenge is through task-agnostic exploration, where an agent attempts to explore without a task-specific reward function, and collect data that can be useful for any downstream task. While these approaches have shown some promise in simple domains, they often struggle to explore the relevant regions of the state space in more challenging settings, such as vision based robotic manipulation. This challenge stems from an objective that encourages exploring everything in a potentially vast state space. To mitigate this challenge, we propose to focus exploration on the important parts of the state space using weak human supervision. Concretely, we propose an exploration technique, Batch Exploration with Examples (BEE), that explores relevant regions of the state-space, guided by a modest number of human provided images of important states. These human provided images only need to be collected once at the beginning of data collection and can be collected in a matter of minutes, allowing us to scalably collect diverse datasets, which can then be combined with any batch RL algorithm. We find that BEE is able to tackle challenging vision-based manipulation tasks both in simulation and on a real Franka robot, and observe that compared to task-agnostic and weakly-supervised exploration techniques, it (1) interacts more than twice as often with relevant objects, and (2) improves downstream task performance when used in conjunction with offline RL.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Can robots learn to autonomously explore their environment?<br><br>We introduce Batch Exploration with Examples (BEE)<a href="https://t.co/8zoswdeZzZ">https://t.co/8zoswdeZzZ</a><br>led by <a href="https://twitter.com/anniee268?ref_src=twsrc%5Etfw">@anniee268</a>, Alex Nam, &amp; <a href="https://twitter.com/SurajNair_1?ref_src=twsrc%5Etfw">@SurajNair_1</a><br><br>Threadüëá  (1/8) <a href="https://t.co/VtZ09y0MVN">pic.twitter.com/VtZ09y0MVN</a></p>&mdash; Chelsea Finn (@chelseabfinn) <a href="https://twitter.com/chelseabfinn/status/1319503855730634752?ref_src=twsrc%5Etfw">October 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. CoinDICE: Off-Policy Confidence Interval Estimation

Bo Dai, Ofir Nachum, Yinlam Chow, Lihong Li, Csaba Szepesv√°ri, Dale Schuurmans

- retweets: 600, favorites: 95 (10/24/2020 10:54:41)

- links: [abs](https://arxiv.org/abs/2010.11652) | [pdf](https://arxiv.org/pdf/2010.11652)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

We study high-confidence behavior-agnostic off-policy evaluation in reinforcement learning, where the goal is to estimate a confidence interval on a target policy's value, given only access to a static experience dataset collected by unknown behavior policies. Starting from a function space embedding of the linear program formulation of the $Q$-function, we obtain an optimization problem with generalized estimating equation constraints. By applying the generalized empirical likelihood method to the resulting Lagrangian, we propose CoinDICE, a novel and efficient algorithm for computing confidence intervals. Theoretically, we prove the obtained confidence intervals are valid, in both asymptotic and finite-sample regimes. Empirically, we show in a variety of benchmarks that the confidence interval estimates are tighter and more accurate than existing methods.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">A new and beautiful (and practical!) technique for computing confidence intervals of policy value in RL! <a href="https://t.co/tfJLuLOm5I">https://t.co/tfJLuLOm5I</a> <br>This is a problem that I &amp; collaborators have been thinking about for ~1 year. At the beginning, I didn&#39;t think such a nice result was possible... 1/ <a href="https://t.co/yRO7icB74k">pic.twitter.com/yRO7icB74k</a></p>&mdash; Ofir Nachum (@ofirnachum) <a href="https://twitter.com/ofirnachum/status/1319660806212964353?ref_src=twsrc%5Etfw">October 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. In Search of Robust Measures of Generalization

Gintare Karolina Dziugaite, Alexandre Drouin, Brady Neal, Nitarshan Rajkumar, Ethan Caballero, Linbo Wang, Ioannis Mitliagkas, Daniel M. Roy

- retweets: 484, favorites: 85 (10/24/2020 10:54:41)

- links: [abs](https://arxiv.org/abs/2010.11924) | [pdf](https://arxiv.org/pdf/2010.11924)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

One of the principal scientific challenges in deep learning is explaining generalization, i.e., why the particular way the community now trains networks to achieve small training error also leads to small error on held-out data from the same population. It is widely appreciated that some worst-case theories -- such as those based on the VC dimension of the class of predictors induced by modern neural network architectures -- are unable to explain empirical performance. A large volume of work aims to close this gap, primarily by developing bounds on generalization error, optimization error, and excess risk. When evaluated empirically, however, most of these bounds are numerically vacuous. Focusing on generalization bounds, this work addresses the question of how to evaluate such bounds empirically. Jiang et al. (2020) recently described a large-scale empirical study aimed at uncovering potential causal relationships between bounds/measures and generalization. Building on their study, we highlight where their proposed methods can obscure failures and successes of generalization measures in explaining generalization. We argue that generalization measures should instead be evaluated within the framework of distributional robustness.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/NeurIPS2020?src=hash&amp;ref_src=twsrc%5Etfw">#NeurIPS2020</a> paper &quot;In Search of Robust Measures of Generalization&quot; evaluates robustness of generalization theories. None is robust, nevermind fantastic! üò≠<a href="https://t.co/4lo03vmXFy">https://t.co/4lo03vmXFy</a><br><br>w/ <a href="https://twitter.com/KDziugaite?ref_src=twsrc%5Etfw">@KDziugaite</a> <a href="https://twitter.com/CasualBrady?ref_src=twsrc%5Etfw">@CasualBrady</a> <a href="https://twitter.com/nitarshan?ref_src=twsrc%5Etfw">@nitarshan</a> <a href="https://twitter.com/ethancaballero?ref_src=twsrc%5Etfw">@ethancaballero</a> Linbo Wang <a href="https://twitter.com/bouzoukipunks?ref_src=twsrc%5Etfw">@bouzoukipunks</a> <a href="https://twitter.com/roydanroy?ref_src=twsrc%5Etfw">@roydanroy</a> <a href="https://t.co/vk2odDAJoH">pic.twitter.com/vk2odDAJoH</a></p>&mdash; Alexandre Drouin (@alexandredrouin) <a href="https://twitter.com/alexandredrouin/status/1319657943684567041?ref_src=twsrc%5Etfw">October 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. Parallel Tacotron: Non-Autoregressive and Controllable TTS

Isaac Elias, Heiga Zen, Jonathan Shen, Yu Zhang, Ye Jia, Ron Weiss, Yonghui Wu

- retweets: 184, favorites: 54 (10/24/2020 10:54:41)

- links: [abs](https://arxiv.org/abs/2010.11439) | [pdf](https://arxiv.org/pdf/2010.11439)
- [cs.SD](https://arxiv.org/list/cs.SD/recent) | [eess.AS](https://arxiv.org/list/eess.AS/recent)

Although neural end-to-end text-to-speech models can synthesize highly natural speech, there is still room for improvements to its efficiency and naturalness. This paper proposes a non-autoregressive neural text-to-speech model augmented with a variational autoencoder-based residual encoder. This model, called \emph{Parallel Tacotron}, is highly parallelizable during both training and inference, allowing efficient synthesis on modern parallel hardware. The use of the variational autoencoder relaxes the one-to-many mapping nature of the text-to-speech problem and improves naturalness. To further improve the naturalness, we use lightweight convolutions, which can efficiently capture local contexts, and introduce an iterative spectrogram loss inspired by iterative refinement. Experimental results show that Parallel Tacotron matches a strong autoregressive baseline in subjective evaluations with significantly decreased inference time.

<blockquote class="twitter-tweet"><p lang="fr" dir="ltr">Parallel Tacotron: Non-Autoregressive and Controllable TTS<br>pdf: <a href="https://t.co/ApvuMNk3B3">https://t.co/ApvuMNk3B3</a><br>abs: <a href="https://t.co/40FZNuHeDM">https://t.co/40FZNuHeDM</a> <a href="https://t.co/VFWi65QMJG">pic.twitter.com/VFWi65QMJG</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1319458145819832320?ref_src=twsrc%5Etfw">October 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. Identifying Learning Rules From Neural Network Observables

Aran Nayebi, Sanjana Srivastava, Surya Ganguli, Daniel L.K. Yamins

- retweets: 132, favorites: 47 (10/24/2020 10:54:41)

- links: [abs](https://arxiv.org/abs/2010.11765) | [pdf](https://arxiv.org/pdf/2010.11765)
- [q-bio.NC](https://arxiv.org/list/q-bio.NC/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

The brain modifies its synaptic strengths during learning in order to better adapt to its environment. However, the underlying plasticity rules that govern learning are unknown. Many proposals have been suggested, including Hebbian mechanisms, explicit error backpropagation, and a variety of alternatives. It is an open question as to what specific experimental measurements would need to be made to determine whether any given learning rule is operative in a real biological system. In this work, we take a "virtual experimental" approach to this problem. Simulating idealized neuroscience experiments with artificial neural networks, we generate a large-scale dataset of learning trajectories of aggregate statistics measured in a variety of neural network architectures, loss functions, learning rule hyperparameters, and parameter initializations. We then take a discriminative approach, training linear and simple non-linear classifiers to identify learning rules from features based on these observables. We show that different classes of learning rules can be separated solely on the basis of aggregate statistics of the weights, activations, or instantaneous layer-wise activity changes, and that these results generalize to limited access to the trajectory and held-out architectures and learning curricula. We identify the statistics of each observable that are most relevant for rule identification, finding that statistics from network activities across training are more robust to unit undersampling and measurement noise than those obtained from the synaptic strengths. Our results suggest that activation patterns, available from electrophysiological recordings of post-synaptic activities on the order of several hundred units, frequently measured at wider intervals over the course of learning, may provide a good basis on which to identify learning rules.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">1/ Excited to share our new work on &quot;Identifying Learning Rules From Neural Network Observables&quot;, to appear as a <a href="https://twitter.com/hashtag/NeurIPS2020?src=hash&amp;ref_src=twsrc%5Etfw">#NeurIPS2020</a> Spotlight!<br>w/ <a href="https://twitter.com/sanjana__z?ref_src=twsrc%5Etfw">@sanjana__z</a> <a href="https://twitter.com/SuryaGanguli?ref_src=twsrc%5Etfw">@SuryaGanguli</a> <a href="https://twitter.com/dyamins?ref_src=twsrc%5Etfw">@dyamins</a><br><br>Paper: <a href="https://t.co/MBGBEJQenT">https://t.co/MBGBEJQenT</a><br>Code: <a href="https://t.co/l1JgOEq9JO">https://t.co/l1JgOEq9JO</a><br><br>Summary below üëá</p>&mdash; Aran Nayebi (@aran_nayebi) <a href="https://twitter.com/aran_nayebi/status/1319698795450388482?ref_src=twsrc%5Etfw">October 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 10. NU-GAN: High resolution neural upsampling with GAN

Rithesh Kumar, Kundan Kumar, Vicki Anand, Yoshua Bengio, Aaron Courville

- retweets: 85, favorites: 65 (10/24/2020 10:54:41)

- links: [abs](https://arxiv.org/abs/2010.11362) | [pdf](https://arxiv.org/pdf/2010.11362)
- [cs.SD](https://arxiv.org/list/cs.SD/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [eess.AS](https://arxiv.org/list/eess.AS/recent)

In this paper, we propose NU-GAN, a new method for resampling audio from lower to higher sampling rates (upsampling). Audio upsampling is an important problem since productionizing generative speech technology requires operating at high sampling rates. Such applications use audio at a resolution of 44.1 kHz or 48 kHz, whereas current speech synthesis methods are equipped to handle a maximum of 24 kHz resolution. NU-GAN takes a leap towards solving audio upsampling as a separate component in the text-to-speech (TTS) pipeline by leveraging techniques for audio generation using GANs. ABX preference tests indicate that our NU-GAN resampler is capable of resampling 22 kHz to 44.1 kHz audio that is distinguishable from original audio only 7.4% higher than random chance for single speaker dataset, and 10.8% higher than chance for multi-speaker dataset.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">NU-GAN: High resolution neural upsampling with GAN<br>pdf: <a href="https://t.co/dYSpxFbUfW">https://t.co/dYSpxFbUfW</a><br>abs: <a href="https://t.co/ldP0zcmhjY">https://t.co/ldP0zcmhjY</a><br>project page: <a href="https://t.co/4hKdK2iDYB">https://t.co/4hKdK2iDYB</a> <a href="https://t.co/cM5tC3Z1FO">pic.twitter.com/cM5tC3Z1FO</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1319451765834698755?ref_src=twsrc%5Etfw">October 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 11. Learning Loss for Test-Time Augmentation

Ildoo Kim, Younghoon Kim, Sungwoong Kim

- retweets: 90, favorites: 48 (10/24/2020 10:54:41)

- links: [abs](https://arxiv.org/abs/2010.11422) | [pdf](https://arxiv.org/pdf/2010.11422)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Data augmentation has been actively studied for robust neural networks. Most of the recent data augmentation methods focus on augmenting datasets during the training phase. At the testing phase, simple transformations are still widely used for test-time augmentation. This paper proposes a novel instance-level test-time augmentation that efficiently selects suitable transformations for a test input. Our proposed method involves an auxiliary module to predict the loss of each possible transformation given the input. Then, the transformations having lower predicted losses are applied to the input. The network obtains the results by averaging the prediction results of augmented inputs. Experimental results on several image classification benchmarks show that the proposed instance-aware test-time augmentation improves the model's robustness against various corruptions.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Learning Loss for Test-Time Augmentation (NeurIPS&#39;20)<a href="https://t.co/jWByubE52h">https://t.co/jWByubE52h</a> <a href="https://t.co/JvAhiE6HcD">pic.twitter.com/JvAhiE6HcD</a></p>&mdash; phalanx (@ZFPhalanx) <a href="https://twitter.com/ZFPhalanx/status/1319515877746765824?ref_src=twsrc%5Etfw">October 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 12. ConVEx: Data-Efficient and Few-Shot Slot Labeling

Matthew Henderson, Ivan Vuliƒá

- retweets: 42, favorites: 80 (10/24/2020 10:54:41)

- links: [abs](https://arxiv.org/abs/2010.11791) | [pdf](https://arxiv.org/pdf/2010.11791)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

We propose ConVEx (Conversational Value Extractor), an efficient pretraining and fine-tuning neural approach for slot-labeling dialog tasks. Instead of relying on more general pretraining objectives from prior work (e.g., language modeling, response selection), ConVEx's pretraining objective, a novel pairwise cloze task using Reddit data, is well aligned with its intended usage on sequence labeling tasks. This enables learning domain-specific slot labelers by simply fine-tuning decoding layers of the pretrained general-purpose sequence labeling model, while the majority of the pretrained model's parameters are kept frozen. We report state-of-the-art performance of ConVEx across a range of diverse domains and data sets for dialog slot-labeling, with the largest gains in the most challenging, few-shot setups. We believe that ConVEx's reduced pretraining times (i.e., only 18 hours on 12 GPUs) and cost, along with its efficient fine-tuning and strong performance, promise wider portability and scalability for data-efficient sequence-labeling tasks in general.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">today we are releasing ConVEx, our new slot labeling framework. It achieves a new leap in performance for few-shot slot labeling. The key ingredient is a new pretraining task, pairwise cloze, that allows pre-training all sequence-level layers-<a href="https://t.co/xplofm3VFA">https://t.co/xplofm3VFA</a> <a href="https://t.co/QIy3hJSAUc">pic.twitter.com/QIy3hJSAUc</a></p>&mdash; Matt Henderson (@matthen2) <a href="https://twitter.com/matthen2/status/1319450970892324866?ref_src=twsrc%5Etfw">October 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 13. STAR: A Schema-Guided Dialog Dataset for Transfer Learning

Johannes E. M. Mosig, Shikib Mehri, Thomas Kober

- retweets: 76, favorites: 45 (10/24/2020 10:54:42)

- links: [abs](https://arxiv.org/abs/2010.11853) | [pdf](https://arxiv.org/pdf/2010.11853)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

We present STAR, a schema-guided task-oriented dialog dataset consisting of 127,833 utterances and knowledge base queries across 5,820 task-oriented dialogs in 13 domains that is especially designed to facilitate task and domain transfer learning in task-oriented dialog. Furthermore, we propose a scalable crowd-sourcing paradigm to collect arbitrarily large datasets of the same quality as STAR. Moreover, we introduce novel schema-guided dialog models that use an explicit description of the task(s) to generalize from known to unknown tasks. We demonstrate the effectiveness of these models, particularly for zero-shot generalization across tasks and domains.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">We are excited to share our latest <a href="https://twitter.com/hashtag/NLProc?src=hash&amp;ref_src=twsrc%5Etfw">#NLProc</a> dataset in collaboration with <a href="https://twitter.com/LTIatCMU?ref_src=twsrc%5Etfw">@LTIatCMU</a>! Work by <a href="https://twitter.com/JEM_Mosig?ref_src=twsrc%5Etfw">@JEM_Mosig</a>, <a href="https://twitter.com/shikibmehri?ref_src=twsrc%5Etfw">@shikibmehri</a> &amp; <a href="https://twitter.com/tttthomasssss?ref_src=twsrc%5Etfw">@tttthomasssss</a><br><br>We release ‚≠êÔ∏è STAR ‚≠êÔ∏è a schema-guided dialog dataset.<br><br>Arxiv: <a href="https://t.co/f9nJX6zVpU">https://t.co/f9nJX6zVpU</a><br>Github: <a href="https://t.co/9ruRaSPwLN">https://t.co/9ruRaSPwLN</a></p>&mdash; Rasa (@Rasa_HQ) <a href="https://twitter.com/Rasa_HQ/status/1319650187107848195?ref_src=twsrc%5Etfw">October 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 14. BlendTorch: A Real-Time, Adaptive Domain Randomization Library

Christoph Heindl, Lukas Brunner, Sebastian Zambal, Josef Scharinger

- retweets: 20, favorites: 52 (10/24/2020 10:54:42)

- links: [abs](https://arxiv.org/abs/2010.11696) | [pdf](https://arxiv.org/pdf/2010.11696)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Solving complex computer vision tasks by deep learning techniques relies on large amounts of (supervised) image data, typically unavailable in industrial environments. The lack of training data starts to impede the successful transfer of state-of-the-art methods in computer vision to industrial applications. We introduce BlendTorch, an adaptive Domain Randomization (DR) library, to help creating infinite streams of synthetic training data. BlendTorch generates data by massively randomizing low-fidelity simulations and takes care of distributing artificial training data for model learning in real-time. We show that models trained with BlendTorch repeatedly perform better in an industrial object detection task than those trained on real or photo-realistic datasets.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">BlendTorch: A Real-Time, Adaptive Domain Randomization Library<a href="https://t.co/YznmfcPo2k">https://t.co/YznmfcPo2k</a> <a href="https://t.co/bsjd5X3XOA">pic.twitter.com/bsjd5X3XOA</a></p>&mdash; sim2real (@sim2realAIorg) <a href="https://twitter.com/sim2realAIorg/status/1319456359570497536?ref_src=twsrc%5Etfw">October 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



