---
title: Hot Papers 2020-07-08
date: 2020-07-09T08:45:39.Z
template: "post"
draft: false
slug: "hot-papers-2020-07-08"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-07-08"
socialImage: "/media/42-line-bible.jpg"

---

# 1. Predicting Afrobeats Hit Songs Using Spotify Data

Adewale Adeagbo

- retweets: 76, favorites: 189 (07/09/2020 08:45:39)

- links: [abs](https://arxiv.org/abs/2007.03137) | [pdf](https://arxiv.org/pdf/2007.03137)
- [cs.IR](https://arxiv.org/list/cs.IR/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.SD](https://arxiv.org/list/cs.SD/recent) | [eess.AS](https://arxiv.org/list/eess.AS/recent)

This study approached the Hit Song Science problem with the aim of predicting which songs in the Afrobeats genre will become popular among Spotify listeners. A dataset of 2063 songs was generated through the Spotify Web API, with the provided audio features. Random Forest and Gradient Boosting algorithms proved to be successful with approximately F1 scores of 86%.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">My paper is now on arxiv! <br><br>Predicting Afrobeats hit songs using Spotify data <a href="https://t.co/6C2AWSNJ3H">https://t.co/6C2AWSNJ3H</a> . It is an experiment on Hit Song Science (HSS) for the Afrobeats genre. Because why not? We global now.</p>&mdash; AA (@Adxpillar) <a href="https://twitter.com/Adxpillar/status/1280761011746803713?ref_src=twsrc%5Etfw">July 8, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. The Go Transformer: Natural Language Modeling for Game Play

David Noever, Matthew Ciolino, Josh Kalin

- retweets: 19, favorites: 88 (07/09/2020 08:45:39)

- links: [abs](https://arxiv.org/abs/2007.03500) | [pdf](https://arxiv.org/pdf/2007.03500)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

This work applies natural language modeling to generate plausible strategic moves in the ancient game of Go. We train the Generative Pretrained Transformer (GPT-2) to mimic the style of Go champions as archived in Smart Game Format (SGF), which offers a text description of move sequences. The trained model further generates valid but previously unseen strategies for Go. Because GPT-2 preserves punctuation and spacing, the raw output of the text generator provides inputs to game visualization and creative patterns, such as the Sabaki project's (2020) game engine using auto-replays. Results demonstrate that language modeling can capture both the sequencing format of championship Go games and their strategic formations. Compared to random game boards, the GPT-2 fine-tuning shows efficient opening move sequences favoring corner play over less advantageous center and side play. Game generation as a language modeling task offers novel approaches to more than 40 other board games where historical text annotation provides training data (e.g., Amazons & Connect 4/6).

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">The Go Transformer: Natural Language Modeling for Game Play<br>pdf: <a href="https://t.co/PRZSVi2so2">https://t.co/PRZSVi2so2</a><br>abs: <a href="https://t.co/WDnhNRCFBX">https://t.co/WDnhNRCFBX</a> <a href="https://t.co/X4CgV1KAeW">pic.twitter.com/X4CgV1KAeW</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1280678366337798149?ref_src=twsrc%5Etfw">July 8, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Off-Policy Evaluation via the Regularized Lagrangian

Mengjiao Yang, Ofir Nachum, Bo Dai, Lihong Li, Dale Schuurmans

- retweets: 10, favorites: 50 (07/09/2020 08:45:39)

- links: [abs](https://arxiv.org/abs/2007.03438) | [pdf](https://arxiv.org/pdf/2007.03438)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [math.OC](https://arxiv.org/list/math.OC/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

The recently proposed distribution correction estimation (DICE) family of estimators has advanced the state of the art in off-policy evaluation from behavior-agnostic data. While these estimators all perform some form of stationary distribution correction, they arise from different derivations and objective functions. In this paper, we unify these estimators as regularized Lagrangians of the same linear program. The unification allows us to expand the space of DICE estimators to new alternatives that demonstrate improved performance. More importantly, by analyzing the expanded space of estimators both mathematically and empirically we find that dual solutions offer greater flexibility in navigating the tradeoff between optimization stability and estimation bias, and generally provide superior estimates in practice.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Policy evaluation via duality/Lagrangian methods presents a lot of choices (how to setup the LPs, regularize them, etc). In <a href="https://t.co/Ics1296x4U">https://t.co/Ics1296x4U</a> we examine how these choices affect accuracy of final eval. Lots of insights in this paper, many of which I didn&#39;t expect.... <a href="https://t.co/0HkEeuSEPE">pic.twitter.com/0HkEeuSEPE</a></p>&mdash; Ofir Nachum (@ofirnachum) <a href="https://twitter.com/ofirnachum/status/1280877800065400832?ref_src=twsrc%5Etfw">July 8, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Provably Safe PAC-MDP Exploration Using Analogies

Melrose Roderick, Vaishnavh Nagarajan, J. Zico Kolter

- retweets: 13, favorites: 40 (07/09/2020 08:45:39)

- links: [abs](https://arxiv.org/abs/2007.03574) | [pdf](https://arxiv.org/pdf/2007.03574)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

A key challenge in applying reinforcement learning to safety-critical domains is understanding how to balance exploration (needed to attain good performance on the task) with safety (needed to avoid catastrophic failure). Although a growing line of work in reinforcement learning has investigated this area of "safe exploration," most existing techniques either 1) do not guarantee safety during the actual exploration process; and/or 2) limit the problem to a priori known and/or deterministic transition dynamics with strong smoothness assumptions. Addressing this gap, we propose Analogous Safe-state Exploration (ASE), an algorithm for provably safe exploration in MDPs with unknown, stochastic dynamics. Our method exploits analogies between state-action pairs to safely learn a near-optimal policy in a PAC-MDP sense. Additionally, ASE also guides exploration towards the most task-relevant states, which empirically results in significant improvements in terms of sample efficiency, when compared to existing methods.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Iâ€™m really excited to release our work on a provably safe and optimal reinforcement learning method: Analogous Safe-state Exploration (with <a href="https://twitter.com/_vaishnavh?ref_src=twsrc%5Etfw">@_vaishnavh</a> and <a href="https://twitter.com/zicokolter?ref_src=twsrc%5Etfw">@zicokolter</a>). Paper: <a href="https://t.co/j38yaYxPNF">https://t.co/j38yaYxPNF</a> Code: <a href="https://t.co/yOJcghXCDN">https://t.co/yOJcghXCDN</a></p>&mdash; Melrose Roderick (@roderickmelrose) <a href="https://twitter.com/roderickmelrose/status/1280890353663565824?ref_src=twsrc%5Etfw">July 8, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



