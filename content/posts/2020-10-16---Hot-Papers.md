---
title: Hot Papers 2020-10-16
date: 2020-10-17T09:14:30.Z
template: "post"
draft: false
slug: "hot-papers-2020-10-16"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-10-16"
socialImage: "/media/flying-marine.jpg"

---

# 1. Refinement Types: A Tutorial

Ranjit Jhala, Niki Vazou

- retweets: 1804, favorites: 153 (10/17/2020 09:14:30)

- links: [abs](https://arxiv.org/abs/2010.07763) | [pdf](https://arxiv.org/pdf/2010.07763)
- [cs.PL](https://arxiv.org/list/cs.PL/recent) | [cs.LO](https://arxiv.org/list/cs.LO/recent) | [cs.SE](https://arxiv.org/list/cs.SE/recent)

Refinement types enrich a language's type system with logical predicates that circumscribe the set of values described by the type, thereby providing software developers a tunable knob with which to inform the type system about what invariants and correctness properties should be checked on their code. In this article, we distill the ideas developed in the substantial literature on refinement types into a unified tutorial that explains the key ingredients of modern refinement type systems. In particular, we show how to implement a refinement type checker via a progression of languages that incrementally add features to the language or type system.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I’m excited  (and TBH, exhausted) to report that <a href="https://twitter.com/nikivazou?ref_src=twsrc%5Etfw">@nikivazou</a> and I wrote a “nanopass style” tutorial on how to implement refinement type checkers<br><br>Preprint: <a href="https://t.co/5D7mmgmE0P">https://t.co/5D7mmgmE0P</a><br><br>Code: <a href="https://t.co/I1Y7nezHxR">https://t.co/I1Y7nezHxR</a><br><br>Comments etc most welcome!</p>&mdash; Ranjit &quot;enough!&quot; Jhala (@RanjitJhala) <a href="https://twitter.com/RanjitJhala/status/1316946237128691712?ref_src=twsrc%5Etfw">October 16, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. NeRF++: Analyzing and Improving Neural Radiance Fields

Kai Zhang, Gernot Riegler, Noah Snavely, Vladlen Koltun

- retweets: 1562, favorites: 178 (10/17/2020 09:14:30)

- links: [abs](https://arxiv.org/abs/2010.07492) | [pdf](https://arxiv.org/pdf/2010.07492)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360 capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF fits multi-layer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we first remark on radiance fields and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF's success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360 captures of objects within large-scale, unbounded 3D scenes. Our method improves view synthesis fidelity in this challenging scenario. Code is available at https://github.com/Kai-46/nerfplusplus.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">NeRF++: Analyzing and Improving Neural Radiance Fields<br>pdf: <a href="https://t.co/AQO0q1WvCi">https://t.co/AQO0q1WvCi</a><br>abs: <a href="https://t.co/iN5w7O6CFW">https://t.co/iN5w7O6CFW</a><br>github: <a href="https://t.co/4nUd3hGJ6n">https://t.co/4nUd3hGJ6n</a> <a href="https://t.co/sFe91zVoH6">pic.twitter.com/sFe91zVoH6</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1316901020748959746?ref_src=twsrc%5Etfw">October 16, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and  Goals of Human Trust in AI

Alon Jacovi, Ana Marasović, Tim Miller, Yoav Goldberg

- retweets: 870, favorites: 104 (10/17/2020 09:14:31)

- links: [abs](https://arxiv.org/abs/2010.07487) | [pdf](https://arxiv.org/pdf/2010.07487)
- [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.CY](https://arxiv.org/list/cs.CY/recent)

Trust is a central component of the interaction between people and AI, in that 'incorrect' levels of trust may cause misuse, abuse or disuse of the technology. But what, precisely, is the nature of trust in AI? What are the prerequisites and goals of the cognitive mechanism of trust, and how can we cause these prerequisites and goals, or assess whether they are being satisfied in a given interaction? This work aims to answer these questions. We discuss a model of trust inspired by, but not identical to, sociology's interpersonal trust (i.e., trust between people). This model rests on two key properties of the vulnerability of the user and the ability to anticipate the impact of the AI model's decisions. We incorporate a formalization of 'contractual trust', such that trust between a user and an AI is trust that some implicit or explicit contract will hold, and a formalization of 'trustworthiness' (which detaches from the notion of trustworthiness in sociology), and with it concepts of 'warranted' and 'unwarranted' trust. We then present the possible causes of warranted trust as intrinsic reasoning and extrinsic behavior, and discuss how to design trustworthy AI, how to evaluate whether trust has manifested, and whether it is warranted. Finally, we elucidate the connection between trust and XAI using our formalization.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">&quot;We want to increase the user&#39;s trust in the model,&quot; or &quot;we want a more trustworthy model&quot; - you probably saw this sentiment in many papers. But what exactly does this mean?<br><br>New paper! --&gt; <a href="https://t.co/DBos4T6AL5">https://t.co/DBos4T6AL5</a> <a href="https://twitter.com/trustworthy_ml?ref_src=twsrc%5Etfw">@trustworthy_ml</a> <br><br>With <a href="https://twitter.com/anmarasovic?ref_src=twsrc%5Etfw">@anmarasovic</a>  <a href="https://twitter.com/tmiller_unimelb?ref_src=twsrc%5Etfw">@tmiller_unimelb</a>  <a href="https://twitter.com/yoavgo?ref_src=twsrc%5Etfw">@yoavgo</a> <a href="https://t.co/VUQM8u0H1P">pic.twitter.com/VUQM8u0H1P</a></p>&mdash; Alon Jacovi (@alon_jacovi) <a href="https://twitter.com/alon_jacovi/status/1317097573736501253?ref_src=twsrc%5Etfw">October 16, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. MOTChallenge: A Benchmark for Single-camera Multiple Target Tracking

Patrick Dendorfer, Aljoša Ošep, Anton Milan, Konrad Schindler, Daniel Cremers, Ian Reid, Stefan Roth, Laura Leal-Taixé

- retweets: 368, favorites: 77 (10/17/2020 09:14:31)

- links: [abs](https://arxiv.org/abs/2010.07548) | [pdf](https://arxiv.org/pdf/2010.07548)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Standardized benchmarks have been crucial in pushing the performance of computer vision algorithms, especially since the advent of deep learning. Although leaderboards should not be over-claimed, they often provide the most objective measure of performance and are therefore important guides for research. We present MOTChallenge, a benchmark for single-camera Multiple Object Tracking (MOT) launched in late 2014, to collect existing and new data, and create a framework for the standardized evaluation of multiple object tracking methods. The benchmark is focused on multiple people tracking, since pedestrians are by far the most studied object in the tracking community, with applications ranging from robot navigation to self-driving cars. This paper collects the first three releases of the benchmark: (i) MOT15, along with numerous state-of-the-art results that were submitted in the last years, (ii) MOT16, which contains new challenging videos, and (iii) MOT17, that extends MOT16 sequences with more precise labels and evaluates tracking performance on three different object detectors. The second and third release not only offers a significant increase in the number of labeled boxes but also provide labels for multiple object classes beside pedestrians, as well as the level of visibility for every single object of interest. We finally provide a categorization of state-of-the-art trackers and a broad error analysis. This will help newcomers understand the related work and research trends in the MOT community, and hopefully shred some light into potential future research directions.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">6 years later it has arrived! <a href="https://twitter.com/MOTChallenge?ref_src=twsrc%5Etfw">@MOTChallenge</a> to appear at IJCV, with new analysis of state-of-the-art trackers. If you are new to MOT or your are struggling to keep up with the literature, give it a look! <a href="https://t.co/KoFJBpznHR">https://t.co/KoFJBpznHR</a> <a href="https://twitter.com/PatrickDendorf1?ref_src=twsrc%5Etfw">@PatrickDendorf1</a> <a href="https://twitter.com/AljosaOsep?ref_src=twsrc%5Etfw">@AljosaOsep</a> <a href="https://twitter.com/antonmil?ref_src=twsrc%5Etfw">@antonmil</a> <a href="https://twitter.com/stefanroth?ref_src=twsrc%5Etfw">@stefanroth</a></p>&mdash; Laura Leal-Taixe (@lealtaixe) <a href="https://twitter.com/lealtaixe/status/1317101212601298950?ref_src=twsrc%5Etfw">October 16, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Probabilistic Time Series Forecasting with Structured Shape and Temporal  Diversity

Vincent Le Guen, Nicolas Thome

- retweets: 169, favorites: 45 (10/17/2020 09:14:31)

- links: [abs](https://arxiv.org/abs/2010.07349) | [pdf](https://arxiv.org/pdf/2010.07349)
- [stat.ML](https://arxiv.org/list/stat.ML/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Probabilistic forecasting consists in predicting a distribution of possible future outcomes. In this paper, we address this problem for non-stationary time series, which is very challenging yet crucially important. We introduce the STRIPE model for representing structured diversity based on shape and time features, ensuring both probable predictions while being sharp and accurate. STRIPE is agnostic to the forecasting model, and we equip it with a diversification mechanism relying on determinantal point processes (DPP). We introduce two DPP kernels for modeling diverse trajectories in terms of shape and time, which are both differentiable and proved to be positive semi-definite. To have an explicit control on the diversity structure, we also design an iterative sampling mechanism to disentangle shape and time representations in the latent space. Experiments carried out on synthetic datasets show that STRIPE significantly outperforms baseline methods for representing diversity, while maintaining accuracy of the forecasting model. We also highlight the relevance of the iterative sampling scheme and the importance to use different criteria for measuring quality and diversity. Finally, experiments on real datasets illustrate that STRIPE is able to outperform state-of-the-art probabilistic forecasting approaches in the best sample prediction.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Probabilistic Time Series Forecasting with Structured Shape and Temporal Diversity. <a href="https://t.co/J9inBUDut9">https://t.co/J9inBUDut9</a> <a href="https://t.co/NQggQJ8KMu">pic.twitter.com/NQggQJ8KMu</a></p>&mdash; arxiv (@arxiv_org) <a href="https://twitter.com/arxiv_org/status/1317111490437021696?ref_src=twsrc%5Etfw">October 16, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. Interpretable Machine Learning with an Ensemble of Gradient Boosting  Machines

Andrei V. Konstantinov, Lev V. Utkin

- retweets: 198, favorites: 13 (10/17/2020 09:14:31)

- links: [abs](https://arxiv.org/abs/2010.07388) | [pdf](https://arxiv.org/pdf/2010.07388)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

A method for the local and global interpretation of a black-box model on the basis of the well-known generalized additive models is proposed. It can be viewed as an extension or a modification of the algorithm using the neural additive model. The method is based on using an ensemble of gradient boosting machines (GBMs) such that each GBM is learned on a single feature and produces a shape function of the feature. The ensemble is composed as a weighted sum of separate GBMs resulting a weighted sum of shape functions which form the generalized additive model. GBMs are built in parallel using randomized decision trees of depth 1, which provide a very simple architecture. Weights of GBMs as well as features are computed in each iteration of boosting by using the Lasso method and then updated by means of a specific smoothing procedure. In contrast to the neural additive model, the method provides weights of features in the explicit form, and it is simply trained. A lot of numerical experiments with an algorithm implementing the proposed method on synthetic and real datasets demonstrate its efficiency and properties for local and global interpretation.




# 7. Neograd: gradient descent with an adaptive learning rate

Michael F. Zimmer

- retweets: 130, favorites: 73 (10/17/2020 09:14:31)

- links: [abs](https://arxiv.org/abs/2010.07873) | [pdf](https://arxiv.org/pdf/2010.07873)
- [cs.LG](https://arxiv.org/list/cs.LG/recent)

Since its inception by Cauchy in 1847, the gradient descent algorithm has been without guidance as to how to efficiently set the learning rate. This paper identifies a concept, defines metrics, and introduces algorithms to provide such guidance. The result is a family of algorithms (Neograd) based on a {\em constant $\rho$ ansatz}, where $\rho$ is a metric based on the error of the updates. This allows one to adjust the learning rate at each step, using a formulaic estimate based on $\rho$. It is now no longer necessary to do trial runs beforehand to estimate a single learning rate for an entire optimization run. The additional costs to operate this metric are trivial. One member of this family of algorithms, NeogradM, can quickly reach much lower cost function values than other first order algorithms. Comparisons are made mainly between NeogradM and Adam on an array of test functions and on a neural network model for identifying hand-written digits. The results show great performance improvements with NeogradM.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Hah, what a day! This (Adabelief) is the 2nd new optimizer I discovered today. Next to the freshly uploaded Neograd, which just saw on arXiv earlier today: <a href="https://t.co/8I6x3ILv64">https://t.co/8I6x3ILv64</a> (the saying goes &quot;all good things come in threes&quot; right?). GitHub repo here: <a href="https://t.co/AcIyWx609a">https://t.co/AcIyWx609a</a> <a href="https://t.co/2jZk6PVvLX">https://t.co/2jZk6PVvLX</a></p>&mdash; Sebastian Raschka (@rasbt) <a href="https://twitter.com/rasbt/status/1316957182005489667?ref_src=twsrc%5Etfw">October 16, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed  Gradients

Juntang Zhuang, Tommy Tang, Sekhar Tatikonda, Nicha Dvornek, Yifan Ding, Xenophon Papademetris, James S. Duncan

- retweets: 160, favorites: 33 (10/17/2020 09:14:31)

- links: [abs](https://arxiv.org/abs/2010.07468) | [pdf](https://arxiv.org/pdf/2010.07468)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Most popular optimizers for deep learning can be broadly categorized as adaptive methods (e.g. Adam) and accelerated schemes (e.g. stochastic gradient descent (SGD) with momentum). For many models such as convolutional neural networks (CNNs), adaptive methods typically converge faster but generalize worse compared to SGD; for complex settings such as generative adversarial networks (GANs), adaptive methods are typically the default because of their stability.We propose AdaBelief to simultaneously achieve three goals: fast convergence as in adaptive methods, good generalization as in SGD, and training stability. The intuition for AdaBelief is to adapt the stepsize according to the "belief" in the current gradient direction. Viewing the exponential moving average (EMA) of the noisy gradient as the prediction of the gradient at the next time step, if the observed gradient greatly deviates from the prediction, we distrust the current observation and take a small step; if the observed gradient is close to the prediction, we trust it and take a large step. We validate AdaBelief in extensive experiments, showing that it outperforms other methods with fast convergence and high accuracy on image classification and language modeling. Specifically, on ImageNet, AdaBelief achieves comparable accuracy to SGD. Furthermore, in the training of a GAN on Cifar10, AdaBelief demonstrates high stability and improves the quality of generated samples compared to a well-tuned Adam optimizer. Code is available at https://github.com/juntang-zhuang/Adabelief-Optimizer

<blockquote class="twitter-tweet"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/NeurIPS2020?src=hash&amp;ref_src=twsrc%5Etfw">#NeurIPS2020</a> Spotlight paper. Delighted to share our AdaBelief optimizer, which trains fast as Adam, generalize well as SGD, stable to train GANs.<br>Paper: <a href="https://t.co/xzQPmd4x0V">https://t.co/xzQPmd4x0V</a><br>Project page: <a href="https://t.co/YpWkXcGpb6">https://t.co/YpWkXcGpb6</a><br>Code: <a href="https://t.co/Bcd3ljAqZ4">https://t.co/Bcd3ljAqZ4</a></p>&mdash; Juntang Zhuang (@JuntangZhuang) <a href="https://twitter.com/JuntangZhuang/status/1316931906626355206?ref_src=twsrc%5Etfw">October 16, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. Natural Language Rationales with Full-Stack Visual Reasoning: From  Pixels to Semantic Frames to Commonsense Graphs

Ana Marasović, Chandra Bhagavatula, Jae Sung Park, Ronan Le Bras, Noah A. Smith, Yejin Choi

- retweets: 74, favorites: 40 (10/17/2020 09:14:31)

- links: [abs](https://arxiv.org/abs/2010.07526) | [pdf](https://arxiv.org/pdf/2010.07526)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent)

Natural language rationales could provide intuitive, higher-level explanations that are easily understandable by humans, complementing the more broadly studied lower-level explanations based on gradients or attention weights. We present the first study focused on generating natural language rationales across several complex visual reasoning tasks: visual commonsense reasoning, visual-textual entailment, and visual question answering. The key challenge of accurate rationalization is comprehensive image understanding at all levels: not just their explicit content at the pixel level, but their contextual contents at the semantic and pragmatic levels. We present Rationale^VT Transformer, an integrated model that learns to generate free-text rationales by combining pretrained language models with object recognition, grounded visual semantic frames, and visual commonsense graphs. Our experiments show that the base pretrained language model benefits from visual adaptation and that free-text rationalization is a promising research direction to complement model interpretability for complex visual-textual reasoning tasks.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">📢 New at Findings <a href="https://twitter.com/hashtag/EMNLP2020?src=hash&amp;ref_src=twsrc%5Etfw">#EMNLP2020</a> 📢 <br><br>&quot;Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs&quot;<br><br>w/ <a href="https://twitter.com/_csBhagav?ref_src=twsrc%5Etfw">@_csBhagav</a> <a href="https://twitter.com/jae_sung_park96?ref_src=twsrc%5Etfw">@jae_sung_park96</a> <a href="https://twitter.com/Ronan_LeBras?ref_src=twsrc%5Etfw">@Ronan_LeBras</a> <a href="https://twitter.com/nlpnoah?ref_src=twsrc%5Etfw">@nlpnoah</a> <a href="https://twitter.com/YejinChoinka?ref_src=twsrc%5Etfw">@YejinChoinka</a><br><br>📖 Paper: <a href="https://t.co/TuyQTRPjFV">https://t.co/TuyQTRPjFV</a><br><br>Thread 👇 <a href="https://t.co/01XUgIwBYW">pic.twitter.com/01XUgIwBYW</a></p>&mdash; Ana Marasović (@anmarasovic) <a href="https://twitter.com/anmarasovic/status/1317129255973654528?ref_src=twsrc%5Etfw">October 16, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 10. A Deep Learning Framework for Predicting Digital Asset Price Movement  from Trade-by-trade Data

Qi Zhao

- retweets: 64, favorites: 32 (10/17/2020 09:14:32)

- links: [abs](https://arxiv.org/abs/2010.07404) | [pdf](https://arxiv.org/pdf/2010.07404)
- [q-fin.ST](https://arxiv.org/list/q-fin.ST/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [q-fin.TR](https://arxiv.org/list/q-fin.TR/recent)

This paper presents a deep learning framework based on Long Short-term Memory Network(LSTM) that predicts price movement of cryptocurrencies from trade-by-trade data. The main focus of this study is on predicting short-term price changes in a fixed time horizon from a looking back period. By carefully designing features and detailed searching for best hyper-parameters, the model is trained to achieve high performance on nearly a year of trade-by-trade data. The optimal model delivers stable high performance(over 60% accuracy) on out-of-sample test periods. In a realistic trading simulation setting, the prediction made by the model could be easily monetized. Moreover, this study shows that the LSTM model could extract universal features from trade-by-trade data, as the learned parameters well maintain their high performance on other cryptocurrency instruments that were not included in training data. This study exceeds existing researches in term of the scale and precision of data used, as well as the high prediction accuracy achieved.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">A Deep Learning Framework for Predicting Digital Asset Price Movement from Trade-by-trade Data <a href="https://t.co/zPbgsgRJVV">https://t.co/zPbgsgRJVV</a></p>&mdash; 🗡🕷 (@StunLikes) <a href="https://twitter.com/StunLikes/status/1317089815599357954?ref_src=twsrc%5Etfw">October 16, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 11. Avoiding Side Effects By Considering Future Tasks

Victoria Krakovna, Laurent Orseau, Richard Ngo, Miljan Martic, Shane Legg

- retweets: 25, favorites: 26 (10/17/2020 09:14:32)

- links: [abs](https://arxiv.org/abs/2010.07877) | [pdf](https://arxiv.org/pdf/2010.07877)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

Designing reward functions is difficult: the designer has to specify what to do (what it means to complete the task) as well as what not to do (side effects that should be avoided while completing the task). To alleviate the burden on the reward designer, we propose an algorithm to automatically generate an auxiliary reward function that penalizes side effects. This auxiliary objective rewards the ability to complete possible future tasks, which decreases if the agent causes side effects during the current task. The future task reward can also give the agent an incentive to interfere with events in the environment that make future tasks less achievable, such as irreversible actions by other agents. To avoid this interference incentive, we introduce a baseline policy that represents a default course of action (such as doing nothing), and use it to filter out future tasks that are not achievable by default. We formally define interference incentives and show that the future task approach with a baseline policy avoids these incentives in the deterministic case. Using gridworld environments that test for side effects and interference, we show that our method avoids interference and is more effective for avoiding side effects than the common approach of penalizing irreversible actions.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Our paper &quot;Avoiding Side Effects By Considering Future Tasks&quot; has been accepted to <a href="https://twitter.com/NeurIPSConf?ref_src=twsrc%5Etfw">@NeurIPSConf</a> 2020! This is joint work with <a href="https://twitter.com/LaurentOrseau?ref_src=twsrc%5Etfw">@LaurentOrseau</a> <a href="https://twitter.com/RichardMCNgo?ref_src=twsrc%5Etfw">@RichardMCNgo</a> <a href="https://twitter.com/MiljanMartic?ref_src=twsrc%5Etfw">@MiljanMartic</a> and <a href="https://twitter.com/ShaneLegg?ref_src=twsrc%5Etfw">@ShaneLegg</a>, laying some theoretical groundwork for the side effects problem. <a href="https://t.co/mYOvYDGWy7">https://t.co/mYOvYDGWy7</a></p>&mdash; Victoria Krakovna (@vkrakovna) <a href="https://twitter.com/vkrakovna/status/1316978097326084096?ref_src=twsrc%5Etfw">October 16, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 12. Rainfall-Runoff Prediction at Multiple Timescales with a Single Long  Short-Term Memory Network

Martin Gauch, Frederik Kratzert, Daniel Klotz, Grey Nearing, Jimmy Lin, Sepp Hochreiter

- retweets: 36, favorites: 15 (10/17/2020 09:14:32)

- links: [abs](https://arxiv.org/abs/2010.07921) | [pdf](https://arxiv.org/pdf/2010.07921)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [physics.ao-ph](https://arxiv.org/list/physics.ao-ph/recent)

Long Short-Term Memory Networks (LSTMs) have been applied to daily discharge prediction with remarkable success. Many practical scenarios, however, require predictions at more granular timescales. For instance, accurate prediction of short but extreme flood peaks can make a life-saving difference, yet such peaks may escape the coarse temporal resolution of daily predictions. Naively training an LSTM on hourly data, however, entails very long input sequences that make learning hard and computationally expensive. In this study, we propose two Multi-Timescale LSTM (MTS-LSTM) architectures that jointly predict multiple timescales within one model, as they process long-past inputs at a single temporal resolution and branch out into each individual timescale for more recent input steps. We test these models on 516 basins across the continental United States and benchmark against the US National Water Model. Compared to naive prediction with a distinct LSTM per timescale, the multi-timescale architectures are computationally more efficient with no loss in accuracy. Beyond prediction quality, the multi-timescale LSTM can process different input variables at different timescales, which is especially relevant to operational applications where the lead time of meteorological forcings depends on their temporal resolution.



