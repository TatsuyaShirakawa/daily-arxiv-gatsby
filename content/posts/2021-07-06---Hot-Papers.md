---
title: Hot Papers 2021-07-06
date: 2021-07-07T09:36:55.Z
template: "post"
draft: false
slug: "hot-papers-2021-07-06"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2021-07-06"
socialImage: "/media/flying-marine.jpg"

---

# 1. A topological solution to object segmentation and tracking

Thomas Tsao, Doris Y. Tsao

- retweets: 10656, favorites: 1 (07/07/2021 09:36:55)

- links: [abs](https://arxiv.org/abs/2107.02036) | [pdf](https://arxiv.org/pdf/2107.02036)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

The world is composed of objects, the ground, and the sky. Visual perception of objects requires solving two fundamental challenges: segmenting visual input into discrete units, and tracking identities of these units despite appearance changes due to object deformation, changing perspective, and dynamic occlusion. Current computer vision approaches to segmentation and tracking that approach human performance all require learning, raising the question: can objects be segmented and tracked without learning? Here, we show that the mathematical structure of light rays reflected from environment surfaces yields a natural representation of persistent surfaces, and this surface representation provides a solution to both the segmentation and tracking problems. We describe how to generate this surface representation from continuous visual input, and demonstrate that our approach can segment and invariantly track objects in cluttered synthetic video despite severe appearance changes, without requiring learning.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">How does perception of objects arise? Objects undergo huge changes in appearance due to deformation, perspective change, &amp; dynamic occlusion. We prove from first principles that it‚Äôs possible, without learning, to perceive invariant objects despite this. <a href="https://t.co/oTWSUmuzbk">https://t.co/oTWSUmuzbk</a></p>&mdash; Doris Tsao (@doristsao) <a href="https://twitter.com/doristsao/status/1412214077988868110?ref_src=twsrc%5Etfw">July 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. DeepRapper: Neural Rap Generation with Rhyme and Rhythm Modeling

Lanqing Xue, Kaitao Song, Duocai Wu, Xu Tan, Nevin L. Zhang, Tao Qin, Wei-Qiang Zhang, Tie-Yan Liu

- retweets: 1488, favorites: 153 (07/07/2021 09:36:55)

- links: [abs](https://arxiv.org/abs/2107.01875) | [pdf](https://arxiv.org/pdf/2107.01875)
- [cs.SD](https://arxiv.org/list/cs.SD/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [eess.AS](https://arxiv.org/list/eess.AS/recent)

Rap generation, which aims to produce lyrics and corresponding singing beats, needs to model both rhymes and rhythms. Previous works for rap generation focused on rhyming lyrics but ignored rhythmic beats, which are important for rap performance. In this paper, we develop DeepRapper, a Transformer-based rap generation system that can model both rhymes and rhythms. Since there is no available rap dataset with rhythmic beats, we develop a data mining pipeline to collect a large-scale rap dataset, which includes a large number of rap songs with aligned lyrics and rhythmic beats. Second, we design a Transformer-based autoregressive language model which carefully models rhymes and rhythms. Specifically, we generate lyrics in the reverse order with rhyme representation and constraint for rhyme enhancement and insert a beat symbol into lyrics for rhythm/beat modeling. To our knowledge, DeepRapper is the first system to generate rap with both rhymes and rhythms. Both objective and subjective evaluations demonstrate that DeepRapper generates creative and high-quality raps with rhymes and rhythms. Code will be released on GitHub.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">DeepRapper: Neural Rap Generation with Rhyme and Rhythm Modeling<br>pdf: <a href="https://t.co/IjrgGeDA4h">https://t.co/IjrgGeDA4h</a><br>abs: <a href="https://t.co/7mANmHJBJk">https://t.co/7mANmHJBJk</a><br>project page: <a href="https://t.co/gIQiFiVSgK">https://t.co/gIQiFiVSgK</a><br>Transformer-based rap generation system that can model both rhymes and rhythms <a href="https://t.co/iFwmdZ6zjZ">pic.twitter.com/iFwmdZ6zjZ</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1412289752351625217?ref_src=twsrc%5Etfw">July 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Automating Generative Deep Learning for Artistic Purposes: Challenges  and Opportunities

Sebastian Berns, Terence Broad, Christian Guckelsberger, Simon Colton

- retweets: 1460, favorites: 114 (07/07/2021 09:36:55)

- links: [abs](https://arxiv.org/abs/2107.01858) | [pdf](https://arxiv.org/pdf/2107.01858)
- [cs.LG](https://arxiv.org/list/cs.LG/recent)

We present a framework for automating generative deep learning with a specific focus on artistic applications. The framework provides opportunities to hand over creative responsibilities to a generative system as targets for automation. For the definition of targets, we adopt core concepts from automated machine learning and an analysis of generative deep learning pipelines, both in standard and artistic settings. To motivate the framework, we argue that automation aligns well with the goal of increasing the creative responsibility of a generative system, a central theme in computational creativity research. We understand automation as the challenge of granting a generative system more creative autonomy, by framing the interaction between the user and the system as a co-creative process. The development of the framework is informed by our analysis of the relationship between automation and creative autonomy. An illustrative example shows how the framework can give inspiration and guidance in the process of handing over creative responsibility.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">üìò Automating Generative Deep Learning for Artistic Purposes: Challenges and Opportunities<br><br>This work makes the case for automation and its role in allowing generative systems more creative autonomy.<br><br>Interesting lessons emerging from ML for creativity.<a href="https://t.co/KWAbzJnWBA">https://t.co/KWAbzJnWBA</a> <a href="https://t.co/0SV129rKcd">pic.twitter.com/0SV129rKcd</a></p>&mdash; elvis (@omarsar0) <a href="https://twitter.com/omarsar0/status/1412366770132996102?ref_src=twsrc%5Etfw">July 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Automating Generative Deep Learning. <a href="https://twitter.com/hashtag/BigData?src=hash&amp;ref_src=twsrc%5Etfw">#BigData</a> <a href="https://twitter.com/hashtag/Analytics?src=hash&amp;ref_src=twsrc%5Etfw">#Analytics</a> <a href="https://twitter.com/hashtag/DataScience?src=hash&amp;ref_src=twsrc%5Etfw">#DataScience</a> <a href="https://twitter.com/hashtag/AI?src=hash&amp;ref_src=twsrc%5Etfw">#AI</a> <a href="https://twitter.com/hashtag/MachineLearning?src=hash&amp;ref_src=twsrc%5Etfw">#MachineLearning</a> <a href="https://twitter.com/hashtag/IoT?src=hash&amp;ref_src=twsrc%5Etfw">#IoT</a> <a href="https://twitter.com/hashtag/IIoT?src=hash&amp;ref_src=twsrc%5Etfw">#IIoT</a> <a href="https://twitter.com/hashtag/PyTorch?src=hash&amp;ref_src=twsrc%5Etfw">#PyTorch</a> <a href="https://twitter.com/hashtag/Python?src=hash&amp;ref_src=twsrc%5Etfw">#Python</a> <a href="https://twitter.com/hashtag/RStats?src=hash&amp;ref_src=twsrc%5Etfw">#RStats</a> <a href="https://twitter.com/hashtag/TensorFlow?src=hash&amp;ref_src=twsrc%5Etfw">#TensorFlow</a> <a href="https://twitter.com/hashtag/Java?src=hash&amp;ref_src=twsrc%5Etfw">#Java</a> <a href="https://twitter.com/hashtag/JavaScript?src=hash&amp;ref_src=twsrc%5Etfw">#JavaScript</a> <a href="https://twitter.com/hashtag/ReactJS?src=hash&amp;ref_src=twsrc%5Etfw">#ReactJS</a> <a href="https://twitter.com/hashtag/CloudComputing?src=hash&amp;ref_src=twsrc%5Etfw">#CloudComputing</a> <a href="https://twitter.com/hashtag/Serverless?src=hash&amp;ref_src=twsrc%5Etfw">#Serverless</a> <a href="https://twitter.com/hashtag/DataScientist?src=hash&amp;ref_src=twsrc%5Etfw">#DataScientist</a> <a href="https://twitter.com/hashtag/Linux?src=hash&amp;ref_src=twsrc%5Etfw">#Linux</a> <a href="https://twitter.com/hashtag/Programming?src=hash&amp;ref_src=twsrc%5Etfw">#Programming</a> <a href="https://twitter.com/hashtag/Coding?src=hash&amp;ref_src=twsrc%5Etfw">#Coding</a> <a href="https://twitter.com/hashtag/100DaysofCode?src=hash&amp;ref_src=twsrc%5Etfw">#100DaysofCode</a> <a href="https://t.co/LldNaUah8v">https://t.co/LldNaUah8v</a> <a href="https://t.co/QpEnWqY4Ke">pic.twitter.com/QpEnWqY4Ke</a></p>&mdash; Dr. Ganapathi Pulipaka üá∫üá∏ (@gp_pulipaka) <a href="https://twitter.com/gp_pulipaka/status/1412498343851397133?ref_src=twsrc%5Etfw">July 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Mava: a research framework for distributed multi-agent reinforcement  learning

Arnu Pretorius, Kale-ab Tessera, Andries P. Smit, Claude Formanek, St John Grimbly, Kevin Eloff, Siphelele Danisa, Lawrence Francis, Jonathan Shock, Herman Kamper, Willie Brink, Herman Engelbrecht, Alexandre Laterre, Karim Beguir

- retweets: 930, favorites: 97 (07/07/2021 09:36:55)

- links: [abs](https://arxiv.org/abs/2107.01460) | [pdf](https://arxiv.org/pdf/2107.01460)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.MA](https://arxiv.org/list/cs.MA/recent)

Breakthrough advances in reinforcement learning (RL) research have led to a surge in the development and application of RL. To support the field and its rapid growth, several frameworks have emerged that aim to help the community more easily build effective and scalable agents. However, very few of these frameworks exclusively support multi-agent RL (MARL), an increasingly active field in itself, concerned with decentralised decision-making problems. In this work, we attempt to fill this gap by presenting Mava: a research framework specifically designed for building scalable MARL systems. Mava provides useful components, abstractions, utilities and tools for MARL and allows for simple scaling for multi-process system training and execution, while providing a high level of flexibility and composability. Mava is built on top of DeepMind's Acme \citep{hoffman2020acme}, and therefore integrates with, and greatly benefits from, a wide range of already existing single-agent RL components made available in Acme. Several MARL baseline systems have already been implemented in Mava. These implementations serve as examples showcasing Mava's reusable features, such as interchangeable system architectures, communication and mixing modules. Furthermore, these implementations allow existing MARL algorithms to be easily reproduced and extended. We provide experimental results for these implementations on a wide range of multi-agent environments and highlight the benefits of distributed system training.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">üßµ1/6 Super excited to launch Mava: a scalable, research framework for multi-agent reinforcement learning (MARL)! ü§ñü•≥<br><br>Humbled to be part of one of the first Deep RL/MARL frameworks built and led by an African teamüåç<br>üìú: <a href="https://t.co/CTb03zGENi">https://t.co/CTb03zGENi</a><br>‚å®Ô∏è: <a href="https://t.co/UT5E8nHz7M">https://t.co/UT5E8nHz7M</a> <a href="https://t.co/ReDYcEBnwZ">pic.twitter.com/ReDYcEBnwZ</a></p>&mdash; Kale-ab Tessera (@KaliTessera) <a href="https://twitter.com/KaliTessera/status/1412402537567764491?ref_src=twsrc%5Etfw">July 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Solving Machine Learning Problems

Sunny Tran, Pranav Krishna, Ishan Pakuwal, Prabhakar Kafle, Nikhil Singh, Jayson Lynch, Iddo Drori

- retweets: 839, favorites: 126 (07/07/2021 09:36:56)

- links: [abs](https://arxiv.org/abs/2107.01238) | [pdf](https://arxiv.org/pdf/2107.01238)
- [cs.LG](https://arxiv.org/list/cs.LG/recent)

Can a machine learn Machine Learning? This work trains a machine learning model to solve machine learning problems from a University undergraduate level course. We generate a new training set of questions and answers consisting of course exercises, homework, and quiz questions from MIT's 6.036 Introduction to Machine Learning course and train a machine learning model to answer these questions. Our system demonstrates an overall accuracy of 96% for open-response questions and 97% for multiple-choice questions, compared with MIT students' average of 93%, achieving grade A performance in the course, all in real-time. Questions cover all 12 topics taught in the course, excluding coding questions or questions with images. Topics include: (i) basic machine learning principles; (ii) perceptrons; (iii) feature extraction and selection; (iv) logistic regression; (v) regression; (vi) neural networks; (vii) advanced neural networks; (viii) convolutional neural networks; (ix) recurrent neural networks; (x) state machines and MDPs; (xi) reinforcement learning; and (xii) decision trees. Our system uses Transformer models within an encoder-decoder architecture with graph and tree representations. An important aspect of our approach is a data-augmentation scheme for generating new example problems. We also train a machine learning model to generate problem hints. Thus, our system automatically generates new questions across topics, answers both open-response questions and multiple-choice questions, classifies problems, and generates problem hints, pushing the envelope of AI for STEM education.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Solving Machine Learning Problems<br>pdf: <a href="https://t.co/jf7EeapP4Z">https://t.co/jf7EeapP4Z</a><br>abs: <a href="https://t.co/uv1JlDLw5M">https://t.co/uv1JlDLw5M</a><br><br>accuracy of 96% for open-response questions and 97% for mcq, compared with MIT students‚Äô average of 93%, achieving grade A performance in the course, all in real-time <a href="https://t.co/NwF3SgMsTv">pic.twitter.com/NwF3SgMsTv</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1412224788362416146?ref_src=twsrc%5Etfw">July 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language  Understanding and Generation

Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen, Yanbin Zhao, Yuxiang Lu, Weixin Liu, Zhihua Wu, Weibao Gong, Jianzhong Liang, Zhizhou Shang, Peng Sun, Wei Liu, Xuan Ouyang, Dianhai Yu, Hao Tian, Hua Wu, Haifeng Wang

- retweets: 467, favorites: 149 (07/07/2021 09:36:56)

- links: [abs](https://arxiv.org/abs/2107.02137) | [pdf](https://arxiv.org/pdf/2107.02137)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

Pre-trained models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. Recent works such as T5 and GPT-3 have shown that scaling up pre-trained language models can improve their generalization abilities. Particularly, the GPT-3 model with 175 billion parameters shows its strong task-agnostic zero-shot/few-shot learning capabilities. Despite their success, these large-scale models are trained on plain texts without introducing knowledge such as linguistic knowledge and world knowledge. In addition, most large-scale models are trained in an auto-regressive way. As a result, this kind of traditional fine-tuning approach demonstrates relatively weak performance when solving downstream language understanding tasks. In order to solve the above problems, we propose a unified framework named ERNIE 3.0 for pre-training large-scale knowledge enhanced models. It fuses auto-regressive network and auto-encoding network, so that the trained model can be easily tailored for both natural language understanding and generation tasks with zero-shot learning, few-shot learning or fine-tuning. We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph. Empirical results show that the model outperforms the state-of-the-art models on 54 Chinese NLP tasks, and its English version achieves the first place on the SuperGLUE benchmark (July 3, 2021), surpassing the human performance by +0.8% (90.6% vs. 89.8%).

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation<br>pdf: <a href="https://t.co/txNBuGFtGa">https://t.co/txNBuGFtGa</a><br>abs: <a href="https://t.co/BQIs7uRq4o">https://t.co/BQIs7uRq4o</a><br><br>trained the model with 10B parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph <a href="https://t.co/ftFY8Y7fcr">pic.twitter.com/ftFY8Y7fcr</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1412214365412024321?ref_src=twsrc%5Etfw">July 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">Baidu„ÅÆAIÁ†îÁ©∂ÔºàERNIE 3.0Ôºâ<br><br>54ÂÄã„ÅÆ‰∏≠ÂõΩË™û„Çø„Çπ„ÇØ„ÅßÂæìÊù•„ÅÆÊúÄÂÖàÁ´Ø„É¢„Éá„É´„ÇíÂáåÈßï„ÄÇËã±Ë™ûÁâà„ÅØ„ÄÅË®ÄË™ûÁêÜËß£„Å´Èñ¢„Åô„Çã8„Å§„ÅÆ„Çø„Çπ„ÇØ„ÇíÈõÜ„ÇÅ„Åü„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÄåSuperGLUE„Äç„Å´„Åä„ÅÑ„Å¶„ÄÅ‰∫∫Èñì„ÅÆÊÄßËÉΩ„Çí+0.8%‰∏äÂõû„Çä„ÄÅ1‰Ωç„ÇíÈÅîÊàêÔºà90.6% vs. 89.8%Ôºâ„ÄÇ4TB„ÅÆ„Ç≥„Éº„Éë„Çπ„Åß100ÂÑÑ„Éë„É©„É°„Éº„Çø„ÅÆ„É¢„Éá„É´„ÇíÂ≠¶Áøí<a href="https://t.co/5UyvINjOum">https://t.co/5UyvINjOum</a></p>&mdash; Â∞èÁå´ÈÅä„Çä„Çá„ÅÜÔºà„Åü„Åã„Å´„ÇÉ„Åó„Éª„Çä„Çá„ÅÜÔºâ (@jaguring1) <a href="https://twitter.com/jaguring1/status/1412305454819024897?ref_src=twsrc%5Etfw">July 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. Dealing with Adversarial Player Strategies in the Neural Network Game  iNNk through Ensemble Learning

Mathias L√∂we, Jennifer Villareale, Evan Freed, Aleksanteri Sladek, Jichen Zhu, Sebastian Risi

- retweets: 240, favorites: 55 (07/07/2021 09:36:56)

- links: [abs](https://arxiv.org/abs/2107.02052) | [pdf](https://arxiv.org/pdf/2107.02052)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

Applying neural network (NN) methods in games can lead to various new and exciting game dynamics not previously possible. However, they also lead to new challenges such as the lack of large, clean datasets, varying player skill levels, and changing gameplay strategies. In this paper, we focus on the adversarial player strategy aspect in the game iNNk, in which players try to communicate secret code words through drawings with the goal of not being deciphered by a NN. Some strategies exploit weaknesses in the NN that consistently trick it into making incorrect classifications, leading to unbalanced gameplay. We present a method that combines transfer learning and ensemble methods to obtain a data-efficient adaptation to these strategies. This combination significantly outperforms the baseline NN across all adversarial player strategies despite only being trained on a limited set of adversarial examples. We expect the methods developed in this paper to be useful for the rapidly growing field of NN-based games, which will require new approaches to deal with unforeseen player creativity.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Can humans outsmart machines to communicate a secret codeword visually? In our recent FDG paper we investigate how to deal with adversarial player strategies in our neural network game iNNk.<br><br>The video shows one such strategy, drawing a rebus puzzle.<br><br>PDF: <a href="https://t.co/3pLrODdp3Y">https://t.co/3pLrODdp3Y</a> <a href="https://t.co/7Qc5lfZkaf">pic.twitter.com/7Qc5lfZkaf</a></p>&mdash; Sebastian Risi (@risi1979) <a href="https://twitter.com/risi1979/status/1412393821959245834?ref_src=twsrc%5Etfw">July 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. Efficient Vision Transformers via Fine-Grained Manifold Distillation

Ding Jia, Kai Han, Yunhe Wang, Yehui Tang, Jianyuan Guo, Chao Zhang, Dacheng Tao

- retweets: 182, favorites: 66 (07/07/2021 09:36:56)

- links: [abs](https://arxiv.org/abs/2107.01378) | [pdf](https://arxiv.org/pdf/2107.01378)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

This paper studies the model compression problem of vision transformers. Benefit from the self-attention module, transformer architectures have shown extraordinary performance on many computer vision tasks. Although the network performance is boosted, transformers are often required more computational resources including memory usage and the inference complexity. Compared with the existing knowledge distillation approaches, we propose to excavate useful information from the teacher transformer through the relationship between images and the divided patches. We then explore an efficient fine-grained manifold distillation approach that simultaneously calculates cross-images, cross-patch, and random-selected manifolds in teacher and student models. Experimental results conducted on several benchmarks demonstrate the superiority of the proposed algorithm for distilling portable transformer models with higher performance. For example, our approach achieves 75.06% Top-1 accuracy on the ImageNet-1k dataset for training a DeiT-Tiny model, which outperforms other ViT distillation methods.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Efficient Vision Transformers via Fine-Grained Manifold Distillation<br>pdf: <a href="https://t.co/WwmHUd17dk">https://t.co/WwmHUd17dk</a><br>abs: <a href="https://t.co/eQ3hbcQfSL">https://t.co/eQ3hbcQfSL</a><br>approach achieves 75.06% Top-1 accuracy on the ImageNet-1k dataset for training a DeiT-Tiny model <a href="https://t.co/kraLMm66X6">pic.twitter.com/kraLMm66X6</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1412291494472523777?ref_src=twsrc%5Etfw">July 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. Test-Time Personalization with a Transformer for Human Pose Estimation

Miao Hao, Yizhuo Li, Zonglin Di, Nitesh B. Gundavarapu, Xiaolong Wang

- retweets: 154, favorites: 66 (07/07/2021 09:36:57)

- links: [abs](https://arxiv.org/abs/2107.02133) | [pdf](https://arxiv.org/pdf/2107.02133)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

We propose to personalize a human pose estimator given a set of test images of a person without using any manual annotations. While there is a significant advancement in human pose estimation, it is still very challenging for a model to generalize to different unknown environments and unseen persons. Instead of using a fixed model for every test case, we adapt our pose estimator during test time to exploit person-specific information. We first train our model on diverse data with both a supervised and a self-supervised pose estimation objectives jointly. We use a Transformer model to build a transformation between the self-supervised keypoints and the supervised keypoints. During test time, we personalize and adapt our model by fine-tuning with the self-supervised objective. The pose is then improved by transforming the updated self-supervised keypoints. We experiment with multiple datasets and show significant improvements on pose estimations with our self-supervised personalization.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Test-Time Personalization with a Transformer for Human Pose Estimation<br>pdf: <a href="https://t.co/LR1CTzMSOF">https://t.co/LR1CTzMSOF</a><br>abs: <a href="https://t.co/272J51jz43">https://t.co/272J51jz43</a> <a href="https://t.co/pPInkaiAmA">pic.twitter.com/pPInkaiAmA</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1412256168005750789?ref_src=twsrc%5Etfw">July 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 10. Data-driven mapping between functional connectomes using optimal  transport

Javid Dadashkarimi, Amin Karbasi, Dustin Scheinost

- retweets: 156, favorites: 45 (07/07/2021 09:36:57)

- links: [abs](https://arxiv.org/abs/2107.01303) | [pdf](https://arxiv.org/pdf/2107.01303)
- [q-bio.NC](https://arxiv.org/list/q-bio.NC/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Functional connectomes derived from functional magnetic resonance imaging have long been used to understand the functional organization of the brain. Nevertheless, a connectome is intrinsically linked to the atlas used to create it. In other words, a connectome generated from one atlas is different in scale and resolution compared to a connectome generated from another atlas. Being able to map connectomes and derived results between different atlases without additional pre-processing is a crucial step in improving interpretation and generalization between studies that use different atlases. Here, we use optimal transport, a powerful mathematical technique, to find an optimum mapping between two atlases. This mapping is then used to transform time series from one atlas to another in order to reconstruct a connectome. We validate our approach by comparing transformed connectomes against their "gold-standard" counterparts (i.e., connectomes generated directly from an atlas) and demonstrate the utility of transformed connectomes by applying these connectomes to predictive models based on a different atlas. We show that these transformed connectomes are significantly similar to their "gold-standard" counterparts and maintain individual differences in brain-behavior associations, demonstrating both the validity of our approach and its utility in downstream analyses. Overall, our approach is a promising avenue to increase the generalization of connectome-based results across different atlases.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I am excited to share my latest work about optimal transport with <a href="https://twitter.com/DScheinost?ref_src=twsrc%5Etfw">@DScheinost</a> and <a href="https://twitter.com/aminkarbasi?ref_src=twsrc%5Etfw">@aminkarbasi</a> :<br><br>Title: Data-driven mapping between functional<br>connectomes using optimal transport<br><br>preprint: <a href="https://t.co/0XCXi1FKpp">https://t.co/0XCXi1FKpp</a><br><br>code: <a href="https://t.co/ntPjLXCJlK">https://t.co/ntPjLXCJlK</a><a href="https://twitter.com/hashtag/MICCAI2021?src=hash&amp;ref_src=twsrc%5Etfw">#MICCAI2021</a> <a href="https://twitter.com/MICCAI_Society?ref_src=twsrc%5Etfw">@MICCAI_Society</a> <a href="https://t.co/Ysg8VegJt9">pic.twitter.com/Ysg8VegJt9</a></p>&mdash; javid.dadashkarimi (@JDadashkarimi) <a href="https://twitter.com/JDadashkarimi/status/1412332040947744768?ref_src=twsrc%5Etfw">July 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 11. Do Different Tracking Tasks Require Different Appearance Models?

Zhongdao Wang, Hengshuang Zhao, Ya-Li Li, Shengjin Wang, Philip H.S. Torr, Luca Bertinetto

- retweets: 63, favorites: 77 (07/07/2021 09:36:57)

- links: [abs](https://arxiv.org/abs/2107.02156) | [pdf](https://arxiv.org/pdf/2107.02156)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

Tracking objects of interest in a video is one of the most popular and widely applicable problems in computer vision. However, with the years, a Cambrian explosion of use cases and benchmarks has fragmented the problem in a multitude of different experimental setups. As a consequence, the literature has fragmented too, and now the novel approaches proposed by the community are usually specialised to fit only one specific setup. To understand to what extent this specialisation is actually necessary, in this work we present UniTrack, a unified tracking solution to address five different tasks within the same framework. UniTrack consists of a single and task-agnostic appearance model, which can be learned in a supervised or self-supervised fashion, and multiple "heads" to address individual tasks and that do not require training. We show how most tracking tasks can be solved within this framework, and that the same appearance model can be used to obtain performance that is competitive against specialised methods for all the five tasks considered. The framework also allows us to analyse appearance models obtained with the most recent self-supervised methods, thus significantly extending their evaluation and comparison to a larger variety of important problems. Code available at https://github.com/Zhongdao/UniTrack.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Do Different Tracking Tasks Require Different Appearance Models?<br>pdf: <a href="https://t.co/YM53bIicxc">https://t.co/YM53bIicxc</a><br>abs: <a href="https://t.co/Lfo1cmK3kx">https://t.co/Lfo1cmK3kx</a><br>github: <a href="https://t.co/3GHtskmBTy">https://t.co/3GHtskmBTy</a> <a href="https://t.co/LZBp1ScfBJ">pic.twitter.com/LZBp1ScfBJ</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1412261166647328770?ref_src=twsrc%5Etfw">July 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 12. What Makes for Hierarchical Vision Transformer?

Yuxin Fang, Xinggang Wang, Rui Wu, Jianwei Niu, Wenyu Liu

- retweets: 80, favorites: 34 (07/07/2021 09:36:57)

- links: [abs](https://arxiv.org/abs/2107.02174) | [pdf](https://arxiv.org/pdf/2107.02174)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Recent studies show that hierarchical Vision Transformer with interleaved non-overlapped intra window self-attention \& shifted window self-attention is able to achieve state-of-the-art performance in various visual recognition tasks and challenges CNN's dense sliding window paradigm. Most follow-up works try to replace shifted window operation with other kinds of cross window communication while treating self-attention as the de-facto standard for intra window information aggregation. In this short preprint, we question whether self-attention is the only choice for hierarchical Vision Transformer to attain strong performance, and what makes for hierarchical Vision Transformer? We replace self-attention layers in Swin Transformer and Shuffle Transformer with simple linear mapping and keep other components unchanged. The resulting architecture with 25.4M parameters and 4.2G FLOPs achieves 80.5\% Top-1 accuracy, compared to 81.3\% for Swin Transformer with 28.3M parameters and 4.5G FLOPs. We also experiment with other alternatives to self-attention for context aggregation inside each non-overlapped window, which all give similar competitive results under the same architecture. Our study reveals that the \textbf{macro architecture} of Swin model families (i.e., interleaved intra window \& cross window communications), other than specific aggregation layers or specific means of cross window communication, may be more responsible for its strong performance and is the real challenger to CNN's dense sliding window paradigm.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">What Makes for Hierarchical Vision Transformer?<br>pdf: <a href="https://t.co/44NsyNZIa8">https://t.co/44NsyNZIa8</a><br>abs: <a href="https://t.co/aQCNVjVk3R">https://t.co/aQCNVjVk3R</a><br><br>resulting architecture with 25.4M parameters and 4.2G FLOPs achieves 80.5% Top-1 accuracy, compared to 81.3% for Swin Transformer with 28.3M parameters and 4.5G FLOP <a href="https://t.co/aiU32BpVvA">pic.twitter.com/aiU32BpVvA</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1412217332194283520?ref_src=twsrc%5Etfw">July 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 13. A Multilayer Network Model of the Coevolution of the Spread of a Disease  and Competing Opinions

Kaiyan Peng, Zheng Lu, Vanessa Lin, Michael R. Lindstrom, Christian Parkinson, Chuntian Wang, Andrea L. Bertozzi, Mason A. Porter

- retweets: 42, favorites: 18 (07/07/2021 09:36:57)

- links: [abs](https://arxiv.org/abs/2107.01713) | [pdf](https://arxiv.org/pdf/2107.01713)
- [cs.SI](https://arxiv.org/list/cs.SI/recent) | [physics.soc-ph](https://arxiv.org/list/physics.soc-ph/recent) | [q-bio.PE](https://arxiv.org/list/q-bio.PE/recent)

During the COVID-19 pandemic, conflicting opinions on physical distancing swept across social media, affecting both human behavior and the spread of COVID-19. Inspired by such phenomena, we construct a two-layer multiplex network for the coupled spread of a disease and conflicting opinions. We model each process as a contagion. On one layer, we consider the concurrent evolution of two opinions -- pro-physical-distancing and anti-physical-distancing -- that compete with each other and have mutual immunity to each other. The disease evolves on the other layer, and individuals are less likely (respectively, more likely) to become infected when they adopt the pro-physical-distancing (respectively, anti-physical-distancing) opinion. We develop approximations of mean-field type by generalizing monolayer pair approximations to multilayer networks; these approximations agree well with Monte Carlo simulations for a broad range of parameters and several network structures. Through numerical simulations, we illustrate the influence of opinion dynamics on the spread of the disease from complex interactions both between the two conflicting opinions and between the opinions and the disease. We find that lengthening the duration that individuals hold an opinion may help suppress disease transmission, and we demonstrate that increasing the cross-layer correlations or intra-layer correlations of node degrees may lead to fewer individuals becoming infected with the disease.




# 14. EditSpeech: A Text Based Speech Editing System Using Partial Inference  and Bidirectional Fusion

Daxin Tan, Liqun Deng, Yu Ting Yeung, Xin Jiang, Xiao Chen, Tan Lee

- retweets: 25, favorites: 25 (07/07/2021 09:36:57)

- links: [abs](https://arxiv.org/abs/2107.01554) | [pdf](https://arxiv.org/pdf/2107.01554)
- [eess.AS](https://arxiv.org/list/eess.AS/recent) | [cs.SD](https://arxiv.org/list/cs.SD/recent)

This paper presents the design, implementation and evaluation of a speech editing system, named EditSpeech, which allows a user to perform deletion, insertion and replacement of words in a given speech utterance, without causing audible degradation in speech quality and naturalness. The EditSpeech system is developed upon a neural text-to-speech (NTTS) synthesis framework. Partial inference and bidirectional fusion are proposed to effectively incorporate the contextual information related to the edited region and achieve smooth transition at both left and right boundaries. Distortion introduced to the unmodified parts of the utterance is alleviated. The EditSpeech system is developed and evaluated on English and Chinese in multi-speaker scenarios. Objective and subjective evaluation demonstrate that EditSpeech outperforms a few baseline systems in terms of low spectral distortion and preferred speech quality. Audio samples are available online for demonstration https://daxintan-cuhk.github.io/EditSpeech/ .

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">EditSpeech: A Text Based Speech Editing System Using Partial Inference and Bidirectional Fusion<br>pdf: <a href="https://t.co/AFupl3y5e4">https://t.co/AFupl3y5e4</a><br>abs: <a href="https://t.co/EzYlVMX1Qd">https://t.co/EzYlVMX1Qd</a><br>project page: <a href="https://t.co/erZVbCwfEE">https://t.co/erZVbCwfEE</a> <a href="https://t.co/o3UHN08eaU">pic.twitter.com/o3UHN08eaU</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1412269382655483905?ref_src=twsrc%5Etfw">July 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



