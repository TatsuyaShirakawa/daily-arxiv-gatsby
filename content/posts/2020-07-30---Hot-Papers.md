---
title: Hot Papers 2020-07-30
date: 2020-07-31T08:44:02.Z
template: "post"
draft: false
slug: "hot-papers-2020-07-30"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-07-30"
socialImage: "/media/flying-marine.jpg"

---

# 1. SipMask: Spatial Information Preservation for Fast Image and Video  Instance Segmentation

Jiale Cao, Rao Muhammad Anwer, Hisham Cholakkal, Fahad Shahbaz Khan, Yanwei Pang, Ling Shao

- retweets: 41, favorites: 153 (07/31/2020 08:44:02)

- links: [abs](https://arxiv.org/abs/2007.14772) | [pdf](https://arxiv.org/pdf/2007.14772)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Single-stage instance segmentation approaches have recently gained popularity due to their speed and simplicity, but are still lagging behind in accuracy, compared to two-stage methods. We propose a fast single-stage instance segmentation method, called SipMask, that preserves instance-specific spatial information by separating mask prediction of an instance to different sub-regions of a detected bounding-box. Our main contribution is a novel light-weight spatial preservation (SP) module that generates a separate set of spatial coefficients for each sub-region within a bounding-box, leading to improved mask predictions. It also enables accurate delineation of spatially adjacent instances. Further, we introduce a mask alignment weighting loss and a feature alignment scheme to better correlate mask prediction with object detection. On COCO test-dev, our SipMask outperforms the existing single-stage methods. Compared to the state-of-the-art single-stage TensorMask, SipMask obtains an absolute gain of 1.0% (mask AP), while providing a four-fold speedup. In terms of real-time capabilities, SipMask outperforms YOLACT with an absolute gain of 3.0% (mask AP) under similar settings, while operating at comparable speed on a Titan Xp. We also evaluate our SipMask for real-time video instance segmentation, achieving promising results on YouTube-VIS dataset. The source code is available at https://github.com/JialeCao001/SipMask.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">SipMask: Spatial Information Preservation for Fast Image and Video Instance Segmentation<br>pdf: <a href="https://t.co/Ec1bLPKFxp">https://t.co/Ec1bLPKFxp</a><br>abs: <a href="https://t.co/Ry6YGHkKpi">https://t.co/Ry6YGHkKpi</a><br>github: <a href="https://t.co/ueoD2OuxDX">https://t.co/ueoD2OuxDX</a> <a href="https://t.co/dTZ118PNMb">pic.twitter.com/dTZ118PNMb</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1288646555306688513?ref_src=twsrc%5Etfw">July 30, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Biomedical and Clinical English Model Packages in the Stanza Python NLP  Library

Yuhao Zhang, Yuhui Zhang, Peng Qi, Christopher D. Manning, Curtis P. Langlotz

- retweets: 36, favorites: 134 (07/31/2020 08:44:03)

- links: [abs](https://arxiv.org/abs/2007.14640) | [pdf](https://arxiv.org/pdf/2007.14640)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

We introduce biomedical and clinical English model packages for the Stanza Python NLP library. These packages offer accurate syntactic analysis and named entity recognition capabilities for biomedical and clinical text, by combining Stanza's fully neural architecture with a wide variety of open datasets as well as large-scale unsupervised biomedical and clinical text data. We show via extensive experiments that our packages achieve syntactic analysis and named entity recognition performance that is on par with or surpasses state-of-the-art results. We further show that these models do not compromise speed compared to existing toolkits when GPU acceleration is available, and are made easy to download and use with Stanza's Python interface. A demonstration of our packages is available at: http://stanza.run/bio.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">🆕 We’ve extended Stanza with first domain-specific <a href="https://twitter.com/hashtag/NLProc?src=hash&amp;ref_src=twsrc%5Etfw">#NLProc</a> models for biomedical and clinical medical English. They range from approaching to significantly improving state of the art results on syntactic and NER tasks. Demo <a href="https://t.co/FSduMn1hZp">https://t.co/FSduMn1hZp</a> Paper <a href="https://t.co/VuK5ZriYEl">https://t.co/VuK5ZriYEl</a> <a href="https://t.co/NmHX8Wzah6">pic.twitter.com/NmHX8Wzah6</a></p>&mdash; Stanford NLP Group (@stanfordnlp) <a href="https://twitter.com/stanfordnlp/status/1288866315743064066?ref_src=twsrc%5Etfw">July 30, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">👋 Introducing biomedical &amp; clinical model packages for the Stanza <a href="https://twitter.com/hashtag/NLProc?src=hash&amp;ref_src=twsrc%5Etfw">#NLProc</a> toolkit, including:<br><br>- 2 bio UD syntactic analysis pipelines<br>- 1 clinical UD syntactic pipeline<br>- 8 bio NER models<br>- 2 clinical NER models<br><br>📖 Paper: <a href="https://t.co/NeyvpLiWuW">https://t.co/NeyvpLiWuW</a><br><br>Highlights in threads... 1/5 <a href="https://t.co/n7borB3eb6">pic.twitter.com/n7borB3eb6</a></p>&mdash; Yuhao Zhang (@yuhaozhangx) <a href="https://twitter.com/yuhaozhangx/status/1288748159854768129?ref_src=twsrc%5Etfw">July 30, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Towards Ecologically Valid Research on Language User Interfaces

Harm de Vries, Dzmitry Bahdanau, Christopher Manning

- retweets: 36, favorites: 133 (07/31/2020 08:44:03)

- links: [abs](https://arxiv.org/abs/2007.14435) | [pdf](https://arxiv.org/pdf/2007.14435)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

Language User Interfaces (LUIs) could improve human-machine interaction for a wide variety of tasks, such as playing music, getting insights from databases, or instructing domestic robots. In contrast to traditional hand-crafted approaches, recent work attempts to build LUIs in a data-driven way using modern deep learning methods. To satisfy the data needs of such learning algorithms, researchers have constructed benchmarks that emphasize the quantity of collected data at the cost of its naturalness and relevance to real-world LUI use cases. As a consequence, research findings on such benchmarks might not be relevant for developing practical LUIs. The goal of this paper is to bootstrap the discussion around this issue, which we refer to as the benchmarks' low ecological validity. To this end, we describe what we deem an ideal methodology for machine learning research on LUIs and categorize five common ways in which recent benchmarks deviate from it. We give concrete examples of the five kinds of deviations and their consequences. Lastly, we offer a number of recommendations as to how to increase the ecological validity of machine learning research on LUIs.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">The need for open data &amp; benchmarks in modern ML research has led to an outpouring of <a href="https://twitter.com/hashtag/NLProc?src=hash&amp;ref_src=twsrc%5Etfw">#NLProc</a> data creation. But <a href="https://twitter.com/harm_devries?ref_src=twsrc%5Etfw">@harm_devries</a>, <a href="https://twitter.com/DBahdanau?ref_src=twsrc%5Etfw">@DBahdanau</a> &amp; I suggest the low ecological validity of most of this data undermines the resulting research. Comments welcome! <a href="https://t.co/scSc2c6Flq">https://t.co/scSc2c6Flq</a> <a href="https://t.co/9Lg8NJLxZT">pic.twitter.com/9Lg8NJLxZT</a></p>&mdash; Christopher Manning (@chrmanning) <a href="https://twitter.com/chrmanning/status/1288912091227648000?ref_src=twsrc%5Etfw">July 30, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. On the Quantum versus Classical Learnability of Discrete Distributions

Ryan Sweke, Jean-Pierre Seifert, Dominik Hangleiter, Jens Eisert

- retweets: 14, favorites: 97 (07/31/2020 08:44:03)

- links: [abs](https://arxiv.org/abs/2007.14451) | [pdf](https://arxiv.org/pdf/2007.14451)
- [quant-ph](https://arxiv.org/list/quant-ph/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Here we study the comparative power of classical and quantum learners for generative modelling within the Probably Approximately Correct (PAC) framework. More specifically we consider the following task: Given samples from some unknown discrete probability distribution, output with high probability an efficient algorithm for generating new samples from a good approximation of the original distribution. Our primary result is the explicit construction of a class of discrete probability distributions which, under the decisional Diffie-Hellman assumption, is provably not efficiently PAC learnable by a classical generative modelling algorithm, but for which we construct an efficient quantum learner. This class of distributions therefore provides a concrete example of a generative modelling problem for which quantum learners exhibit a provable advantage over classical learning algorithms. In addition, we discuss techniques for proving classical generative modelling hardness results, as well as the relationship between the PAC learnability of Boolean functions and the PAC learnability of discrete probability distributions.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Quantum versus classical learnability of distributions, featuring a generative modelling problem for which quantum learners exhibit a provable advantage over classical learning algorithms.<a href="https://t.co/52HkUOEMC3">https://t.co/52HkUOEMC3</a> <a href="https://t.co/7uEdpSBBHQ">pic.twitter.com/7uEdpSBBHQ</a></p>&mdash; Jens Eisert (@jenseisert) <a href="https://twitter.com/jenseisert/status/1288714800885911552?ref_src=twsrc%5Etfw">July 30, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Translate the Facial Regions You Like Using Region-Wise Normalization

Wenshuang Liu, Wenting Chen, Linlin Shen

- retweets: 12, favorites: 50 (07/31/2020 08:44:03)

- links: [abs](https://arxiv.org/abs/2007.14615) | [pdf](https://arxiv.org/pdf/2007.14615)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Though GAN (Generative Adversarial Networks) based technique has greatly advanced the performance of image synthesis and face translation, only few works available in literature provide region based style encoding and translation. We propose in this paper a region-wise normalization framework, for region level face translation. While per-region style is encoded using available approach, we build a so called RIN (region-wise normalization) block to individually inject the styles into per-region feature maps and then fuse them for following convolution and upsampling. Both shape and texture of different regions can thus be translated to various target styles. A region matching loss has also been proposed to significantly reduce the inference between regions during the translation process. Extensive experiments on three publicly available datasets, i.e. Morph, RaFD and CelebAMask-HQ, suggest that our approach demonstrate a large improvement over state-of-the-art methods like StarGAN, SEAN and FUNIT. Our approach has further advantages in precise control of the regions to be translated. As a result, region level expression changes and step by step make up can be achieved. The video demo is available at https://youtu.be/ceRqsbzXAfk.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Translate the Facial Regions You Like Using Region-Wise<br>Normalization<br>pdf: <a href="https://t.co/QZVjz9HIMQ">https://t.co/QZVjz9HIMQ</a><br>abs: <a href="https://t.co/78zFHn3QY4">https://t.co/78zFHn3QY4</a> <a href="https://t.co/lWnMgjSIGO">pic.twitter.com/lWnMgjSIGO</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1288655702945300481?ref_src=twsrc%5Etfw">July 30, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. Visual Reasoning Strategies and Satisficing: How Uncertainty  Visualization Design Impacts Effect Size Judgments and Decisions

Alex Kale, Matthew Kay, Jessica Hullman

- retweets: 10, favorites: 47 (07/31/2020 08:44:03)

- links: [abs](https://arxiv.org/abs/2007.14516) | [pdf](https://arxiv.org/pdf/2007.14516)
- [cs.HC](https://arxiv.org/list/cs.HC/recent)

Uncertainty visualizations often emphasize point estimates to support magnitude estimates or decisions through visual comparison. However, when design choices emphasize means, users may overlook uncertainty information and misinterpret visual distance as a proxy for effect size. We present findings from a mixed design experiment on Mechanical Turk which tests eight uncertainty visualization designs: 95% containment intervals, hypothetical outcome plots, densities, and quantile dotplots, each with and without means added. We find that adding means to uncertainty visualizations has small biasing effects on both magnitude estimation and decision-making, consistent with discounting uncertainty. We also see that visualization designs that support the least biased effect size estimation do not support the best decision-making, suggesting that a chart user's sense of effect size may not necessarily be identical when they use the same information for different tasks. In a qualitative analysis of users' strategy descriptions, we find that many users switch strategies and do not employ an optimal strategy when one exists. Uncertainty visualizations which are optimally designed in theory may not be the most effective in practice because of the ways that users satisfice with heuristics, suggesting opportunities to better understand visualization effectiveness by modeling sets of potential strategies.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">We’re excited to share our preprint, “Visual Reasoning Strategies and Satisficing: How Uncertainty Visualization Design Impacts Effect Size Judgments and Decisions”, with <a href="https://twitter.com/JessicaHullman?ref_src=twsrc%5Etfw">@JessicaHullman</a> &amp; <a href="https://twitter.com/mjskay?ref_src=twsrc%5Etfw">@mjskay</a> <br><br>Details in the thread! (1/n)<a href="https://t.co/T2vqM0USQ4">https://t.co/T2vqM0USQ4</a></p>&mdash; Alex Kale (@AlexKale17) <a href="https://twitter.com/AlexKale17/status/1288902588235001857?ref_src=twsrc%5Etfw">July 30, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



