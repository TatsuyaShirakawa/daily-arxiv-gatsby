---
title: Hot Papers 2020-11-04
date: 2020-11-05T11:13:31.Z
template: "post"
draft: false
slug: "hot-papers-2020-11-04"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-11-04"
socialImage: "/media/flying-marine.jpg"

---

# 1. Learning Deformable Tetrahedral Meshes for 3D Reconstruction

Jun Gao, Wenzheng Chen, Tommy Xiang, Alec Jacobson, Morgan McGuire, Sanja Fidler

- retweets: 1625, favorites: 310 (11/05/2020 11:13:31)

- links: [abs](https://arxiv.org/abs/2011.01437) | [pdf](https://arxiv.org/pdf/2011.01437)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

3D shape representations that accommodate learning-based 3D reconstruction are an open problem in machine learning and computer graphics. Previous work on neural 3D reconstruction demonstrated benefits, but also limitations, of point cloud, voxel, surface mesh, and implicit function representations. We introduce Deformable Tetrahedral Meshes (DefTet) as a particular parameterization that utilizes volumetric tetrahedral meshes for the reconstruction problem. Unlike existing volumetric approaches, DefTet optimizes for both vertex placement and occupancy, and is differentiable with respect to standard 3D reconstruction loss functions. It is thus simultaneously high-precision, volumetric, and amenable to learning-based neural architectures. We show that it can represent arbitrary, complex topology, is both memory and computationally efficient, and can produce high-fidelity reconstructions with a significantly smaller grid size than alternative volumetric approaches. The predicted surfaces are also inherently defined as tetrahedral meshes, thus do not require post-processing. We demonstrate that DefTet matches or exceeds both the quality of the previous best approaches and the performance of the fastest ones. Our approach obtains high-quality tetrahedral meshes computed directly from noisy point clouds, and is the first to showcase high-quality 3D tet-mesh results using only a single image as input.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Learning Deformable Tetrahedral Meshes for 3D Reconstruction<br>pdf: <a href="https://t.co/Y4JtTegNQM">https://t.co/Y4JtTegNQM</a><br>abs: <a href="https://t.co/SFOP0bq7nm">https://t.co/SFOP0bq7nm</a><br>project page: <a href="https://t.co/JC7OYZLBOy">https://t.co/JC7OYZLBOy</a> <a href="https://t.co/noZqIAIJyi">pic.twitter.com/noZqIAIJyi</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1323844179177459713?ref_src=twsrc%5Etfw">November 4, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Excited to share our <a href="https://twitter.com/NVIDIAAI?ref_src=twsrc%5Etfw">@NVIDIAAI</a> work DefTet, which makes tetrahedral meshes, one of the major 3D representations in graphics and physics-based simulation, amenable to Deep Learning.<a href="https://twitter.com/ChenWenzheng?ref_src=twsrc%5Etfw">@ChenWenzheng</a> <a href="https://twitter.com/TommyX058?ref_src=twsrc%5Etfw">@TommyX058</a> <a href="https://twitter.com/_AlecJacobson?ref_src=twsrc%5Etfw">@_AlecJacobson</a> <a href="https://twitter.com/CasualEffects?ref_src=twsrc%5Etfw">@CasualEffects</a> <a href="https://twitter.com/FidlerSanja?ref_src=twsrc%5Etfw">@FidlerSanja</a><br>Paper: <a href="https://t.co/8DFyKsvMGs">https://t.co/8DFyKsvMGs</a> <a href="https://t.co/JmGBuiBqlu">pic.twitter.com/JmGBuiBqlu</a></p>&mdash; Jun Gao (@JunGao33210520) <a href="https://twitter.com/JunGao33210520/status/1324064554498883585?ref_src=twsrc%5Etfw">November 4, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Sample-efficient reinforcement learning using deep Gaussian processes

Charles Gadd, Markus Heinonen, Harri L√§hdesm√§ki, Samuel Kaski

- retweets: 545, favorites: 30 (11/05/2020 11:13:32)

- links: [abs](https://arxiv.org/abs/2011.01226) | [pdf](https://arxiv.org/pdf/2011.01226)
- [stat.ML](https://arxiv.org/list/stat.ML/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Reinforcement learning provides a framework for learning to control which actions to take towards completing a task through trial-and-error. In many applications observing interactions is costly, necessitating sample-efficient learning. In model-based reinforcement learning efficiency is improved by learning to simulate the world dynamics. The challenge is that model inaccuracies rapidly accumulate over planned trajectories. We introduce deep Gaussian processes where the depth of the compositions introduces model complexity while incorporating prior knowledge on the dynamics brings smoothness and structure. Our approach is able to sample a Bayesian posterior over trajectories. We demonstrate highly improved early sample-efficiency over competing methods. This is shown across a number of continuous control tasks, including the half-cheetah whose contact dynamics have previously posed an insurmountable problem for earlier sample-efficient Gaussian process based models.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Sample-efficient reinforcement learning using deep Gaussian processes. <a href="https://twitter.com/hashtag/AI?src=hash&amp;ref_src=twsrc%5Etfw">#AI</a> <a href="https://twitter.com/hashtag/DataScience?src=hash&amp;ref_src=twsrc%5Etfw">#DataScience</a> <a href="https://twitter.com/hashtag/BigData?src=hash&amp;ref_src=twsrc%5Etfw">#BigData</a> <a href="https://twitter.com/hashtag/Analytics?src=hash&amp;ref_src=twsrc%5Etfw">#Analytics</a> <a href="https://twitter.com/hashtag/Cloud?src=hash&amp;ref_src=twsrc%5Etfw">#Cloud</a> <a href="https://twitter.com/hashtag/IoT?src=hash&amp;ref_src=twsrc%5Etfw">#IoT</a> <a href="https://twitter.com/hashtag/Python?src=hash&amp;ref_src=twsrc%5Etfw">#Python</a> <a href="https://twitter.com/hashtag/RStats?src=hash&amp;ref_src=twsrc%5Etfw">#RStats</a> <a href="https://twitter.com/hashtag/JavaScript?src=hash&amp;ref_src=twsrc%5Etfw">#JavaScript</a> <a href="https://twitter.com/hashtag/ReactJS?src=hash&amp;ref_src=twsrc%5Etfw">#ReactJS</a> <a href="https://twitter.com/hashtag/Serverless?src=hash&amp;ref_src=twsrc%5Etfw">#Serverless</a> <a href="https://twitter.com/hashtag/Linux?src=hash&amp;ref_src=twsrc%5Etfw">#Linux</a> <a href="https://twitter.com/hashtag/100DaysOfCode?src=hash&amp;ref_src=twsrc%5Etfw">#100DaysOfCode</a> <a href="https://twitter.com/hashtag/Developers?src=hash&amp;ref_src=twsrc%5Etfw">#Developers</a> <a href="https://twitter.com/hashtag/Programming?src=hash&amp;ref_src=twsrc%5Etfw">#Programming</a> <a href="https://twitter.com/hashtag/Coding?src=hash&amp;ref_src=twsrc%5Etfw">#Coding</a> <a href="https://twitter.com/hashtag/MachineLearning?src=hash&amp;ref_src=twsrc%5Etfw">#MachineLearning</a> <a href="https://twitter.com/hashtag/DeepLearning?src=hash&amp;ref_src=twsrc%5Etfw">#DeepLearning</a><a href="https://t.co/Hbdztsjpo6">https://t.co/Hbdztsjpo6</a> <a href="https://t.co/m0pAy570Ro">pic.twitter.com/m0pAy570Ro</a></p>&mdash; Marcus Borba (@marcusborba) <a href="https://twitter.com/marcusborba/status/1324162719596072960?ref_src=twsrc%5Etfw">November 5, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. StyleMelGAN: An Efficient High-Fidelity Adversarial Vocoder with  Temporal Adaptive Normalization

Ahmed Mustafa, Nicola Pia, Guillaume Fuchs

- retweets: 144, favorites: 65 (11/05/2020 11:13:32)

- links: [abs](https://arxiv.org/abs/2011.01557) | [pdf](https://arxiv.org/pdf/2011.01557)
- [eess.AS](https://arxiv.org/list/eess.AS/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.SD](https://arxiv.org/list/cs.SD/recent) | [eess.SP](https://arxiv.org/list/eess.SP/recent)

In recent years, neural vocoders have surpassed classical speech generation approaches in naturalness and perceptual quality of the synthesized speech. Computationally heavy models like WaveNet and WaveGlow achieve best results, while lightweight GAN models, e.g. MelGAN and Parallel WaveGAN, remain inferior in terms of perceptual quality. We therefore propose StyleMelGAN, a lightweight neural vocoder allowing synthesis of high-fidelity speech with low computational complexity. StyleMelGAN employs temporal adaptive normalization to style a low-dimensional noise vector with the acoustic features of the target speech. For efficient training, multiple random-window discriminators adversarially evaluate the speech signal analyzed by a filter bank, with regularization provided by a multi-scale spectral reconstruction loss. The highly parallelizable speech generation is several times faster than real-time on CPUs and GPUs. MUSHRA and P.800 listening tests show that StyleMelGAN outperforms prior neural vocoders in copy-synthesis and Text-to-Speech scenarios.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">StyleMelGAN: An Efficient High-Fidelity Adversarial Vocoder with Temporal Adaptive Normalization<br>pdf: <a href="https://t.co/chyZhDQhGJ">https://t.co/chyZhDQhGJ</a><br>abs: <a href="https://t.co/XmxbdQzQ2X">https://t.co/XmxbdQzQ2X</a><br>project page: <a href="https://t.co/gliYnFPdS4">https://t.co/gliYnFPdS4</a> <a href="https://t.co/1az7JdQaq2">pic.twitter.com/1az7JdQaq2</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1323813093919641604?ref_src=twsrc%5Etfw">November 4, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Recommendations for Bayesian hierarchical model specifications for  case-control studies in mental health

Vincent Valton, Toby Wise, Oliver J. Robinson

- retweets: 98, favorites: 49 (11/05/2020 11:13:32)

- links: [abs](https://arxiv.org/abs/2011.01725) | [pdf](https://arxiv.org/pdf/2011.01725)
- [cs.CY](https://arxiv.org/list/cs.CY/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.AP](https://arxiv.org/list/stat.AP/recent) | [stat.ME](https://arxiv.org/list/stat.ME/recent)

Hierarchical model fitting has become commonplace for case-control studies of cognition and behaviour in mental health. However, these techniques require us to formalise assumptions about the data-generating process at the group level, which may not be known. Specifically, researchers typically must choose whether to assume all subjects are drawn from a common population, or to model them as deriving from separate populations. These assumptions have profound implications for computational psychiatry, as they affect the resulting inference (latent parameter recovery) and may conflate or mask true group-level differences. To test these assumptions we ran systematic simulations on synthetic multi-group behavioural data from a commonly used multi-armed bandit task (reinforcement learning task). We then examined recovery of group differences in latent parameter space under the two commonly used generative modelling assumptions: (1) modelling groups under a common shared group-level prior (assuming all participants are generated from a common distribution, and are likely to share common characteristics); (2) modelling separate groups based on symptomatology or diagnostic labels, resulting in separate group-level priors. We evaluated the robustness of these approaches to variations in data quality and prior specifications on a variety of metrics. We found that fitting groups separately (assumptions 2), provided the most accurate and robust inference across all conditions. Our results suggest that when dealing with data from multiple clinical groups, researchers should analyse patient and control groups separately as it provides the most accurate and robust recovery of the parameters of interest.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">üéâ Extended Abs. paper got accepted at the <a href="https://twitter.com/NeurIPSConf?ref_src=twsrc%5Etfw">@NeurIPSConf</a> <a href="https://twitter.com/hashtag/ML4H?src=hash&amp;ref_src=twsrc%5Etfw">#ML4H</a> workshop! üéä<br><br>In this work, we quantify and demonstrate how hierarchical modelling assumptions &amp; data quality impacts parameter recovery for case-control studies in mental health. <a href="https://twitter.com/hashtag/neurips2020?src=hash&amp;ref_src=twsrc%5Etfw">#neurips2020</a><a href="https://t.co/NImFCAp5MN">https://t.co/NImFCAp5MN</a></p>&mdash; vincent valton (@vincentvalton) <a href="https://twitter.com/vincentvalton/status/1323963886081576961?ref_src=twsrc%5Etfw">November 4, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">A brief welcome distraction...our work on the importance of prior specification in hierarchical modelling (which we turned into a paper in a day just before CovidüôÉ) has been expanded and accepted for the <a href="https://twitter.com/NeurIPSConf?ref_src=twsrc%5Etfw">@NeurIPSConf</a> <a href="https://twitter.com/hashtag/ML4H?src=hash&amp;ref_src=twsrc%5Etfw">#ML4H</a> workshop: <a href="https://t.co/5EURUDyCiy">https://t.co/5EURUDyCiy</a> <a href="https://t.co/sBSkj72KPw">https://t.co/sBSkj72KPw</a></p>&mdash; Oliver Robinson (@olijrobinson) <a href="https://twitter.com/olijrobinson/status/1323967223548022784?ref_src=twsrc%5Etfw">November 4, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. The Complexity of Gradient Descent: CLS = PPAD $\cap$ PLS

John Fearnley, Paul W. Goldberg, Alexandros Hollender, Rahul Savani

- retweets: 86, favorites: 57 (11/05/2020 11:13:32)

- links: [abs](https://arxiv.org/abs/2011.01929) | [pdf](https://arxiv.org/pdf/2011.01929)
- [cs.CC](https://arxiv.org/list/cs.CC/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [math.OC](https://arxiv.org/list/math.OC/recent)

We study search problems that can be solved by performing Gradient Descent on a bounded convex polytopal domain and show that this class is equal to the intersection of two well-known classes: PPAD and PLS. As our main underlying technical contribution, we show that computing a Karush-Kuhn-Tucker (KKT) point of a continuously differentiable function over the domain $[0,1]^2$ is PPAD $\cap$ PLS-complete. This is the first natural problem to be shown complete for this class. Our results also imply that the class CLS (Continuous Local Search) - which was defined by Daskalakis and Papadimitriou as a more "natural" counterpart to PPAD $\cap$ PLS and contains many interesting problems - is itself equal to PPAD $\cap$ PLS.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">A beautiful paper (<a href="https://t.co/QcsEFUpYGS">https://t.co/QcsEFUpYGS</a>) solving a great open problem in the complexity theory of search problems. <a href="https://twitter.com/paulwgoldberg?ref_src=twsrc%5Etfw">@paulwgoldberg</a> <a href="https://twitter.com/rahul__savani?ref_src=twsrc%5Etfw">@rahul__savani</a> and co show the first natural PLS-complete problem: gradient descent.</p>&mdash; Arnab Bhattacharyya (@abhatt2) <a href="https://twitter.com/abhatt2/status/1323819993767686145?ref_src=twsrc%5Etfw">November 4, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Thanks, Arnab. In the following thread I provide some more discussion of our paper.<br><br>The Complexity of Gradient Descent: CLS = PPAD‚à©PLS<a href="https://t.co/8dA0GkxeeW">https://t.co/8dA0GkxeeW</a><br><br>with my wonderful co-authors John Fearnley, Alexandros Hollender, and <a href="https://twitter.com/rahul__savani?ref_src=twsrc%5Etfw">@rahul__savani</a> <br>1/7 <a href="https://t.co/4fbUqFSoRM">https://t.co/4fbUqFSoRM</a></p>&mdash; Paul Goldberg (@paulwgoldberg) <a href="https://twitter.com/paulwgoldberg/status/1323902581089984513?ref_src=twsrc%5Etfw">November 4, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. CharBERT: Character-aware Pre-trained Language Model

Wentao Ma, Yiming Cui, Chenglei Si, Ting Liu, Shijin Wang, Guoping Hu

- retweets: 90, favorites: 50 (11/05/2020 11:13:33)

- links: [abs](https://arxiv.org/abs/2011.01513) | [pdf](https://arxiv.org/pdf/2011.01513)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

Most pre-trained language models (PLMs) construct word representations at subword level with Byte-Pair Encoding (BPE) or its variations, by which OOV (out-of-vocab) words are almost avoidable. However, those methods split a word into subword units and make the representation incomplete and fragile. In this paper, we propose a character-aware pre-trained language model named CharBERT improving on the previous methods (such as BERT, RoBERTa) to tackle these problems. We first construct the contextual word embedding for each token from the sequential character representations, then fuse the representations of characters and the subword representations by a novel heterogeneous interaction module. We also propose a new pre-training task named NLM (Noisy LM) for unsupervised character representation learning. We evaluate our method on question answering, sequence labeling, and text classification tasks, both on the original datasets and adversarial misspelling test sets. The experimental results show that our method can significantly improve the performance and robustness of PLMs simultaneously. Pretrained models, evaluation sets, and code are available at https://github.com/wtma/CharBERT

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">CharBERT: Character-aware Pre-trained Language Model<br>pdf: <a href="https://t.co/YeO9iMJMuZ">https://t.co/YeO9iMJMuZ</a><br>abs: <a href="https://t.co/oBL69ehtL9">https://t.co/oBL69ehtL9</a><br>github: <a href="https://t.co/4gbBtF5cnw">https://t.co/4gbBtF5cnw</a> <a href="https://t.co/NbxSwneu0T">pic.twitter.com/NbxSwneu0T</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1323817013257621512?ref_src=twsrc%5Etfw">November 4, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. Supervised Contrastive Learning for Pre-trained Language Model  Fine-tuning

Beliz Gunel, Jingfei Du, Alexis Conneau, Ves Stoyanov

- retweets: 62, favorites: 40 (11/05/2020 11:13:33)

- links: [abs](https://arxiv.org/abs/2011.01403) | [pdf](https://arxiv.org/pdf/2011.01403)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

State-of-the-art natural language understanding classification models follow two-stages: pre-training a large language model on an auxiliary task, and then fine-tuning the model on a task-specific labeled dataset using cross-entropy loss. Cross-entropy loss has several shortcomings that can lead to sub-optimal generalization and instability. Driven by the intuition that good generalization requires capturing the similarity between examples in one class and contrasting them with examples in other classes, we propose a supervised contrastive learning (SCL) objective for the fine-tuning stage. Combined with cross-entropy, the SCL loss we propose obtains improvements over a strong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in both the high-data and low-data regimes, and it does not require any specialized architecture, data augmentation of any kind, memory banks, or additional unsupervised data. We also demonstrate that the new objective leads to models that are more robust to different levels of noise in the training data, and can generalize better to related tasks with limited labeled task data.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning<br>pdf: <a href="https://t.co/FLAq94ZWnX">https://t.co/FLAq94ZWnX</a><br>abs: <a href="https://t.co/88UnjqmaKr">https://t.co/88UnjqmaKr</a> <a href="https://t.co/aU5JA9tjzx">pic.twitter.com/aU5JA9tjzx</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1323827665376366593?ref_src=twsrc%5Etfw">November 4, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. Transforming Gaussian Processes With Normalizing Flows

Juan Maro√±as, Oliver Hamelijnck, Jeremias Knoblauch, Theodoros Damoulas

- retweets: 64, favorites: 35 (11/05/2020 11:13:33)

- links: [abs](https://arxiv.org/abs/2011.01596) | [pdf](https://arxiv.org/pdf/2011.01596)
- [cs.LG](https://arxiv.org/list/cs.LG/recent)

Gaussian Processes (GPs) can be used as flexible, non-parametric function priors. Inspired by the growing body of work on Normalizing Flows, we enlarge this class of priors through a parametric invertible transformation that can be made input-dependent. Doing so also allows us to encode interpretable prior knowledge (e.g., boundedness constraints). We derive a variational approximation to the resulting Bayesian inference problem, which is as fast as stochastic variational GP regression (Hensman et al., 2013; Dezfouli and Bonilla,2015). This makes the model a computationally efficient alternative to other hierarchical extensions of GP priors (Lazaro-Gredilla,2012; Damianou and Lawrence, 2013). The resulting algorithm's computational and inferential performance is excellent, and we demonstrate this on a range of data sets. For example, even with only 5 inducing points and an input-dependent flow, our method is consistently competitive with a standard sparse GP fitted using 100 inducing points.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">„Ç¨„Ç¶„ÇπÈÅéÁ®ã„Å®Normalizing flowÁµÑ„ÅøÂêà„Çè„Åõ„Çå„Å∞„ÄÅ„Çà„ÇäÊüîËªü„Å™Èñ¢Êï∞„ÅÆ‰∫ãÂâçÂàÜÂ∏É„ÅåÊßãÁØâ„Åß„Åç„Åæ„Åô„Çà„Å®„ÅÑ„ÅÜË©±„Å£„ÅΩ„ÅÑ<a href="https://t.co/KM7VDlZMis">https://t.co/KM7VDlZMis</a></p>&mdash; Ryuji WATANABE (@ae14watanabe) <a href="https://twitter.com/ae14watanabe/status/1323894916179582977?ref_src=twsrc%5Etfw">November 4, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. Revisiting Adaptive Convolutions for Video Frame Interpolation

Simon Niklaus, Long Mai, Oliver Wang

- retweets: 56, favorites: 28 (11/05/2020 11:13:33)

- links: [abs](https://arxiv.org/abs/2011.01280) | [pdf](https://arxiv.org/pdf/2011.01280)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Video frame interpolation, the synthesis of novel views in time, is an increasingly popular research direction with many new papers further advancing the state of the art. But as each new method comes with a host of variables that affect the interpolation quality, it can be hard to tell what is actually important for this task. In this work, we show, somewhat surprisingly, that it is possible to achieve near state-of-the-art results with an older, simpler approach, namely adaptive separable convolutions, by a subtle set of low level improvements. In doing so, we propose a number of intuitive but effective techniques to improve the frame interpolation quality, which also have the potential to other related applications of adaptive convolutions such as burst image denoising, joint image filtering, or video prediction.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Revisiting Adaptive Convolutions for Video Frame Interpolation<br>pdf: <a href="https://t.co/ECd2PwPyZE">https://t.co/ECd2PwPyZE</a><br>abs: <a href="https://t.co/2DTzM1NNKp">https://t.co/2DTzM1NNKp</a><br>project page: <a href="https://t.co/4XE6GXI4qt">https://t.co/4XE6GXI4qt</a> <a href="https://t.co/smTDTE7IZq">pic.twitter.com/smTDTE7IZq</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1323814266655395843?ref_src=twsrc%5Etfw">November 4, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 10. Levels of Coupling in Dyadic Interaction: An Analysis of Neural and  Behavioral Complexity

Georgina Montserrat Res√©ndiz-Benhumea, Ekaterina Sangati, Tom Froese

- retweets: 30, favorites: 20 (11/05/2020 11:13:33)

- links: [abs](https://arxiv.org/abs/2011.01727) | [pdf](https://arxiv.org/pdf/2011.01727)
- [cs.MA](https://arxiv.org/list/cs.MA/recent)

From an enactive approach, some previous studies have demonstrated that social interaction plays a fundamental role in the dynamics of neural and behavioral complexity of embodied agents. In particular, it has been shown that agents with a limited internal structure (2-neuron brains) that evolve in interaction can overcome this limitation and exhibit chaotic neural activity, typically associated with more complex dynamical systems (at least 3-dimensional). In the present paper we make two contributions to this line of work. First, we propose a conceptual distinction in levels of coupling between agents that could have an effect on neural and behavioral complexity. Second, we test the generalizability of previous results by testing agents with richer internal structure and evolving them in a richer, yet non-social, environment. We demonstrate that such agents can achieve levels of complexity comparable to agents that evolve in interactive settings. We discuss the significance of this result for the study of interaction.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">New preprint of paper for the IEEE Symposium on Artificial Life: we used an agent-based model to investigate the relationship between neural and behavioral complexity in solitary and social scenarios. <a href="https://t.co/21sRBsabaM">https://t.co/21sRBsabaM</a></p>&mdash; Embodied Cognitive Science Unit (@EcsuOist) <a href="https://twitter.com/EcsuOist/status/1323865533679202309?ref_src=twsrc%5Etfw">November 4, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



