---
title: Hot Papers 2021-02-19
date: 2021-02-20T09:36:06.Z
template: "post"
draft: false
slug: "hot-papers-2021-02-19"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2021-02-19"
socialImage: "/media/flying-marine.jpg"

---

# 1. Combinatorial optimization and reasoning with graph neural networks

Quentin Cappart, Didier Ch√©telat, Elias Khalil, Andrea Lodi, Christopher Morris, Petar Veliƒçkoviƒá

- retweets: 2068, favorites: 207 (02/20/2021 09:36:06)

- links: [abs](https://arxiv.org/abs/2102.09544) | [pdf](https://arxiv.org/pdf/2102.09544)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.DS](https://arxiv.org/list/cs.DS/recent) | [cs.NE](https://arxiv.org/list/cs.NE/recent) | [math.OC](https://arxiv.org/list/math.OC/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Combinatorial optimization is a well-established area in operations research and computer science. Until recently, its methods have focused on solving problem instances in isolation, ignoring the fact that they often stem from related data distributions in practice. However, recent years have seen a surge of interest in using machine learning, especially graph neural networks (GNNs), as a key building block for combinatorial tasks, either as solvers or as helper functions. GNNs are an inductive bias that effectively encodes combinatorial and relational input due to their permutation-invariance and sparsity awareness. This paper presents a conceptual review of recent key advancements in this emerging field, aiming at both the optimization and machine learning researcher.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">If you&#39;re interested in GNNs for combinatorial tasks (certainly an exciting time!), we&#39;ve released our 43-page comprehensive survey on the area! + detailed blueprint of algorithmic reasoning in S3.3.<a href="https://t.co/F4TG4svKMG">https://t.co/F4TG4svKMG</a><br><br>with <a href="https://twitter.com/chrsmrrs?ref_src=twsrc%5Etfw">@chrsmrrs</a> <a href="https://twitter.com/69alodi?ref_src=twsrc%5Etfw">@69alodi</a> <a href="https://twitter.com/lyeskhalil?ref_src=twsrc%5Etfw">@lyeskhalil</a> <a href="https://twitter.com/qcappart?ref_src=twsrc%5Etfw">@qcappart</a> &amp; Didier <a href="https://t.co/P6TANTgLvr">pic.twitter.com/P6TANTgLvr</a></p>&mdash; Petar Veliƒçkoviƒá (@PetarV_93) <a href="https://twitter.com/PetarV_93/status/1362802841459523593?ref_src=twsrc%5Etfw">February 19, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Mobile Computational Photography: A Tour

Mauricio Delbracio, Damien Kelly, Michael S. Brown, Peyman Milanfar

- retweets: 1384, favorites: 144 (02/20/2021 09:36:06)

- links: [abs](https://arxiv.org/abs/2102.09000) | [pdf](https://arxiv.org/pdf/2102.09000)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [eess.IV](https://arxiv.org/list/eess.IV/recent)

The first mobile camera phone was sold only 20 years ago, when taking pictures with one's phone was an oddity, and sharing pictures online was unheard of. Today, the smartphone is more camera than phone. How did this happen? This transformation was enabled by advances in computational photography -the science and engineering of making great images from small form factor, mobile cameras. Modern algorithmic and computing advances, including machine learning, have changed the rules of photography, bringing to it new modes of capture, post-processing, storage, and sharing. In this paper, we give a brief history of mobile computational photography and describe some of the key technological components, including burst photography, noise reduction, and super-resolution. At each step, we may draw naive parallels to the human visual system.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">1/ With colleagues including <a href="https://twitter.com/2ptmvd?ref_src=twsrc%5Etfw">@2ptmvd</a> we&#39;ve written an expository article on (mobile) computational photography meant for non-expert (technical) audience. Appearing soon in <a href="https://twitter.com/AnnualReviews?ref_src=twsrc%5Etfw">@AnnualReviews</a> of Vision Science, it gives a sense of how your mobile camera works<a href="https://t.co/4nzYcw9VNI">https://t.co/4nzYcw9VNI</a> <a href="https://t.co/JHuujfDHJx">pic.twitter.com/JHuujfDHJx</a></p>&mdash; Peyman Milanfar (@docmilanfar) <a href="https://twitter.com/docmilanfar/status/1362608784728756224?ref_src=twsrc%5Etfw">February 19, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Monitoring behavioural responses during pandemic via reconstructed  contact matrices from online and representative surveys

J√∫lia Koltai, Orsolya V√°s√°rhelyi, Gergely R√∂st, M√°rton Karsai

- retweets: 306, favorites: 47 (02/20/2021 09:36:07)

- links: [abs](https://arxiv.org/abs/2102.09021) | [pdf](https://arxiv.org/pdf/2102.09021)
- [physics.soc-ph](https://arxiv.org/list/physics.soc-ph/recent) | [cs.CY](https://arxiv.org/list/cs.CY/recent) | [cs.SI](https://arxiv.org/list/cs.SI/recent) | [stat.AP](https://arxiv.org/list/stat.AP/recent)

The unprecedented behavioural responses of societies have been evidently shaping the COVID-19 pandemic, yet it is a significant challenge to accurately monitor the continuously changing social mixing patterns in real-time. Contact matrices, usually stratified by age, summarise interaction motifs efficiently, but their collection relies on conventional representative survey techniques, which are expensive and slow to obtain. Here we report a data collection effort involving over $2.3\%$ of the Hungarian population to simultaneously record contact matrices through a longitudinal online and sequence of representative phone surveys. To correct non-representative biases characterising the online data, by using census data and the representative samples we develop a reconstruction method to provide a scalable, cheap, and flexible way to dynamically obtain closer-to-representative contact matrices. Our results demonstrate the potential of combined online-offline data collections to understand the changing behavioural responses determining the future evolution of the outbreak, and inform epidemic models with crucial data.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">After long-long time our paper on dynamical contact matrix reconstruction is out <a href="https://t.co/xo3ptQClcg">https://t.co/xo3ptQClcg</a>. This is the largest data ever collected for contact matrix estimation and could never been done without <a href="https://twitter.com/koltaijuli?ref_src=twsrc%5Etfw">@koltaijuli</a> <a href="https://twitter.com/Orsi_Vasarhelyi?ref_src=twsrc%5Etfw">@Orsi_Vasarhelyi</a> and <a href="https://twitter.com/gergely_rost?ref_src=twsrc%5Etfw">@gergely_rost</a>. <a href="https://t.co/EV5wekdxFy">pic.twitter.com/EV5wekdxFy</a></p>&mdash; Marton Karsai (@MartonKarsai) <a href="https://twitter.com/MartonKarsai/status/1362648017791299584?ref_src=twsrc%5Etfw">February 19, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. State Entropy Maximization with Random Encoders for Efficient  Exploration

Younggyo Seo, Lili Chen, Jinwoo Shin, Honglak Lee, Pieter Abbeel, Kimin Lee

- retweets: 194, favorites: 53 (02/20/2021 09:36:07)

- links: [abs](https://arxiv.org/abs/2102.09430) | [pdf](https://arxiv.org/pdf/2102.09430)
- [cs.LG](https://arxiv.org/list/cs.LG/recent)

Recent exploration methods have proven to be a recipe for improving sample-efficiency in deep reinforcement learning (RL). However, efficient exploration in high-dimensional observation spaces still remains a challenge. This paper presents Random Encoders for Efficient Exploration (RE3), an exploration method that utilizes state entropy as an intrinsic reward. In order to estimate state entropy in environments with high-dimensional observations, we utilize a k-nearest neighbor entropy estimator in the low-dimensional representation space of a convolutional encoder. In particular, we find that the state entropy can be estimated in a stable and compute-efficient manner by utilizing a randomly initialized encoder, which is fixed throughout training. Our experiments show that RE3 significantly improves the sample-efficiency of both model-free and model-based RL methods on locomotion and navigation tasks from DeepMind Control Suite and MiniGrid benchmarks. We also show that RE3 allows learning diverse behaviors without extrinsic rewards, effectively improving sample-efficiency in downstream tasks. Source code and videos are available at https://sites.google.com/view/re3-rl.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Excited to share RE3! Maximizing state entropy in high-dimensional observation spaces can improve sample-efficiency of model-free (RAD) and model-based (Dreamer) RL.<br>üéì<a href="https://t.co/hvLjUhtz5o">https://t.co/hvLjUhtz5o</a><br>üíª<a href="https://t.co/jewQmxreLz">https://t.co/jewQmxreLz</a><br>w/<a href="https://twitter.com/younggyoseo?ref_src=twsrc%5Etfw">@younggyoseo</a> <a href="https://twitter.com/lchen915?ref_src=twsrc%5Etfw">@lchen915</a> <a href="https://twitter.com/jinwoos0417?ref_src=twsrc%5Etfw">@jinwoos0417</a> <a href="https://twitter.com/honglaklee?ref_src=twsrc%5Etfw">@honglaklee</a> <a href="https://twitter.com/pabbeel?ref_src=twsrc%5Etfw">@pabbeel</a><br>1/N <a href="https://t.co/4F2JLswKko">pic.twitter.com/4F2JLswKko</a></p>&mdash; Kimin (@kimin_le2) <a href="https://twitter.com/kimin_le2/status/1362815234830786565?ref_src=twsrc%5Etfw">February 19, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Unbiased Teacher for Semi-Supervised Object Detection

Yen-Cheng Liu, Chih-Yao Ma, Zijian He, Chia-Wen Kuo, Kan Chen, Peizhao Zhang, Bichen Wu, Zsolt Kira, Peter Vajda

- retweets: 132, favorites: 63 (02/20/2021 09:36:07)

- links: [abs](https://arxiv.org/abs/2102.09480) | [pdf](https://arxiv.org/pdf/2102.09480)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Semi-supervised learning, i.e., training networks with both labeled and unlabeled data, has made significant progress recently. However, existing works have primarily focused on image classification tasks and neglected object detection which requires more annotation effort. In this work, we revisit the Semi-Supervised Object Detection (SS-OD) and identify the pseudo-labeling bias issue in SS-OD. To address this, we introduce Unbiased Teacher, a simple yet effective approach that jointly trains a student and a gradually progressing teacher in a mutually-beneficial manner. Together with a class-balance loss to downweight overly confident pseudo-labels, Unbiased Teacher consistently improved state-of-the-art methods by significant margins on COCO-standard, COCO-additional, and VOC datasets. Specifically, Unbiased Teacher achieves 6.8 absolute mAP improvements against state-of-the-art method when using 1% of labeled data on MS-COCO, achieves around 10 mAP improvements against the supervised baseline when using only 0.5, 1, 2% of labeled data on MS-COCO.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Unbiased Teacher for Semi-Supervised Object Detection<br>pdf: <a href="https://t.co/qBCPsvd8nI">https://t.co/qBCPsvd8nI</a><br>abs: <a href="https://t.co/E6lCLBBf3T">https://t.co/E6lCLBBf3T</a><br>github: <a href="https://t.co/fAjP4ZW2lP">https://t.co/fAjP4ZW2lP</a> <a href="https://t.co/2eIVnP7PPK">pic.twitter.com/2eIVnP7PPK</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1362610664594956297?ref_src=twsrc%5Etfw">February 19, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Semi-supervised object detection is hard. Why? Imbalance! We&#39;ve released our <a href="https://twitter.com/hashtag/ICLR21?src=hash&amp;ref_src=twsrc%5Etfw">#ICLR21</a> Unbiased Teacher: ~17% mAP with only 0.5% labeled data on COCO!<br><br>Great work by <a href="https://twitter.com/yen_cheng_liu?ref_src=twsrc%5Etfw">@yen_cheng_liu</a>, with <a href="https://twitter.com/chihyaoma?ref_src=twsrc%5Etfw">@chihyaoma</a> &amp; FB!<br>Paper: <a href="https://t.co/D0TuhiJ2IB">https://t.co/D0TuhiJ2IB</a><br>Code: <a href="https://t.co/OyKE43TgLc">https://t.co/OyKE43TgLc</a><a href="https://twitter.com/mlatgt?ref_src=twsrc%5Etfw">@mlatgt</a> <a href="https://twitter.com/ICatGT?ref_src=twsrc%5Etfw">@ICatGT</a> <a href="https://t.co/BOILoW4B2J">pic.twitter.com/BOILoW4B2J</a></p>&mdash; Zsolt Kira (@zsoltkira) <a href="https://twitter.com/zsoltkira/status/1362601269748256772?ref_src=twsrc%5Etfw">February 19, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. Quantum field-theoretic machine learning

Dimitrios Bachtis, Gert Aarts, Biagio Lucini

- retweets: 149, favorites: 21 (02/20/2021 09:36:07)

- links: [abs](https://arxiv.org/abs/2102.09449) | [pdf](https://arxiv.org/pdf/2102.09449)
- [hep-lat](https://arxiv.org/list/hep-lat/recent) | [cond-mat.dis-nn](https://arxiv.org/list/cond-mat.dis-nn/recent) | [cond-mat.stat-mech](https://arxiv.org/list/cond-mat.stat-mech/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [hep-th](https://arxiv.org/list/hep-th/recent)

We derive machine learning algorithms from discretized Euclidean field theories, making inference and learning possible within dynamics described by quantum field theory. Specifically, we demonstrate that the $\phi^{4}$ scalar field theory satisfies the Hammersley-Clifford theorem, therefore recasting it as a machine learning algorithm within the mathematically rigorous framework of Markov random fields. We illustrate the concepts by minimizing an asymmetric distance between the probability distribution of the $\phi^{4}$ theory and that of target distributions, by quantifying the overlap of statistical ensembles between probability distributions and through reweighting to complex-valued actions with longer-range interactions. Neural networks architectures are additionally derived from the $\phi^{4}$ theory which can be viewed as generalizations of conventional neural networks and applications are presented. We conclude by discussing how the proposal opens up a new research avenue, that of developing a mathematical and computational framework of machine learning within quantum field theory.




# 7. Risk Framework for Bitcoin Custody Operation with the Revault Protocol

Jacob Swambo, Antoine Poinsot

- retweets: 132, favorites: 23 (02/20/2021 09:36:07)

- links: [abs](https://arxiv.org/abs/2102.09392) | [pdf](https://arxiv.org/pdf/2102.09392)
- [cs.CY](https://arxiv.org/list/cs.CY/recent)

Our contributions with this paper are twofold. First, we elucidate the methodological requirements for a risk framework of custodial operations and argue for the value of this type of risk model as complementary with cryptographic and blockchain security models. Second, we present a risk model in the form of a library of attack-trees for Revault -- an open-source custody protocol. The model can be used by organisations as a risk quantification framework for a thorough security analysis in their specific deployment context. Our work exemplifies an approach that can be used independent of which custody protocol is being considered, including complex protocols with multiple stakeholders and active defence infrastructure.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">We made a thing, and it was accepted at FC21.<a href="https://t.co/77zbJn0Z6B">https://t.co/77zbJn0Z6B</a> <br><br>&quot;Risk Framework for Bitcoin Custody Operation<br>with the Revault Protocol&quot;</p>&mdash; Antoine Poinsot (@darosior) <a href="https://twitter.com/darosior/status/1362704756985237505?ref_src=twsrc%5Etfw">February 19, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. Reaching Consensus for Asynchronous Distributed Key Generation

Ittai Abraham, Philipp Jovanovic, Mary Maller, Sarah Meiklejohn, Gilad Stern, Alin Tomescu

- retweets: 100, favorites: 30 (02/20/2021 09:36:07)

- links: [abs](https://arxiv.org/abs/2102.09041) | [pdf](https://arxiv.org/pdf/2102.09041)
- [cs.DC](https://arxiv.org/list/cs.DC/recent)

We give a protocol for Asynchronous Distributed Key Generation (A-DKG) that is optimally resilient (can withstand $f<\frac{n}{3}$ faulty parties), has a constant expected number of rounds, has $\tilde{O}(n^3)$ expected communication complexity, and assumes only the existence of a PKI. Prior to our work, the best A-DKG protocols required $\Omega(n)$ expected number of rounds, and $\Omega(n^4)$ expected communication.   Our A-DKG protocol relies on several building blocks that are of independent interest. We define and design a Proposal Election (PE) protocol that allows parties to retrospectively agree on a valid proposal after enough proposals have been sent from different parties. With constant probability the elected proposal was proposed by a non-faulty party. In building our PE protocol, we design a Verifiable Gather protocol which allows parties to communicate which proposals they have and have not seen in a verifiable manner. The final building block to our A-DKG is a Validated Asynchronous Byzantine Agreement (VABA) protocol. We use our PE protocol to construct a VABA protocol that does not require leaders or an asynchronous DKG setup. Our VABA protocol can be used more generally when it is not possible to use threshold signatures.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">No Waitin&#39; HotStuff<br><br>Asynchronous Distributed Key Generation with optimal resilience (f&lt;n/3), O(n^3) words and O(1) expected time <a href="https://t.co/oQdszxPze2">https://t.co/oQdszxPze2</a><br><br>new work from lead author Gilad Stern with <a href="https://twitter.com/Daeinar?ref_src=twsrc%5Etfw">@Daeinar</a>, Mary Maller, Sarah Meiklejohn, and <a href="https://twitter.com/alinush407?ref_src=twsrc%5Etfw">@alinush407</a></p>&mdash; Ittai Abraham (@ittaia) <a href="https://twitter.com/ittaia/status/1362710878785839107?ref_src=twsrc%5Etfw">February 19, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize  Long-Tail Visual Concepts

Soravit Changpinyo, Piyush Sharma, Nan Ding, Radu Soricut

- retweets: 64, favorites: 40 (02/20/2021 09:36:07)

- links: [abs](https://arxiv.org/abs/2102.08981) | [pdf](https://arxiv.org/pdf/2102.08981)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.CL](https://arxiv.org/list/cs.CL/recent)

The availability of large-scale image captioning and visual question answering datasets has contributed significantly to recent successes in vision-and-language pre-training. However, these datasets are often collected with overrestrictive requirements, inherited from their original target tasks (e.g., image caption generation), which limit the resulting dataset scale and diversity. We take a step further in pushing the limits of vision-and-language pre-training data by relaxing the data collection pipeline used in Conceptual Captions 3M (CC3M) [Sharma et al. 2018] and introduce the Conceptual 12M (CC12M), a dataset with 12 million image-text pairs specifically meant to be used for vision-and-language pre-training. We perform an analysis of this dataset, as well as benchmark its effectiveness against CC3M on multiple downstream tasks with an emphasis on long-tail visual recognition. The quantitative and qualitative results clearly illustrate the benefit of scaling up pre-training data for vision-and-language tasks, as indicated by the new state-of-the-art results on both the nocaps and Conceptual Captions benchmarks.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts<br>pdf: <a href="https://t.co/5OzAiywQKE">https://t.co/5OzAiywQKE</a><br>abs: <a href="https://t.co/DSg5ZGtGE2">https://t.co/DSg5ZGtGE2</a> <a href="https://t.co/AP1iUA9Yhf">pic.twitter.com/AP1iUA9Yhf</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1362596321400672259?ref_src=twsrc%5Etfw">February 19, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 10. Is preprint the future of science? A thirty year journey of online  preprint services

Boya Xie, Zhihong Shen, Kuansan Wang

- retweets: 90, favorites: 14 (02/20/2021 09:36:08)

- links: [abs](https://arxiv.org/abs/2102.09066) | [pdf](https://arxiv.org/pdf/2102.09066)
- [cs.DL](https://arxiv.org/list/cs.DL/recent) | [cs.CY](https://arxiv.org/list/cs.CY/recent)

Preprint is a version of a scientific paper that is publicly distributed preceding formal peer review. Since the launch of arXiv in 1991, preprints have been increasingly distributed over the Internet as opposed to paper copies. It allows open online access to disseminate the original research within a few days, often at a very low operating cost. This work overviews how preprint has been evolving and impacting the research community over the past thirty years alongside the growth of the Web. In this work, we first report that the number of preprints has exponentially increased 63 times in 30 years, although it only accounts for 4% of research articles. Second, we quantify the benefits that preprints bring to authors: preprints reach an audience 14 months earlier on average and associate with five times more citations compared with a non-preprint counterpart. Last, to address the quality concern of preprints, we discover that 41% of preprints are ultimately published at a peer-reviewed destination, and the published venues are as influential as papers without a preprint version. Additionally, we discuss the unprecedented role of preprints in communicating the latest research data during recent public health emergencies. In conclusion, we provide quantitative evidence to unveil the positive impact of preprints on individual researchers and the community. Preprints make scholarly communication more efficient by disseminating scientific discoveries more rapidly and widely with the aid of Web technologies. The measurements we present in this study can help researchers and policymakers make informed decisions about how to effectively use and responsibly embrace a preprint culture.




# 11. Going Full-TILT Boogie on Document Understanding with Text-Image-Layout  Transformer

Rafa≈Ç Powalski, ≈Åukasz Borchmann, Dawid Jurkiewicz, Tomasz Dwojak, Micha≈Ç Pietruszka, Gabriela Pa≈Çka

- retweets: 26, favorites: 61 (02/20/2021 09:36:08)

- links: [abs](https://arxiv.org/abs/2102.09550) | [pdf](https://arxiv.org/pdf/2102.09550)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

We address the challenging problem of Natural Language Comprehension beyond plain-text documents by introducing the TILT neural network architecture which simultaneously learns layout information, visual features, and textual semantics. Contrary to previous approaches, we rely on a decoder capable of solving all problems involving natural language. The layout is represented as an attention bias and complemented with contextualized visual information, while the core of our model is a pretrained encoder-decoder Transformer. We trained our network on real-world documents with different layouts, such as tables, figures, and forms. Our novel approach achieves state-of-the-art in extracting information from documents and answering questions, demanding layout understanding (DocVQA, CORD, WikiOps, SROIE). At the same time, we simplify the process by employing an end-to-end model.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Our model, which simultaneously learns layout information, visual features, and textual semantics, achieves state-of-the-art results in extracting data from documents and answering questions, demanding layout understanding! <br><br>Draft available: <a href="https://t.co/ziSKJiVPKi">https://t.co/ziSKJiVPKi</a><a href="https://twitter.com/hashtag/NLProc?src=hash&amp;ref_src=twsrc%5Etfw">#NLProc</a> <a href="https://t.co/tzZGX7vOtF">pic.twitter.com/tzZGX7vOtF</a></p>&mdash; ≈Åukasz Borchmann (@LukaszBorchmann) <a href="https://twitter.com/LukaszBorchmann/status/1362835585744986114?ref_src=twsrc%5Etfw">February 19, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer<br>pdf: <a href="https://t.co/LLKPeCLbrW">https://t.co/LLKPeCLbrW</a><br>abs: <a href="https://t.co/SQKYSd5Ma2">https://t.co/SQKYSd5Ma2</a> <a href="https://t.co/HvkyWTJWkd">pic.twitter.com/HvkyWTJWkd</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1362614227618459648?ref_src=twsrc%5Etfw">February 19, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 12. Clockwork Variational Autoencoders for Video Prediction

Vaibhav Saxena, Jimmy Ba, Danijar Hafner

- retweets: 42, favorites: 39 (02/20/2021 09:36:08)

- links: [abs](https://arxiv.org/abs/2102.09532) | [pdf](https://arxiv.org/pdf/2102.09532)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Deep learning has enabled algorithms to generate realistic images. However, accurately predicting long video sequences requires understanding long-term dependencies and remains an open challenge. While existing video prediction models succeed at generating sharp images, they tend to fail at accurately predicting far into the future. We introduce the Clockwork VAE (CW-VAE), a video prediction model that leverages a hierarchy of latent sequences, where higher levels tick at slower intervals. We demonstrate the benefits of both hierarchical latents and temporal abstraction on 4 diverse video prediction datasets with sequences of up to 1000 frames, where CW-VAE outperforms top video prediction models. Additionally, we propose a Minecraft benchmark for long-term video prediction. We conduct several experiments to gain insights into CW-VAE and confirm that slower levels learn to represent objects that change more slowly in the video, and faster levels learn to represent faster objects.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Clockwork Variational Autoencoders for Video Prediction<br>pdf: <a href="https://t.co/91vDORVpuZ">https://t.co/91vDORVpuZ</a><br>abs: <a href="https://t.co/XwuIeK0309">https://t.co/XwuIeK0309</a> <a href="https://t.co/13xc6ZRAVt">pic.twitter.com/13xc6ZRAVt</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1362605632243834888?ref_src=twsrc%5Etfw">February 19, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 13. DINO: A Conditional Energy-Based GAN for Domain Translation

Konstantinos Vougioukas, Stavros Petridis, Maja Pantic

- retweets: 42, favorites: 38 (02/20/2021 09:36:08)

- links: [abs](https://arxiv.org/abs/2102.09281) | [pdf](https://arxiv.org/pdf/2102.09281)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.SD](https://arxiv.org/list/cs.SD/recent) | [eess.AS](https://arxiv.org/list/eess.AS/recent)

Domain translation is the process of transforming data from one domain to another while preserving the common semantics. Some of the most popular domain translation systems are based on conditional generative adversarial networks, which use source domain data to drive the generator and as an input to the discriminator. However, this approach does not enforce the preservation of shared semantics since the conditional input can often be ignored by the discriminator. We propose an alternative method for conditioning and present a new framework, where two networks are simultaneously trained, in a supervised manner, to perform domain translation in opposite directions. Our method is not only better at capturing the shared information between two domains but is more generic and can be applied to a broader range of problems. The proposed framework performs well even in challenging cross-modal translations, such as video-driven speech reconstruction, for which other systems struggle to maintain correspondence.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">DINO: A Conditional Energy-Based GAN for Domain Translation<br>pdf: <a href="https://t.co/rgyOigOVQb">https://t.co/rgyOigOVQb</a><br>abs: <a href="https://t.co/dDI5zH9kLo">https://t.co/dDI5zH9kLo</a> <a href="https://t.co/HviLLk94Ra">pic.twitter.com/HviLLk94Ra</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1362587422903697410?ref_src=twsrc%5Etfw">February 19, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 14. A Systematic Review of Natural Language Processing Applied to Radiology  Reports

Arlene Casey, Emma Davidson, Michael Poon, Hang Dong, Daniel Duma, Andreas Grivas, Claire Grover, V√≠ctor Su√°rez-Paniagua, Richard Tobin, William Whiteley, Honghan Wu, Beatrice Alex

- retweets: 40, favorites: 15 (02/20/2021 09:36:08)

- links: [abs](https://arxiv.org/abs/2102.09553) | [pdf](https://arxiv.org/pdf/2102.09553)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

NLP has a significant role in advancing healthcare and has been found to be key in extracting structured information from radiology reports. Understanding recent developments in NLP application to radiology is of significance but recent reviews on this are limited. This study systematically assesses recent literature in NLP applied to radiology reports. Our automated literature search yields 4,799 results using automated filtering, metadata enriching steps and citation search combined with manual review. Our analysis is based on 21 variables including radiology characteristics, NLP methodology, performance, study, and clinical application characteristics. We present a comprehensive analysis of the 164 publications retrieved with each categorised into one of 6 clinical application categories. Deep learning use increases but conventional machine learning approaches are still prevalent. Deep learning remains challenged when data is scarce and there is little evidence of adoption into clinical practice. Despite 17% of studies reporting greater than 0.85 F1 scores, it is hard to comparatively evaluate these approaches given that most of them use different datasets. Only 14 studies made their data and 15 their code available with 10 externally validating results. Automated understanding of clinical narratives of the radiology reports has the potential to enhance the healthcare process but reproducibility and explainability of models are important if the domain is to move applications into clinical use. More could be done to share code enabling validation of methods on different institutional data and to reduce heterogeneity in reporting of study properties allowing inter-study comparisons. Our results have significance for researchers providing a systematic synthesis of existing work to build on, identify gaps, opportunities for collaboration and avoid duplication.



