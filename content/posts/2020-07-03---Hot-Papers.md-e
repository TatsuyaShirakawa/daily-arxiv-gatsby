---
title: Hot Papers 2020-07-03
date: 2020-07-04T07:55:21.Z
template: "post"
draft: false
slug: "hot-papers-2020-07-03"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-07-03"
socialImage: "/media/42-line-bible.jpg"

---

# 1. Deep Single Image Manipulation

Yael Vinker, Eliahu Horwitz, Nir Zabari, Yedid Hoshen

- retweets: 27, favorites: 119 (07/04/2020 07:55:21)

- links: [abs](https://arxiv.org/abs/2007.01289) | [pdf](https://arxiv.org/pdf/2007.01289)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Image manipulation has attracted much research over the years due to the popularity and commercial importance of the task. In recent years, deep neural network methods have been proposed for many image manipulation tasks. A major issue with deep methods is the need to train on large amounts of data from the same distribution as the target image, whereas collecting datasets encompassing the entire long-tail of images is impossible. In this paper, we demonstrate that simply training a conditional adversarial generator on the single target image is sufficient for performing complex image manipulations. We find that the key for enabling single image training is extensive augmentation of the input image and provide a novel augmentation method. Our network learns to map between a primitive representation of the image (e.g. edges) to the image itself. At manipulation time, our generator allows for making general image changes by modifying the primitive input representation and mapping it through the network. We extensively evaluate our method and find that it provides remarkable performance.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Deep Single Image Manipulation<br>pdf: <a href="https://t.co/v5hYKYjTw5">https://t.co/v5hYKYjTw5</a><br>abs: <a href="https://t.co/5Lk9UIWzk4">https://t.co/5Lk9UIWzk4</a><br>project page: <a href="https://t.co/etX9ikl0h6">https://t.co/etX9ikl0h6</a> <a href="https://t.co/xIQhdpjEje">pic.twitter.com/xIQhdpjEje</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1278861621432070150?ref_src=twsrc%5Etfw">July 3, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. ReXNet: Diminishing Representational Bottleneck on Convolutional Neural  Network

Dongyoon Han, Sangdoo Yun, Byeongho Heo, YoungJoon Yoo

- retweets: 20, favorites: 62 (07/04/2020 07:55:21)

- links: [abs](https://arxiv.org/abs/2007.00992) | [pdf](https://arxiv.org/pdf/2007.00992)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

This paper addresses representational bottleneck in a network and propose a set of design principles that improves model performance significantly. We argue that a representational bottleneck may happen in a network designed by a conventional design and results in degrading the model performance. To investigate the representational bottleneck, we study the matrix rank of the features generated by ten thousand random networks. We further study the entire layer's channel configuration towards designing more accurate network architectures. Based on the investigation, we propose simple yet effective design principles to mitigate the representational bottleneck. Slight changes on baseline networks by following the principle leads to achieving remarkable performance improvements on ImageNet classification. Additionally, COCO object detection results and transfer learning results on several datasets provide other backups of the link between diminishing representational bottleneck of a network and improving performance. Code and pretrained models are available at https://github.com/clovaai/rexnet.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Happy to announce that <a href="https://twitter.com/hashtag/rexnet?src=hash&amp;ref_src=twsrc%5Etfw">#rexnet</a>, a new lightweight image backbone of <a href="https://twitter.com/hashtag/ClovaAI?src=hash&amp;ref_src=twsrc%5Etfw">#ClovaAI</a>, was released with the official pytorch code and some pretrained models. Please check it out!<br><br>Paper: <a href="https://t.co/E5gUhtnjBo">https://t.co/E5gUhtnjBo</a><br>Github: <a href="https://t.co/PcdFXOVeSl">https://t.co/PcdFXOVeSl</a></p>&mdash; Jung-Woo Ha (@JungWooHa2) <a href="https://twitter.com/JungWooHa2/status/1278957394798579713?ref_src=twsrc%5Etfw">July 3, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Processing South Asian Languages Written in the Latin Script: the  Dakshina Dataset

Brian Roark, Lawrence Wolf-Sonkin, Christo Kirov, Sabrina J. Mielke, Cibu Johny, Isin Demirsahin, Keith Hall

- retweets: 4, favorites: 67 (07/04/2020 07:55:21)

- links: [abs](https://arxiv.org/abs/2007.01176) | [pdf](https://arxiv.org/pdf/2007.01176)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

This paper describes the Dakshina dataset, a new resource consisting of text in both the Latin and native scripts for 12 South Asian languages. The dataset includes, for each language: 1) native script Wikipedia text; 2) a romanization lexicon; and 3) full sentence parallel data in both a native script of the language and the basic Latin alphabet. We document the methods used for preparation and selection of the Wikipedia text in each language; collection of attested romanizations for sampled lexicons; and manual romanization of held-out sentences from the native script collections. We additionally provide baseline results on several tasks made possible by the dataset, including single word transliteration, full sentence transliteration, and language modeling of native script and romanized text. Keywords: romanization, transliteration, South Asian languages

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Proud to have worked on this last summer <a href="https://twitter.com/GoogleAI?ref_src=twsrc%5Etfw">@GoogleAI</a>:<br><br>&quot;Dakshina:&quot; 2GB of transliterated sentences &amp; transliteration lexica (preserving task-inherent ambiguity, giving you frequencies for variants!) in 12 South Asian languages!<br><br>➡️ <a href="https://t.co/iPdCwZtFt5">https://t.co/iPdCwZtFt5</a><br>➡️ <a href="https://t.co/SgnyvfhcXX">https://t.co/SgnyvfhcXX</a> <a href="https://t.co/tJKxYrkwl8">https://t.co/tJKxYrkwl8</a></p>&mdash; Sabrina J. Mielke (@sjmielke) <a href="https://twitter.com/sjmielke/status/1278850672671236097?ref_src=twsrc%5Etfw">July 3, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Can We Achieve More with Less? Exploring Data Augmentation for Toxic  Comment Classification

Chetanya Rastogi, Nikka Mofid, Fang-I Hsiao

- retweets: 7, favorites: 52 (07/04/2020 07:55:22)

- links: [abs](https://arxiv.org/abs/2007.00875) | [pdf](https://arxiv.org/pdf/2007.00875)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.IR](https://arxiv.org/list/cs.IR/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

This paper tackles one of the greatest limitations in Machine Learning: Data Scarcity. Specifically, we explore whether high accuracy classifiers can be built from small datasets, utilizing a combination of data augmentation techniques and machine learning algorithms. In this paper, we experiment with Easy Data Augmentation (EDA) and Backtranslation, as well as with three popular learning algorithms, Logistic Regression, Support Vector Machine (SVM), and Bidirectional Long Short-Term Memory Network (Bi-LSTM). For our experimentation, we utilize the Wikipedia Toxic Comments dataset so that in the process of exploring the benefits of data augmentation, we can develop a model to detect and classify toxic speech in comments to help fight back against cyberbullying and online harassment. Ultimately, we found that data augmentation techniques can be used to significantly boost the performance of classifiers and are an excellent strategy to combat lack of data in NLP problems.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">文書分類タスクにおけるデータ水増しの効果を検証している研究。アルゴリズム(LR, SVM, Bi-LSTM)×水増し手法の組み合わせを実験している。<br><br>Can We Achieve More with Less? Exploring Data Augmentation for Toxic Comment Classification<a href="https://t.co/VJMsUoD0Ac">https://t.co/VJMsUoD0Ac</a></p>&mdash; u++ (@upura0) <a href="https://twitter.com/upura0/status/1278925074481901569?ref_src=twsrc%5Etfw">July 3, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Leveraging Passage Retrieval with Generative Models for Open Domain  Question Answering

Gautier Izacard, Edouard Grave

- retweets: 9, favorites: 46 (07/04/2020 07:55:22)

- links: [abs](https://arxiv.org/abs/2007.01282) | [pdf](https://arxiv.org/pdf/2007.01282)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that generative models are good at aggregating and combining evidence from multiple passages.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">New work w/ <a href="https://twitter.com/gizacard?ref_src=twsrc%5Etfw">@gizacard</a> (Gautier Izacard): how much do generative models for open domain QA benefit from retrieval? A lot! Retrieving 100 passages, we get 51.4 EM on NaturalQuestions, 67.6 EM on TriviaQA. 1/3<br>Paper: <a href="https://t.co/ftRwYSry1R">https://t.co/ftRwYSry1R</a> <a href="https://t.co/vyr7cBqoUs">pic.twitter.com/vyr7cBqoUs</a></p>&mdash; Edouard Grave (@EXGRV) <a href="https://twitter.com/EXGRV/status/1279017657178763270?ref_src=twsrc%5Etfw">July 3, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



