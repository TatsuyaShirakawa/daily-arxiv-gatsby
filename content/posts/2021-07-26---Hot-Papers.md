---
title: Hot Papers 2021-07-26
date: 2021-07-27T11:56:02.Z
template: "post"
draft: false
slug: "hot-papers-2021-07-26"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2021-07-26"
socialImage: "/media/flying-marine.jpg"

---

# 1. RewriteNet: Realistic Scene Text Image Generation via Editing Text in  Real-world Image

Junyeop Lee, Yoonsik Kim, Seonghyeon Kim, Moonbin Yim, Seung Shin, Gayoung Lee, Sungrae Park

- retweets: 754, favorites: 147 (07/27/2021 11:56:02)

- links: [abs](https://arxiv.org/abs/2107.11041) | [pdf](https://arxiv.org/pdf/2107.11041)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Scene text editing (STE), which converts a text in a scene image into the desired text while preserving an original style, is a challenging task due to a complex intervention between text and style. To address this challenge, we propose a novel representational learning-based STE model, referred to as RewriteNet that employs textual information as well as visual information. We assume that the scene text image can be decomposed into content and style features where the former represents the text information and style represents scene text characteristics such as font, alignment, and background. Under this assumption, we propose a method to separately encode content and style features of the input image by introducing the scene text recognizer that is trained by text information. Then, a text-edited image is generated by combining the style feature from the original image and the content feature from the target text. Unlike previous works that are only able to use synthetic images in the training phase, we also exploit real-world images by proposing a self-supervised training scheme, which bridges the domain gap between synthetic and real data. Our experiments demonstrate that RewriteNet achieves better quantitative and qualitative performance than other comparisons. Moreover, we validate that the use of text information and the self-supervised training scheme improves text switching performance. The implementation and dataset will be publicly available.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">RewriteNet: Realistic Scene Text Image Generation via Editing Text in Real-world Image<br>pdf: <a href="https://t.co/zMzJ5A2MoL">https://t.co/zMzJ5A2MoL</a><br>abs: <a href="https://t.co/irru1jvj2e">https://t.co/irru1jvj2e</a> <a href="https://t.co/Cp6YwrUNd0">pic.twitter.com/Cp6YwrUNd0</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1419460645016195077?ref_src=twsrc%5Etfw">July 26, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. LARGE: Latent-Based Regression through GAN Semantics

Yotam Nitzan, Rinon Gal, Ofir Brenner, Daniel Cohen-Or

- retweets: 91, favorites: 46 (07/27/2021 11:56:02)

- links: [abs](https://arxiv.org/abs/2107.11186) | [pdf](https://arxiv.org/pdf/2107.11186)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.GR](https://arxiv.org/list/cs.GR/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

We propose a novel method for solving regression tasks using few-shot or weak supervision. At the core of our method is the fundamental observation that GANs are incredibly successful at encoding semantic information within their latent space, even in a completely unsupervised setting. For modern generative frameworks, this semantic encoding manifests as smooth, linear directions which affect image attributes in a disentangled manner. These directions have been widely used in GAN-based image editing. We show that such directions are not only linear, but that the magnitude of change induced on the respective attribute is approximately linear with respect to the distance traveled along them. By leveraging this observation, our method turns a pre-trained GAN into a regression model, using as few as two labeled samples. This enables solving regression tasks on datasets and attributes which are difficult to produce quality supervision for. Additionally, we show that the same latent-distances can be used to sort collections of images by the strength of given attributes, even in the absence of explicit supervision. Extensive experimental evaluations demonstrate that our method can be applied across a wide range of domains, leverage multiple latent direction discovery frameworks, and achieve state-of-the-art results in few-shot and low-supervision settings, even when compared to methods designed to tackle a single task.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">LARGE: Latent-Based Regression through GAN Semantics<br>pdf: <a href="https://t.co/tv45QSPzoD">https://t.co/tv45QSPzoD</a><br>abs: <a href="https://t.co/jmTb1OSFxS">https://t.co/jmTb1OSFxS</a><br>github: <a href="https://t.co/sAB98GkP2r">https://t.co/sAB98GkP2r</a><br><br>a method for leveraging implicit knowledge of a generative model for regression tasks <a href="https://t.co/1Tjt3BNR3L">pic.twitter.com/1Tjt3BNR3L</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1419458946474070020?ref_src=twsrc%5Etfw">July 26, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Modelling Latent Translations for Cross-Lingual Transfer

Edoardo Maria Ponti, Julia Kreutzer, Ivan VuliÄ‡, Siva Reddy

- retweets: 60, favorites: 54 (07/27/2021 11:56:03)

- links: [abs](https://arxiv.org/abs/2107.11353) | [pdf](https://arxiv.org/pdf/2107.11353)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

While achieving state-of-the-art results in multiple tasks and languages, translation-based cross-lingual transfer is often overlooked in favour of massively multilingual pre-trained encoders. Arguably, this is due to its main limitations: 1) translation errors percolating to the classification phase and 2) the insufficient expressiveness of the maximum-likelihood translation. To remedy this, we propose a new technique that integrates both steps of the traditional pipeline (translation and classification) into a single model, by treating the intermediate translations as a latent random variable. As a result, 1) the neural machine translation system can be fine-tuned with a variant of Minimum Risk Training where the reward is the accuracy of the downstream task classifier. Moreover, 2) multiple samples can be drawn to approximate the expected loss across all possible translations during inference. We evaluate our novel latent translation-based model on a series of multilingual NLU tasks, including commonsense reasoning, paraphrase identification, and natural language inference. We report gains for both zero-shot and few-shot learning setups, up to 2.7 accuracy points on average, which are even more prominent for low-resource languages (e.g., Haitian Creole). Finally, we carry out in-depth analyses comparing different underlying NMT models and assessing the impact of alternative translations on the downstream performance.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">In our new paper, <a href="https://twitter.com/KreutzerJulia?ref_src=twsrc%5Etfw">@KreutzerJulia</a> <a href="https://twitter.com/licwu?ref_src=twsrc%5Etfw">@licwu</a> <a href="https://twitter.com/sivareddyg?ref_src=twsrc%5Etfw">@sivareddyg</a>   and I present a method to enhance translation-based cross-lingual transfer (gains up to 2.7 per task and 5.6 per language). Pdf: <a href="https://t.co/iiG3joGn8A">https://t.co/iiG3joGn8A</a>. Code: <a href="https://t.co/lOJKVPJu2C">https://t.co/lOJKVPJu2C</a> <a href="https://twitter.com/Mila_Quebec?ref_src=twsrc%5Etfw">@Mila_Quebec</a> <a href="https://twitter.com/CambridgeLTL?ref_src=twsrc%5Etfw">@CambridgeLTL</a> <a href="https://twitter.com/GoogleAI?ref_src=twsrc%5Etfw">@GoogleAI</a></p>&mdash; Edoardo Ponti (@PontiEdoardo) <a href="https://twitter.com/PontiEdoardo/status/1419645116638183424?ref_src=twsrc%5Etfw">July 26, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



