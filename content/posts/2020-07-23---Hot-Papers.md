---
title: Hot Papers 2020-07-23
date: 2020-07-24T21:17:58.Z
template: "post"
draft: false
slug: "hot-papers-2020-07-23"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-07-23"
socialImage: "/media/42-line-bible.jpg"

---

# 1. DeepSVG: A Hierarchical Generative Network for Vector Graphics Animation

Alexandre Carlier, Martin Danelljan, Alexandre Alahi, Radu Timofte

- retweets: 93, favorites: 422 (07/24/2020 21:17:58)

- links: [abs](https://arxiv.org/abs/2007.11301) | [pdf](https://arxiv.org/pdf/2007.11301)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Scalable Vector Graphics (SVG) are ubiquitous in modern 2D interfaces due to their ability to scale to different resolutions. However, despite the success of deep learning-based models applied to rasterized images, the problem of vector graphics representation learning and generation remains largely unexplored. In this work, we propose a novel hierarchical generative network, called DeepSVG, for complex SVG icons generation and interpolation. Our architecture effectively disentangles high-level shapes from the low-level commands that encode the shape itself. The network directly predicts a set of shapes in a non-autoregressive fashion. We introduce the task of complex SVG icons generation by releasing a new large-scale dataset along with an open-source library for SVG manipulation. We demonstrate that our network learns to accurately reconstruct diverse vector graphics, and can serve as a powerful animation tool by performing interpolations and other latent space operations. Our code is available at https://github.com/alexandre01/deepsvg.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">DeepSVG: A Hierarchical Generative Network for Vector Graphics Animation<br><br>Exciting work from <a href="https://twitter.com/alxandrecarlier?ref_src=twsrc%5Etfw">@alxandrecarlier</a> et al. Transformer-based hierarchical generative models learn latent representations of vector graphics, with nice applications in SVG animation.<a href="https://t.co/2FBICYu6NM">https://t.co/2FBICYu6NM</a> <a href="https://t.co/gLNMaTLAYP">https://t.co/gLNMaTLAYP</a> <a href="https://t.co/ooD0rYvVl3">pic.twitter.com/ooD0rYvVl3</a></p>&mdash; hardmaru (@hardmaru) <a href="https://twitter.com/hardmaru/status/1286120490570903552?ref_src=twsrc%5Etfw">July 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">DeepSVG: A Hierarchical Generative Network for Vector Graphics Animation<br>pdf: <a href="https://t.co/Q4X0WnvXPB">https://t.co/Q4X0WnvXPB</a><br>abs: <a href="https://t.co/9kbvgeOBBI">https://t.co/9kbvgeOBBI</a> <a href="https://t.co/2moB2SHmFX">pic.twitter.com/2moB2SHmFX</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1286108909913153541?ref_src=twsrc%5Etfw">July 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. CrossTransformers: spatially-aware few-shot transfer

Carl Doersch, Ankush Gupta, Andrew Zisserman

- retweets: 45, favorites: 212 (07/24/2020 21:17:59)

- links: [abs](https://arxiv.org/abs/2007.11498) | [pdf](https://arxiv.org/pdf/2007.11498)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Given new tasks with very little data--such as new classes in a classification problem or a domain shift in the input--performance of modern vision systems degrades remarkably quickly. In this work, we illustrate how the neural network representations which underpin modern vision systems are subject to supervision collapse, whereby they lose any information that is not necessary for performing the training task, including information that may be necessary for transfer to new tasks or domains. We then propose two methods to mitigate this problem. First, we employ self-supervised learning to encourage general-purpose features that transfer better. Second, we propose a novel Transformer based neural network architecture called CrossTransformers, which can take a small number of labeled images and an unlabeled query, find coarse spatial correspondence between the query and the labeled images, and then infer class membership by computing distances between spatially-corresponding features. The result is a classifier that is more robust to task and domain shift, which we demonstrate via state-of-the-art performance on Meta-Dataset, a recent dataset for evaluating transfer from ImageNet to many other vision datasets.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">We present CrossTransformers: a new SOTA for few-shot recognition/transfer on Meta-Dataset. Self-supervised learning and spatial correspondence helps represent new categories using parts of familiar ones. With Ankush Gupta and Andrew Zisserman. <a href="https://t.co/hIyH5jDjMl">https://t.co/hIyH5jDjMl</a> <a href="https://t.co/QJ8ruaY355">pic.twitter.com/QJ8ruaY355</a></p>&mdash; Carl Doersch (@CarlDoersch) <a href="https://twitter.com/CarlDoersch/status/1286285306782781444?ref_src=twsrc%5Etfw">July 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Unsupervised Shape and Pose Disentanglement for 3D Meshes

Keyang Zhou, Bharat Lal Bhatnagar, Gerard Pons-Moll

- retweets: 38, favorites: 195 (07/24/2020 21:17:59)

- links: [abs](https://arxiv.org/abs/2007.11341) | [pdf](https://arxiv.org/pdf/2007.11341)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Parametric models of humans, faces, hands and animals have been widely used for a range of tasks such as image-based reconstruction, shape correspondence estimation, and animation. Their key strength is the ability to factor surface variations into shape and pose dependent components. Learning such models requires lots of expert knowledge and hand-defined object-specific constraints, making the learning approach unscalable to novel objects. In this paper, we present a simple yet effective approach to learn disentangled shape and pose representations in an unsupervised setting. We use a combination of self-consistency and cross-consistency constraints to learn pose and shape space from registered meshes. We additionally incorporate as-rigid-as-possible deformation(ARAP) into the training loop to avoid degenerate solutions. We demonstrate the usefulness of learned representations through a number of tasks including pose transfer and shape retrieval. The experiments on datasets of 3D humans, faces, hands and animals demonstrate the generality of our approach. Code is made available at https://virtualhumans.mpi-inf.mpg.de/unsup_shape_pose/.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Unsupervised Shape and Pose Disentanglement for 3D Meshes<br>pdf: <a href="https://t.co/gkUqGGx5hN">https://t.co/gkUqGGx5hN</a><br>abs: <a href="https://t.co/hAsm30WZFJ">https://t.co/hAsm30WZFJ</a><br>project page: <a href="https://t.co/MZmGHX1VaP">https://t.co/MZmGHX1VaP</a> <a href="https://t.co/YVhN7VVSWU">pic.twitter.com/YVhN7VVSWU</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1286098753431187457?ref_src=twsrc%5Etfw">July 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Neural Sparse Voxel Fields

Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, Christian Theobalt

- retweets: 28, favorites: 152 (07/24/2020 21:17:59)

- links: [abs](https://arxiv.org/abs/2007.11571) | [pdf](https://arxiv.org/pdf/2007.11571)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.GR](https://arxiv.org/list/cs.GR/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Photo-realistic free-viewpoint rendering of real-world scenes using classical computer graphics techniques is challenging, because it requires the difficult step of capturing detailed appearance and geometry models. Recent studies have demonstrated promising results by learning scene representations that implicitly encode both geometry and appearance without 3D supervision. However, existing approaches in practice often show blurry renderings caused by the limited network capacity or the difficulty in finding accurate intersections of camera rays with the scene geometry. Synthesizing high-resolution imagery from these representations often requires time-consuming optical ray marching. In this work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene representation for fast and high-quality free-viewpoint rendering. NSVF defines a set of voxel-bounded implicit fields organized in a sparse voxel octree to model local properties in each cell. We progressively learn the underlying voxel structures with a diffentiable ray-marching operation from only a set of posed RGB images. With the sparse voxel octree structure, rendering novel views can be accelerated by skipping the voxels containing no relevant scene content. Our method is over 10 times faster than the state-of-the-art (namely, NeRF) at inference time while achieving higher quality results. Furthermore, by utilizing an explicit sparse voxel representation, our method can easily be applied to scene editing and scene composition. We also demonstrate several challenging tasks, including multi-scene learning, free-viewpoint rendering of a moving human, and large-scale scene rendering.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Neural Sparse Voxel Fields. Cool new work that extends the NeRF idea; check it out if you’re interested novel ways of representing space. <a href="https://t.co/3bLkU7X8ty">https://t.co/3bLkU7X8ty</a> <a href="https://twitter.com/hashtag/computervision?src=hash&amp;ref_src=twsrc%5Etfw">#computervision</a> <a href="https://twitter.com/hashtag/graphics?src=hash&amp;ref_src=twsrc%5Etfw">#graphics</a> <a href="https://t.co/Ckdt8XzZPD">pic.twitter.com/Ckdt8XzZPD</a></p>&mdash; Tomasz Malisiewicz (@quantombone) <a href="https://twitter.com/quantombone/status/1286110528272621569?ref_src=twsrc%5Etfw">July 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="de" dir="ltr">Neural Sparse Voxel Fields<br>pdf: <a href="https://t.co/PTDwOQpE2A">https://t.co/PTDwOQpE2A</a><br>abs: <a href="https://t.co/xRMsnSCOoZ">https://t.co/xRMsnSCOoZ</a> <a href="https://t.co/6yBMJkV2nR">pic.twitter.com/6yBMJkV2nR</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1286105324957642754?ref_src=twsrc%5Etfw">July 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Coarse Graining Molecular Dynamics with Graph Neural Networks

Brooke E. Husic, Nicholas E. Charron, Dominik Lemm, Jiang Wang, Adrià Pérez, Andreas Krämer, Yaoyi Chen, Simon Olsson, Gianni de Fabritiis, Frank Noé, Cecilia Clementi

- retweets: 25, favorites: 136 (07/24/2020 21:17:59)

- links: [abs](https://arxiv.org/abs/2007.11412) | [pdf](https://arxiv.org/pdf/2007.11412)
- [physics.comp-ph](https://arxiv.org/list/physics.comp-ph/recent) | [physics.bio-ph](https://arxiv.org/list/physics.bio-ph/recent) | [physics.chem-ph](https://arxiv.org/list/physics.chem-ph/recent) | [q-bio.BM](https://arxiv.org/list/q-bio.BM/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Coarse graining enables the investigation of molecular dynamics for larger systems and at longer timescales than is possible at atomic resolution. However, a coarse graining model must be formulated such that the conclusions we draw from it are consistent with the conclusions we would draw from a model at a finer level of detail. It has been proven that a force matching scheme defines a thermodynamically consistent coarse-grained model for an atomistic system in the variational limit. Wang et al. [ACS Cent. Sci. 5, 755 (2019)] demonstrated that the existence of such a variational limit enables the use of a supervised machine learning framework to generate a coarse-grained force field, which can then be used for simulation in the coarse-grained space. Their framework, however, requires the manual input of molecular features upon which to machine learn the force field. In the present contribution, we build upon the advance of Wang et al.and introduce a hybrid architecture for the machine learning of coarse-grained force fields that learns their own features via a subnetwork that leverages continuous filter convolutions on a graph neural network architecture. We demonstrate that this framework succeeds at reproducing the thermodynamics for small biomolecular systems. Since the learned molecular representations are inherently transferable, the architecture presented here sets the stage for the development of machine-learned, coarse-grained force fields that are transferable across molecular systems.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Learning coarse-grained molecular dynamics with graph neural networks. Led by <a href="https://twitter.com/brookehus?ref_src=twsrc%5Etfw">@brookehus</a>, Nick Charron and <a href="https://twitter.com/CecClementi?ref_src=twsrc%5Etfw">@CecClementi</a> <a href="https://t.co/XsMye2IjEV">https://t.co/XsMye2IjEV</a></p>&mdash; Frank Noe (@FrankNoeBerlin) <a href="https://twitter.com/FrankNoeBerlin/status/1286425481865244673?ref_src=twsrc%5Etfw">July 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. Exploratory Search with Sentence Embeddings

Austin Silveria

- retweets: 15, favorites: 73 (07/24/2020 21:18:00)

- links: [abs](https://arxiv.org/abs/2007.11198) | [pdf](https://arxiv.org/pdf/2007.11198)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.IR](https://arxiv.org/list/cs.IR/recent)

Exploratory search aims to guide users through a corpus rather than pinpointing exact information. We propose an exploratory search system based on hierarchical clusters and document summaries using sentence embeddings. With sentence embeddings, we represent documents as the mean of their embedded sentences, extract summaries containing sentences close to this document representation and extract keyphrases close to the document representation. To evaluate our search system, we scrape our personal search history over the past year and report our experience with the system. We then discuss motivating use cases of an exploratory search system of this nature and conclude with possible directions of future work.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">This work could help provide some ideas on how to build simple exploratory search systems using sentence embeddings, text summarization, and keyphrase extraction.<a href="https://t.co/SCh8aONIK3">https://t.co/SCh8aONIK3</a> <a href="https://t.co/jzWAzFMZzM">pic.twitter.com/jzWAzFMZzM</a></p>&mdash; elvis (@omarsar0) <a href="https://twitter.com/omarsar0/status/1286200409401548802?ref_src=twsrc%5Etfw">July 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. Online Monitoring of Global Attitudes Towards Wildlife

Joss Wright, Robert Lennox, Diogo Veríssimo

- retweets: 19, favorites: 63 (07/24/2020 21:18:00)

- links: [abs](https://arxiv.org/abs/2007.11506) | [pdf](https://arxiv.org/pdf/2007.11506)
- [cs.CY](https://arxiv.org/list/cs.CY/recent)

Human factors are increasingly recognised as central to conservation of biodiversity. Despite this, there are no existing systematic efforts to monitor global trends in perceptions of wildlife. With traditional news reporting now largely online, the internet presents a powerful means to monitor global attitudes towards species. In this work we develop a method using the Global Database of Events, Language, and Tone (GDELT) to scan global news media, allowing us to identify and download conservation-related articles. Applying supervised machine learning techniques, we filter irrelevant articles to create a continually updated global dataset of news coverage for seven target taxa: lion, tiger, saiga, rhinoceros, pangolins, elephants, and orchids, and observe that over two-thirds of articles matching a simple keyword search were irrelevant. We examine coverage of each taxa in different regions, and find that elephants, rhinos, tigers, and lions receive the most coverage, with daily peaks of around 200 articles. Mean sentiment was positive for all taxa, except saiga for which it was neutral. Coverage was broadly distributed, with articles from 73 countries across all continents. Elephants and tigers received coverage in the most countries overall, whilst orchids and saiga were mentioned in the smallest number of countries. We further find that sentiment towards charismatic megafauna is most positive in non-range countries, with the opposite being true for pangolins and orchids. Despite promising results, there remain substantial obstacles to achieving globally representative results. Disparities in internet access between low and high income countries and users is a major source of bias, with the need to focus on a diversity of data sources and languages, presenting sizable technical challenges...

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">1/6 Welcome to my <a href="https://twitter.com/hashtag/DICECON20?src=hash&amp;ref_src=twsrc%5Etfw">#DICECON20</a> Presentation entitled:<br><br>“Online Monitoring of Global Attitudes Towards Wildlife” <br><br>I will start by acknowledging my co-authors <a href="https://twitter.com/josswright?ref_src=twsrc%5Etfw">@josswright</a> and <a href="https://twitter.com/FisheriesRobert?ref_src=twsrc%5Etfw">@FisheriesRobert</a> <br><br>And here is PREPRINT <a href="https://t.co/h5ELlxCNnW">https://t.co/h5ELlxCNnW</a> in case you want to follow along! <a href="https://twitter.com/hashtag/HumWild1?src=hash&amp;ref_src=twsrc%5Etfw">#HumWild1</a> <a href="https://t.co/xu7TUE5VmV">pic.twitter.com/xu7TUE5VmV</a></p>&mdash; Diogo Veríssimo (@verissimodiogo) <a href="https://twitter.com/verissimodiogo/status/1286220933125025792?ref_src=twsrc%5Etfw">July 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. Undercutting Bitcoin Is Not Profitable

Tiantian Gong, Mohsen Minaei, Wenhai Sun, Aniket Kate

- retweets: 17, favorites: 47 (07/24/2020 21:18:00)

- links: [abs](https://arxiv.org/abs/2007.11480) | [pdf](https://arxiv.org/pdf/2007.11480)
- [cs.CR](https://arxiv.org/list/cs.CR/recent) | [cs.GT](https://arxiv.org/list/cs.GT/recent)

A fixed block reward and voluntary transaction fees are two sources of economic incentives for mining in Bitcoin and other cryptocurrencies. For Bitcoin, the block reward halves every 210,000 blocks and it is supposed to vanish gradually. The remaining incentive of transaction fees is optional and arbitrary, and an undercutting attack becomes a potential threat, where the attacker deliberately forks an existing chain by leaving wealthy transactions unclaimed to attract other miners. We look into the profitability of the undercutting attack in this work.   Our numerical simulations and experiments demonstrate that (i) only miners with mining power > 40% have a reasonable probability of successfully undercutting. (ii) As honest miners do not shift to the fork immediately in the first round, an undercutter's profit drops with the number of honest miners. Given the current transaction fee rate distribution in Bitcoin, with half of the miners being honest, undercutting cannot be profitable at all; With 25% honest mining power, an undercutter with > 45% mining power can expect income more than its "fair share"; With no honest miners present, the threshold mining power for a profitable undercutting is 42%. (iii) For the current largest Bitcoin mining pool with 17.2% mining power, the probability of successfully launching an undercutting attack is tiny and the expected returns are far below honest mining gains. (iv) While the larger the prize the undercutter left unclaimed, the higher is the probability of the attack succeeding but the attack's profits also go down. Finally, we analyze the best responses to undercutting for other rational miners. (v) For two rational miners and one of them being the potential undercutter with 45% mining power, we find the dominant strategy for the responding rational miner is to typical rational.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">the undercutting attacks on Bitcoin are not profitable as previous thought (check <a href="https://t.co/NI4PSp62ji">https://t.co/NI4PSp62ji</a>)<br>w/ Tiantian Gong, <a href="https://twitter.com/mminaeib?ref_src=twsrc%5Etfw">@mminaeib</a> <a href="https://t.co/eBRrD1lXTN">pic.twitter.com/eBRrD1lXTN</a></p>&mdash; Aniket Kate (@aniketpkate) <a href="https://twitter.com/aniketpkate/status/1286384094427914242?ref_src=twsrc%5Etfw">July 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. IBM Federated Learning: an Enterprise Framework White Paper V0.1

Heiko Ludwig, Nathalie Baracaldo, Gegi Thomas, Yi Zhou, Ali Anwar, Shashank Rajamoni, Yuya Ong, Jayaram Radhakrishnan, Ashish Verma, Mathieu Sinn, Mark Purcell, Ambrish Rawat, Tran Minh, Naoise Holohan, Supriyo Chakraborty, Shalisha Whitherspoon, Dean Steuer, Laura Wynter, Hifaz Hassan, Sean Laguna, Mikhail Yurochkin, Mayank Agarwal, Ebube Chuba, Annie Abay

- retweets: 41, favorites: 22 (07/24/2020 21:18:00)

- links: [abs](https://arxiv.org/abs/2007.10987) | [pdf](https://arxiv.org/pdf/2007.10987)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.CR](https://arxiv.org/list/cs.CR/recent) | [cs.DC](https://arxiv.org/list/cs.DC/recent)

Federated Learning (FL) is an approach to conduct machine learning without centralizing training data in a single place, for reasons of privacy, confidentiality or data volume. However, solving federated machine learning problems raises issues above and beyond those of centralized machine learning. These issues include setting up communication infrastructure between parties, coordinating the learning process, integrating party results, understanding the characteristics of the training data sets of different participating parties, handling data heterogeneity, and operating with the absence of a verification data set.   IBM Federated Learning provides infrastructure and coordination for federated learning. Data scientists can design and run federated learning jobs based on existing, centralized machine learning models and can provide high-level instructions on how to run the federation. The framework applies to both Deep Neural Networks as well as ``traditional'' approaches for the most common machine learning libraries. {\proj} enables data scientists to expand their scope from centralized to federated machine learning, minimizing the learning curve at the outset while also providing the flexibility to deploy to different compute environments and design custom fusion algorithms.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">IBM Federated Learning: an Enterprise Framework White Paper. <a href="https://twitter.com/hashtag/DataScience?src=hash&amp;ref_src=twsrc%5Etfw">#DataScience</a> <a href="https://twitter.com/hashtag/BigData?src=hash&amp;ref_src=twsrc%5Etfw">#BigData</a> <a href="https://twitter.com/hashtag/Analytics?src=hash&amp;ref_src=twsrc%5Etfw">#Analytics</a> <a href="https://twitter.com/hashtag/AI?src=hash&amp;ref_src=twsrc%5Etfw">#AI</a> <a href="https://twitter.com/hashtag/PyTorch?src=hash&amp;ref_src=twsrc%5Etfw">#PyTorch</a> <a href="https://twitter.com/hashtag/Python?src=hash&amp;ref_src=twsrc%5Etfw">#Python</a> <a href="https://twitter.com/hashtag/RStats?src=hash&amp;ref_src=twsrc%5Etfw">#RStats</a> <a href="https://twitter.com/hashtag/TensorFlow?src=hash&amp;ref_src=twsrc%5Etfw">#TensorFlow</a> <a href="https://twitter.com/hashtag/IoT?src=hash&amp;ref_src=twsrc%5Etfw">#IoT</a> <a href="https://twitter.com/hashtag/IIoT?src=hash&amp;ref_src=twsrc%5Etfw">#IIoT</a> <a href="https://twitter.com/hashtag/Java?src=hash&amp;ref_src=twsrc%5Etfw">#Java</a> <a href="https://twitter.com/hashtag/JavaScript?src=hash&amp;ref_src=twsrc%5Etfw">#JavaScript</a> <a href="https://twitter.com/hashtag/ReactJS?src=hash&amp;ref_src=twsrc%5Etfw">#ReactJS</a> <a href="https://twitter.com/hashtag/GoLang?src=hash&amp;ref_src=twsrc%5Etfw">#GoLang</a> <a href="https://twitter.com/hashtag/CloudComputing?src=hash&amp;ref_src=twsrc%5Etfw">#CloudComputing</a> <a href="https://twitter.com/hashtag/Serverless?src=hash&amp;ref_src=twsrc%5Etfw">#Serverless</a> <a href="https://twitter.com/hashtag/Linux?src=hash&amp;ref_src=twsrc%5Etfw">#Linux</a> <a href="https://twitter.com/hashtag/Programming?src=hash&amp;ref_src=twsrc%5Etfw">#Programming</a> <a href="https://twitter.com/hashtag/Coding?src=hash&amp;ref_src=twsrc%5Etfw">#Coding</a> <a href="https://twitter.com/hashtag/MachineLearning?src=hash&amp;ref_src=twsrc%5Etfw">#MachineLearning</a> <a href="https://t.co/fDVzKkt6ss">https://t.co/fDVzKkt6ss</a> <a href="https://t.co/yv2GtWNUFE">pic.twitter.com/yv2GtWNUFE</a></p>&mdash; Marcus Borba (@marcusborba) <a href="https://twitter.com/marcusborba/status/1286426741708333063?ref_src=twsrc%5Etfw">July 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 10. EMaQ: Expected-Max Q-Learning Operator for Simple Yet Effective Offline  and Online RL

Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, Shixiang Shane Gu

- retweets: 11, favorites: 48 (07/24/2020 21:18:00)

- links: [abs](https://arxiv.org/abs/2007.11091) | [pdf](https://arxiv.org/pdf/2007.11091)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Off-policy reinforcement learning (RL) holds the promise of sample-efficient learning of decision-making policies by leveraging past experience. However, in the offline RL setting -- where a fixed collection of interactions are provided and no further interactions are allowed -- it has been shown that standard off-policy RL methods can significantly underperform. Recently proposed methods aim to address this shortcoming by regularizing learned policies to remain close to the given dataset of interactions. However, these methods involve several configurable components such as learning a separate policy network on top of a behavior cloning actor, and explicitly constraining action spaces through clipping or reward penalties. Striving for simultaneous simplicity and performance, in this work we present a novel backup operator, Expected-Max Q-Learning (EMaQ), which naturally restricts learned policies to remain within the support of the offline dataset \emph{without any explicit regularization}, while retaining desirable theoretical properties such as contraction. We demonstrate that EMaQ is competitive with Soft Actor Critic (SAC) in online RL, and surpasses SAC in the deployment-efficient setting. In the offline RL setting -- the main focus of this work -- through EMaQ we are able to make important observations regarding key components of offline RL, and the nature of standard benchmark tasks. Lastly but importantly, we observe that EMaQ achieves state-of-the-art performance with fewer moving parts such as one less function approximation, making it a strong, yet easy to implement baseline for future work.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Use EMaQ! A simple RL algo that works surprisingly well in ALL of online/offline/deployment-constrained settings. It requires one less function approx. than prior offline methods, and is competitive with SAC in online RL. <a href="https://t.co/xv4mWMCBzy">https://t.co/xv4mWMCBzy</a> w/ <a href="https://twitter.com/coolboi95?ref_src=twsrc%5Etfw">@coolboi95</a> Dale <a href="https://twitter.com/GoogleAI?ref_src=twsrc%5Etfw">@GoogleAI</a> 1/ <a href="https://t.co/dQ1lkXSdj1">pic.twitter.com/dQ1lkXSdj1</a></p>&mdash; Shane Gu 顾世翔 (@shaneguML) <a href="https://twitter.com/shaneguML/status/1286531833740460032?ref_src=twsrc%5Etfw">July 24, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 11. Interpolating GANs to Scaffold Autotelic Creativity

Ziv Epstein, Océane Boulais, Skylar Gordon, Matt Groh

- retweets: 14, favorites: 37 (07/24/2020 21:18:01)

- links: [abs](https://arxiv.org/abs/2007.11119) | [pdf](https://arxiv.org/pdf/2007.11119)
- [cs.HC](https://arxiv.org/list/cs.HC/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.MM](https://arxiv.org/list/cs.MM/recent)

The latent space modeled by generative adversarial networks (GANs) represents a large possibility space. By interpolating categories generated by GANs, it is possible to create novel hybrid images. We present "Meet the Ganimals," a casual creator built on interpolations of BigGAN that can generate novel, hybrid animals called ganimals by efficiently searching this possibility space. Like traditional casual creators, the system supports a simple creative flow that encourages rapid exploration of the possibility space. Users can discover new ganimals, create their own, and share their reactions to aesthetic, emotional, and morphological characteristics of the ganimals. As users provide input to the system, the system adapts and changes the distribution of categories upon which ganimals are generated. As one of the first GAN-based casual creators, Meet the Ganimals is an example how casual creators can leverage human curation and citizen science to discover novel artifacts within a large possibility space.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">🎨 ML for creativity 🎨<br><br>This work proposes a way to foster mixed-initiative co-creativity involving creator &amp; system.<br><br>This method allows users to co-create so-called *Ganimals* by interpolating between the categories modeled by a BigGAN model.<a href="https://t.co/s30LyruQ8B">https://t.co/s30LyruQ8B</a> <a href="https://t.co/Q8X4ngKPIK">pic.twitter.com/Q8X4ngKPIK</a></p>&mdash; elvis (@omarsar0) <a href="https://twitter.com/omarsar0/status/1286210465538998272?ref_src=twsrc%5Etfw">July 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



