---
title: Hot Papers 2021-04-01
date: 2021-04-02T10:43:09.Z
template: "post"
draft: false
slug: "hot-papers-2021-04-01"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2021-04-01"
socialImage: "/media/flying-marine.jpg"

---

# 1. StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery

Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, Dani Lischinski

- retweets: 13054, favorites: 29 (04/02/2021 10:43:09)

- links: [abs](https://arxiv.org/abs/2103.17249) | [pdf](https://arxiv.org/pdf/2103.17249)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.GR](https://arxiv.org/list/cs.GR/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Inspired by the ability of StyleGAN to generate highly realistic images in a variety of domains, much recent work has focused on understanding how to use the latent spaces of StyleGAN to manipulate generated and real images. However, discovering semantically meaningful latent manipulations typically involves painstaking human examination of the many degrees of freedom, or an annotated collection of images for each desired manipulation. In this work, we explore leveraging the power of recently introduced Contrastive Language-Image Pre-training (CLIP) models in order to develop a text-based interface for StyleGAN image manipulation that does not require such manual effort. We first introduce an optimization scheme that utilizes a CLIP-based loss to modify an input latent vector in response to a user-provided text prompt. Next, we describe a latent mapper that infers a text-guided latent manipulation step for a given input image, allowing faster and more stable text-based manipulation. Finally, we present a method for mapping a text prompts to input-agnostic directions in StyleGAN's style space, enabling interactive text-driven image manipulation. Extensive results and comparisons demonstrate the effectiveness of our approaches.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery<br>pdf: <a href="https://t.co/20g03cMRYu">https://t.co/20g03cMRYu</a><br>abs: <a href="https://t.co/XN4XjVX7Pi">https://t.co/XN4XjVX7Pi</a><br>github: <a href="https://t.co/6fxI9mDXUN">https://t.co/6fxI9mDXUN</a> <a href="https://t.co/DNRHX3bmtk">pic.twitter.com/DNRHX3bmtk</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1377426610245169153?ref_src=twsrc%5Etfw">April 1, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Going deeper with Image Transformers

Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, Herv√© J√©gou

- retweets: 2932, favorites: 436 (04/02/2021 10:43:09)

- links: [abs](https://arxiv.org/abs/2103.17239) | [pdf](https://arxiv.org/pdf/2103.17239)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Transformers have been recently adapted for large scale image classification, achieving high scores shaking up the long supremacy of convolutional neural networks. However the optimization of image transformers has been little studied so far. In this work, we build and optimize deeper transformer networks for image classification. In particular, we investigate the interplay of architecture and optimization of such dedicated transformers. We make two transformers architecture changes that significantly improve the accuracy of deep transformers. This leads us to produce models whose performance does not saturate early with more depth, for instance we obtain 86.3% top-1 accuracy on Imagenet when training with no external data. Our best model establishes the new state of the art on Imagenet with Reassessed labels and Imagenet-V2 / match frequency, in the setting with no additional training data.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Going deeper with Image Transformers<br><br>Achieves the new SotA on Imagenet benchmarks with a deeper transformer architecture optimized for image classification.<a href="https://t.co/CgfBFYPSAy">https://t.co/CgfBFYPSAy</a> <a href="https://t.co/q8cbEkP0Qv">pic.twitter.com/q8cbEkP0Qv</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1377424713631166466?ref_src=twsrc%5Etfw">April 1, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Going deeper with Image Transformers<br>pdf: <a href="https://t.co/7JhBix6DPh">https://t.co/7JhBix6DPh</a><br>abs: <a href="https://t.co/zaWehUCNMc">https://t.co/zaWehUCNMc</a><br>&quot;Our best model establishes the new state of the art on Imagenet with Reassessed labels and Imagenet-V2 / match frequency, in the setting with no additional training data. <a href="https://t.co/M5IX7A566J">pic.twitter.com/M5IX7A566J</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1377423704154603522?ref_src=twsrc%5Etfw">April 1, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Using Artificial Intelligence to Shed Light on the Star of Biscuits: The  Jaffa Cake

H. F. Stevance

- retweets: 2095, favorites: 141 (04/02/2021 10:43:09)

- links: [abs](https://arxiv.org/abs/2103.16575) | [pdf](https://arxiv.org/pdf/2103.16575)
- [astro-ph.IM](https://arxiv.org/list/astro-ph.IM/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Before Brexit, one of the greatest causes of arguments amongst British families was the question of the nature of Jaffa Cakes. Some argue that their size and host environment (the biscuit aisle) should make them a biscuit in their own right. Others consider that their physical properties (e.g. they harden rather than soften on becoming stale) suggest that they are in fact cake. In order to finally put this debate to rest, we re-purpose technologies used to classify transient events. We train two classifiers (a Random Forest and a Support Vector Machine) on 100 recipes of traditional cakes and biscuits. Our classifiers have 95 percent and 91 percent accuracy respectively. Finally we feed two Jaffa Cake recipes to the algorithms and find that Jaffa Cakes are, without a doubt, cakes. Finally, we suggest a new theory as to why some believe Jaffa Cakes are biscuits.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">ü§°IT&#39;S APRIL&#39;S FOOLS TIMEü§°<br><br>This paper is two years in the making so I hope you will enjoy it!<br><br>&quot;Using Artificial Intelligence to Shed Light on the Star of Biscuits: The Jaffa Cake&quot;<a href="https://t.co/IrrBQ59FB9">https://t.co/IrrBQ59FB9</a></p>&mdash; Dr. H√©lo√Øse Stevance üñ§‚ú®(she) (@Sydonahi) <a href="https://twitter.com/Sydonahi/status/1377428934975889409?ref_src=twsrc%5Etfw">April 1, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. On the Origin of Species of Self-Supervised Learning

Samuel Albanie, Erika Lu, Joao F. Henriques

- retweets: 842, favorites: 177 (04/02/2021 10:43:10)

- links: [abs](https://arxiv.org/abs/2103.17143) | [pdf](https://arxiv.org/pdf/2103.17143)
- [cs.LG](https://arxiv.org/list/cs.LG/recent)

In the quiet backwaters of cs.CV, cs.LG and stat.ML, a cornucopia of new learning systems is emerging from a primordial soup of mathematics-learning systems with no need for external supervision. To date, little thought has been given to how these self-supervised learners have sprung into being or the principles that govern their continuing diversification. After a period of deliberate study and dispassionate judgement during which each author set their Zoom virtual background to a separate Galapagos island, we now entertain no doubt that each of these learning machines are lineal descendants of some older and generally extinct species. We make five contributions: (1) We gather and catalogue row-major arrays of machine learning specimens, each exhibiting heritable discriminative features; (2) We document a mutation mechanism by which almost imperceptible changes are introduced to the genotype of new systems, but their phenotype (birdsong in the form of tweets and vestigial plumage such as press releases) communicates dramatic changes; (3) We propose a unifying theory of self-supervised machine evolution and compare to other unifying theories on standard unifying theory benchmarks, where we establish a new (and unifying) state of the art; (4) We discuss the importance of digital biodiversity, in light of the endearingly optimistic Paris Agreement.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">On the Origin of Species of Self-Supervised Learning<br><br>Proposes a unifying theory of self-supervised machine evolution and compares to other unifying theories on standard unifying theory benchmarks, where they establish a new (and unifying) SotA.<a href="https://t.co/3XFHlBIdfE">https://t.co/3XFHlBIdfE</a> <a href="https://t.co/QrVC0TrrJ8">pic.twitter.com/QrVC0TrrJ8</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1377437534116962306?ref_src=twsrc%5Etfw">April 1, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Learning Generalizable Robotic Reward Functions from "In-The-Wild" Human  Videos

Annie S. Chen, Suraj Nair, Chelsea Finn

- retweets: 796, favorites: 171 (04/02/2021 10:43:10)

- links: [abs](https://arxiv.org/abs/2103.16817) | [pdf](https://arxiv.org/pdf/2103.16817)
- [cs.RO](https://arxiv.org/list/cs.RO/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

We are motivated by the goal of generalist robots that can complete a wide range of tasks across many environments. Critical to this is the robot's ability to acquire some metric of task success or reward, which is necessary for reinforcement learning, planning, or knowing when to ask for help. For a general-purpose robot operating in the real world, this reward function must also be able to generalize broadly across environments, tasks, and objects, while depending only on on-board sensor observations (e.g. RGB images). While deep learning on large and diverse datasets has shown promise as a path towards such generalization in computer vision and natural language, collecting high quality datasets of robotic interaction at scale remains an open challenge. In contrast, "in-the-wild" videos of humans (e.g. YouTube) contain an extensive collection of people doing interesting tasks across a diverse range of settings. In this work, we propose a simple approach, Domain-agnostic Video Discriminator (DVD), that learns multitask reward functions by training a discriminator to classify whether two videos are performing the same task, and can generalize by virtue of learning from a small amount of robot data with a broad dataset of human videos. We find that by leveraging diverse human datasets, this reward function (a) can generalize zero shot to unseen environments, (b) generalize zero shot to unseen tasks, and (c) can be combined with visual model predictive control to solve robotic manipulation tasks on a real WidowX200 robot in an unseen environment from a single human demo.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">How can robots generalize to new environments &amp; tasks?<br><br>We find that using in-the-wild videos of people can allow learned reward functions to do so!<br>Paper: <a href="https://t.co/afz2PWw0rT">https://t.co/afz2PWw0rT</a><br><br>Led by <a href="https://twitter.com/_anniechen_?ref_src=twsrc%5Etfw">@_anniechen_</a>, <a href="https://twitter.com/SurajNair_1?ref_src=twsrc%5Etfw">@SurajNair_1</a><br>üßµ(1/5) <a href="https://t.co/5BqpzVgK31">pic.twitter.com/5BqpzVgK31</a></p>&mdash; Chelsea Finn (@chelseabfinn) <a href="https://twitter.com/chelseabfinn/status/1377484517858975747?ref_src=twsrc%5Etfw">April 1, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. What do Indian Researchers download from Sci-Hub

Vivek Kumar Singh, Satya Swarup Srichandan, Sujit Bhattacharya

- retweets: 750, favorites: 72 (04/02/2021 10:43:10)

- links: [abs](https://arxiv.org/abs/2103.16783) | [pdf](https://arxiv.org/pdf/2103.16783)
- [cs.DL](https://arxiv.org/list/cs.DL/recent)

Recently three foreign academic publishers filed a case of copyright infringement against Sci-Hub and LibGen before the Delhi High Court and prayed for complete blocking these websites in India. In this context, this paper attempted to assess the impact that blocking of Sci-Hub may have on Indian research community. The download requests originating from India on a daily-basis are counted, geotagged and analysed by discipline, publisher, country and publication year etc. Results indicate that blocking Sci-Hub in India may actually hurt Indian research community in a significant way.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Why Sci-Hub should not be blocked in India- evidence from Sci-Hub access log  analysis. See- <a href="https://t.co/wzJujSGY3E">https://t.co/wzJujSGY3E</a><a href="https://twitter.com/hashtag/scihub?src=hash&amp;ref_src=twsrc%5Etfw">#scihub</a> <a href="https://twitter.com/SciResMatters?ref_src=twsrc%5Etfw">@SciResMatters</a> <a href="https://twitter.com/spf_in?ref_src=twsrc%5Etfw">@spf_in</a> <a href="https://twitter.com/arulscaria?ref_src=twsrc%5Etfw">@arulscaria</a> <a href="https://twitter.com/rsidd120?ref_src=twsrc%5Etfw">@rsidd120</a> <a href="https://twitter.com/b_sujit1965?ref_src=twsrc%5Etfw">@b_sujit1965</a> <a href="https://twitter.com/OAIndia?ref_src=twsrc%5Etfw">@OAIndia</a> <a href="https://twitter.com/openscience?ref_src=twsrc%5Etfw">@openscience</a> <a href="https://twitter.com/SciHubUpdated?ref_src=twsrc%5Etfw">@SciHubUpdated</a> <a href="https://twitter.com/asia_open?ref_src=twsrc%5Etfw">@asia_open</a> <a href="https://twitter.com/ashwani_mahajan?ref_src=twsrc%5Etfw">@ashwani_mahajan</a></p>&mdash; Vivek Singh (@vivekks12) <a href="https://twitter.com/vivekks12/status/1377426083214061570?ref_src=twsrc%5Etfw">April 1, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. Dual Contrastive Loss and Attention for GANs

Ning Yu, Guilin Liu, Aysegul Dundar, Andrew Tao, Bryan Catanzaro, Larry Davis, Mario Fritz

- retweets: 469, favorites: 153 (04/02/2021 10:43:10)

- links: [abs](https://arxiv.org/abs/2103.16748) | [pdf](https://arxiv.org/pdf/2103.16748)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.GR](https://arxiv.org/list/cs.GR/recent)

Generative Adversarial Networks (GANs) produce impressive results on unconditional image generation when powered with large-scale image datasets. Yet generated images are still easy to spot especially on datasets with high variance (e.g. bedroom, church). In this paper, we propose various improvements to further push the boundaries in image generation. Specifically, we propose a novel dual contrastive loss and show that, with this loss, discriminator learns more generalized and distinguishable representations to incentivize generation. In addition, we revisit attention and extensively experiment with different attention blocks in the generator. We find attention to be still an important module for successful image generation even though it was not used in the recent state-of-the-art models. Lastly, we study different attention architectures in the discriminator, and propose a reference attention mechanism. By combining the strengths of these remedies, we improve the compelling state-of-the-art Fr\'{e}chet Inception Distance (FID) by at least 17.5% on several benchmark datasets. We obtain even more significant improvements on compositional synthetic scenes (up to 47.5% in FID).

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Dual Contrastive Loss and Attention for GANs<br><br>Improve the SotA FID by at least 17.5% on several benchmark datasets by improving attention blocks and adding a novel dual contrastive loss.<a href="https://t.co/aUEaMQql01">https://t.co/aUEaMQql01</a> <a href="https://t.co/ugykdhSiw4">pic.twitter.com/ugykdhSiw4</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1377429206355693572?ref_src=twsrc%5Etfw">April 1, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Dual Contrastive Loss and Attention for GANs<br>pdf: <a href="https://t.co/mHYdXWFRzp">https://t.co/mHYdXWFRzp</a><br>abs: <a href="https://t.co/wwJfH0uXsR">https://t.co/wwJfH0uXsR</a><br><br>&quot;By combining the strengths of these remedies, we improve the compelling state-of-the-art Frechet Inception Distance (FID) by at least 17.5% on several benchmark datasets.&quot; <a href="https://t.co/hvD83YZe4w">pic.twitter.com/hvD83YZe4w</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1377427267886845953?ref_src=twsrc%5Etfw">April 1, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. A Neighbourhood Framework for Resource-Lean Content Flagging

Sheikh Muhammad Sarwar, Dimitrina Zlatkova, Momchil Hardalov, Yoan Dinkov, Isabelle Augenstein, Preslav Nakov

- retweets: 370, favorites: 74 (04/02/2021 10:43:11)

- links: [abs](https://arxiv.org/abs/2103.17055) | [pdf](https://arxiv.org/pdf/2103.17055)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

We propose a novel interpretable framework for cross-lingual content flagging, which significantly outperforms prior work both in terms of predictive performance and average inference time. The framework is based on a nearest-neighbour architecture and is interpretable by design. Moreover, it can easily adapt to new instances without the need to retrain it from scratch. Unlike prior work, (i) we encode not only the texts, but also the labels in the neighbourhood space (which yields better accuracy), and (ii) we use a bi-encoder instead of a cross-encoder (which saves computation time). Our evaluation results on ten different datasets for abusive language detection in eight languages shows sizable improvements over the state of the art, as well as a speed-up at inference time.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Excited about our new preprint, presenting an efficient, effective &amp; inherently interpretable framework for content flagging. This is the result of <a href="https://twitter.com/zzz2aaa?ref_src=twsrc%5Etfw">@zzz2aaa</a>&#39;s internship <a href="https://twitter.com/checkstep?ref_src=twsrc%5Etfw">@checkstep</a> &amp; work w. <a href="https://twitter.com/didizlatkova?ref_src=twsrc%5Etfw">@didizlatkova</a> <a href="https://twitter.com/mhardalov?ref_src=twsrc%5Etfw">@mhardalov</a> Yoan Dinkov <a href="https://twitter.com/preslav_nakov?ref_src=twsrc%5Etfw">@preslav_nakov</a> <a href="https://t.co/XMeZS7fSLM">https://t.co/XMeZS7fSLM</a><a href="https://twitter.com/hashtag/NLProc?src=hash&amp;ref_src=twsrc%5Etfw">#NLProc</a> <a href="https://t.co/gRxAsbR7gi">pic.twitter.com/gRxAsbR7gi</a></p>&mdash; Isabelle Augenstein (@IAugenstein) <a href="https://twitter.com/IAugenstein/status/1377504809717223425?ref_src=twsrc%5Etfw">April 1, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. Seasonal Contrast: Unsupervised Pre-Training from Uncurated Remote  Sensing Data

Oscar Ma√±as, Alexandre Lacoste, Xavier Giro-i-Nieto, David Vazquez, Pau Rodriguez

- retweets: 240, favorites: 42 (04/02/2021 10:43:11)

- links: [abs](https://arxiv.org/abs/2103.16607) | [pdf](https://arxiv.org/pdf/2103.16607)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Remote sensing and automatic earth monitoring are key to solve global-scale challenges such as disaster prevention, land use monitoring, or tackling climate change. Although there exist vast amounts of remote sensing data, most of it remains unlabeled and thus inaccessible for supervised learning algorithms. Transfer learning approaches can reduce the data requirements of deep learning algorithms. However, most of these methods are pre-trained on ImageNet and their generalization to remote sensing imagery is not guaranteed due to the domain gap. In this work, we propose Seasonal Contrast (SeCo), an effective pipeline to leverage unlabeled data for in-domain pre-training of re-mote sensing representations. The SeCo pipeline is com-posed of two parts. First, a principled procedure to gather large-scale, unlabeled and uncurated remote sensing datasets containing images from multiple Earth locations at different timestamps. Second, a self-supervised algorithm that takes advantage of time and position invariance to learn transferable representations for re-mote sensing applications. We empirically show that models trained with SeCo achieve better performance than their ImageNet pre-trained counterparts and state-of-the-art self-supervised learning methods on multiple downstream tasks. The datasets and models in SeCo will be made public to facilitate transfer learning and enable rapid progress in re-mote sensing applications.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Are you using ImageNet pre-training onüò∫üê∂ for satellite imagery? Seasonal Contrast (SeCO) does better with self-supervised pre-training on unlabeled üõ∞Ô∏è data w/ temporal changes! Done during <a href="https://twitter.com/oscmansan?ref_src=twsrc%5Etfw">@oscmansan</a> internship <a href="https://twitter.com/element_ai?ref_src=twsrc%5Etfw">@element_ai</a> <a href="https://twitter.com/servicenow?ref_src=twsrc%5Etfw">@servicenow</a> üîó<a href="https://t.co/Z3kGy2rDUo">https://t.co/Z3kGy2rDUo</a><br>Public code soon! <a href="https://t.co/3kRt486owK">pic.twitter.com/3kRt486owK</a></p>&mdash; Pau Rodr√≠guez L√≥pez (@prlz77) <a href="https://twitter.com/prlz77/status/1377624563564085253?ref_src=twsrc%5Etfw">April 1, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 10. Semi-supervised Synthesis of High-Resolution Editable Textures for 3D  Humans

Bindita Chaudhuri, Nikolaos Sarafianos, Linda Shapiro, Tony Tung

- retweets: 182, favorites: 42 (04/02/2021 10:43:11)

- links: [abs](https://arxiv.org/abs/2103.17266) | [pdf](https://arxiv.org/pdf/2103.17266)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

We introduce a novel approach to generate diverse high fidelity texture maps for 3D human meshes in a semi-supervised setup. Given a segmentation mask defining the layout of the semantic regions in the texture map, our network generates high-resolution textures with a variety of styles, that are then used for rendering purposes. To accomplish this task, we propose a Region-adaptive Adversarial Variational AutoEncoder (ReAVAE) that learns the probability distribution of the style of each region individually so that the style of the generated texture can be controlled by sampling from the region-specific distributions. In addition, we introduce a data generation technique to augment our training set with data lifted from single-view RGB inputs. Our training strategy allows the mixing of reference image styles with arbitrary styles for different regions, a property which can be valuable for virtual try-on AR/VR applications. Experimental results show that our method synthesizes better texture maps compared to prior work while enabling independent layout and style controllability.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Semi-supervised Synthesis of High-Resolution Editable Textures for 3D Humans<br>pdf: <a href="https://t.co/yBOSW4VWFE">https://t.co/yBOSW4VWFE</a><br>abs: <a href="https://t.co/kHsMxdnMPk">https://t.co/kHsMxdnMPk</a><br>project page: <a href="https://t.co/3w20O67cmY">https://t.co/3w20O67cmY</a> <a href="https://t.co/b1S0wvQ5yq">pic.twitter.com/b1S0wvQ5yq</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1377425458770313220?ref_src=twsrc%5Etfw">April 1, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 11. Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes

Dmytro Kotovenko, Matthias Wright, Arthur Heimbrecht, Bj√∂rn Ommer

- retweets: 90, favorites: 116 (04/02/2021 10:43:11)

- links: [abs](https://arxiv.org/abs/2103.17185) | [pdf](https://arxiv.org/pdf/2103.17185)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.GR](https://arxiv.org/list/cs.GR/recent)

There have been many successful implementations of neural style transfer in recent years. In most of these works, the stylization process is confined to the pixel domain. However, we argue that this representation is unnatural because paintings usually consist of brushstrokes rather than pixels. We propose a method to stylize images by optimizing parameterized brushstrokes instead of pixels and further introduce a simple differentiable rendering mechanism. Our approach significantly improves visual quality and enables additional control over the stylization process such as controlling the flow of brushstrokes through user input. We provide qualitative and quantitative evaluations that show the efficacy of the proposed parameterized representation.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes<br>pdf: <a href="https://t.co/5XT1A34usy">https://t.co/5XT1A34usy</a><br>abs: <a href="https://t.co/6uVWXT2iSO">https://t.co/6uVWXT2iSO</a> <a href="https://t.co/srBUvXioZR">pic.twitter.com/srBUvXioZR</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1377467356138590209?ref_src=twsrc%5Etfw">April 1, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes<br><br>Proposes a method to stylize images by optimizing parameterized brushstrokes instead of pixels, which  significantly improves visual quality.<br><br>abs: <a href="https://t.co/fYbG38oo9s">https://t.co/fYbG38oo9s</a><br>code: <a href="https://t.co/aSDtwaJNdD">https://t.co/aSDtwaJNdD</a> <a href="https://t.co/OtR5VtDzFA">pic.twitter.com/OtR5VtDzFA</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1377425811423170561?ref_src=twsrc%5Etfw">April 1, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 12. Trusted Artificial Intelligence: Towards Certification of Machine  Learning Applications

Philip Matthias Winter, Sebastian Eder, Johannes Weissenb√∂ck, Christoph Schwald, Thomas Doms, Tom Vogt, Sepp Hochreiter, Bernhard Nessler

- retweets: 175, favorites: 27 (04/02/2021 10:43:11)

- links: [abs](https://arxiv.org/abs/2103.16910) | [pdf](https://arxiv.org/pdf/2103.16910)
- [stat.ML](https://arxiv.org/list/stat.ML/recent) | [cs.CY](https://arxiv.org/list/cs.CY/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.SE](https://arxiv.org/list/cs.SE/recent)

Artificial Intelligence is one of the fastest growing technologies of the 21st century and accompanies us in our daily lives when interacting with technical applications. However, reliance on such technical systems is crucial for their widespread applicability and acceptance. The societal tools to express reliance are usually formalized by lawful regulations, i.e., standards, norms, accreditations, and certificates. Therefore, the T\"UV AUSTRIA Group in cooperation with the Institute for Machine Learning at the Johannes Kepler University Linz, proposes a certification process and an audit catalog for Machine Learning applications. We are convinced that our approach can serve as the foundation for the certification of applications that use Machine Learning and Deep Learning, the techniques that drive the current revolution in Artificial Intelligence. While certain high-risk areas, such as fully autonomous robots in workspaces shared with humans, are still some time away from certification, we aim to cover low-risk applications with our certification procedure. Our holistic approach attempts to analyze Machine Learning applications from multiple perspectives to evaluate and verify the aspects of secure software development, functional requirements, data quality, data protection, and ethics. Inspired by existing work, we introduce four criticality levels to map the criticality of a Machine Learning application regarding the impact of its decisions on people, environment, and organizations. Currently, the audit catalog can be applied to low-risk applications within the scope of supervised learning as commonly encountered in industry. Guided by field experience, scientific developments, and market demands, the audit catalog will be extended and modified accordingly.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Our white paper &quot;Trusted AI: Towards Certification of Machine Learning Applications&quot; is now available on arxiv: <a href="https://t.co/C73Jqz5S3m">https://t.co/C73Jqz5S3m</a></p>&mdash; Philip M. Winter (@PhilipMWinter) <a href="https://twitter.com/PhilipMWinter/status/1377505826294861824?ref_src=twsrc%5Etfw">April 1, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 13. VITON-HD: High-Resolution Virtual Try-On via Misalignment-Aware  Normalization

Seunghwan Choi, Sunghyun Park, Minsoo Lee, Jaegul Choo

- retweets: 117, favorites: 71 (04/02/2021 10:43:11)

- links: [abs](https://arxiv.org/abs/2103.16874) | [pdf](https://arxiv.org/pdf/2103.16874)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

The task of image-based virtual try-on aims to transfer a target clothing item onto the corresponding region of a person, which is commonly tackled by fitting the item to the desired body part and fusing the warped item with the person. While an increasing number of studies have been conducted, the resolution of synthesized images is still limited to low (e.g., 256x192), which acts as the critical limitation against satisfying online consumers. We argue that the limitation stems from several challenges: as the resolution increases, the artifacts in the misaligned areas between the warped clothes and the desired clothing regions become noticeable in the final results; the architectures used in existing methods have low performance in generating high-quality body parts and maintaining the texture sharpness of the clothes. To address the challenges, we propose a novel virtual try-on method called VITON-HD that successfully synthesizes 1024x768 virtual try-on images. Specifically, we first prepare the segmentation map to guide our virtual try-on synthesis, and then roughly fit the target clothing item to a given person's body. Next, we propose ALIgnment-Aware Segment (ALIAS) normalization and ALIAS generator to handle the misaligned areas and preserve the details of 1024x768 inputs. Through rigorous comparison with existing methods, we demonstrate that VITON-HD highly sur-passes the baselines in terms of synthesized image quality both qualitatively and quantitatively.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">VITON-HD: High-Resolution Virtual Try-On via Misalignment-Aware Normalization<br>pdf: <a href="https://t.co/dXrLMTwYHq">https://t.co/dXrLMTwYHq</a><br>abs: <a href="https://t.co/QrlQDYOZDk">https://t.co/QrlQDYOZDk</a> <a href="https://t.co/XLiZIEi24X">pic.twitter.com/XLiZIEi24X</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1377436986349264897?ref_src=twsrc%5Etfw">April 1, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 14. Symmetric and antisymmetric kernels for machine learning problems in  quantum physics and chemistry

Stefan Klus, Patrick Gel√ü, Feliks N√ºske, Frank No√©

- retweets: 90, favorites: 87 (04/02/2021 10:43:12)

- links: [abs](https://arxiv.org/abs/2103.17233) | [pdf](https://arxiv.org/pdf/2103.17233)
- [quant-ph](https://arxiv.org/list/quant-ph/recent) | [math-ph](https://arxiv.org/list/math-ph/recent) | [physics.chem-ph](https://arxiv.org/list/physics.chem-ph/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

We derive symmetric and antisymmetric kernels by symmetrizing and antisymmetrizing conventional kernels and analyze their properties. In particular, we compute the feature space dimensions of the resulting polynomial kernels, prove that the reproducing kernel Hilbert spaces induced by symmetric and antisymmetric Gaussian kernels are dense in the space of symmetric and antisymmetric functions, and propose a Slater determinant representation of the antisymmetric Gaussian kernel, which allows for an efficient evaluation even if the state space is high-dimensional. Furthermore, we show that by exploiting symmetries or antisymmetries the size of the training data set can be significantly reduced. The results are illustrated with guiding examples and simple quantum physics and chemistry applications.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Antisymmetry is what makes electronic structure calculations hard. We need more <a href="https://twitter.com/hashtag/MachineLearning?src=hash&amp;ref_src=twsrc%5Etfw">#MachineLearning</a> work to make progress with this fundamental problem. Stefan Klus takes a stab at it by developing antisymmetric kernels:<a href="https://t.co/UOzW3w2k3c">https://t.co/UOzW3w2k3c</a></p>&mdash; Frank Noe (@FrankNoeBerlin) <a href="https://twitter.com/FrankNoeBerlin/status/1377642462701846530?ref_src=twsrc%5Etfw">April 1, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 15. Tracking Knowledge Propagation Across Wikipedia Languages

Roldolfo Valentim, Giovanni Comarela, Souneil Park, Diego Saez-Trumper

- retweets: 72, favorites: 33 (04/02/2021 10:43:12)

- links: [abs](https://arxiv.org/abs/2103.16613) | [pdf](https://arxiv.org/pdf/2103.16613)
- [cs.CY](https://arxiv.org/list/cs.CY/recent)

In this paper, we present a dataset of inter-language knowledge propagation in Wikipedia. Covering the entire 309 language editions and 33M articles, the dataset aims to track the full propagation history of Wikipedia concepts, and allow follow up research on building predictive models of them. For this purpose, we align all the Wikipedia articles in a language-agnostic manner according to the concept they cover, which results in 13M propagation instances. To the best of our knowledge, this dataset is the first to explore the full inter-language propagation at a large scale. Together with the dataset, a holistic overview of the propagation and key insights about the underlying structural factors are provided to aid future research. For example, we find that although long cascades are unusual, the propagation tends to continue further once it reaches more than four language editions. We also find that the size of language editions is associated with the speed of propagation. We believe the dataset not only contributes to the prior literature on Wikipedia growth but also enables new use cases such as edit recommendation for addressing knowledge gaps, detection of disinformation, and cultural relationship analysis.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Our dataset paper &quot;Tracking Knowledge Propagation Across <a href="https://twitter.com/hashtag/Wikipedia?src=hash&amp;ref_src=twsrc%5Etfw">#Wikipedia</a> Languages&quot; accepted at <a href="https://twitter.com/icwsm?ref_src=twsrc%5Etfw">@icwsm</a> is now available<br>Topics and creation time for each article across all languages +model to predict content propagation in WP<br>Paper:<a href="https://t.co/xUTxyiEJ67">https://t.co/xUTxyiEJ67</a><br>Data:<a href="https://t.co/ShGZi9MREg">https://t.co/ShGZi9MREg</a> <a href="https://t.co/MmAtlJuu2E">pic.twitter.com/MmAtlJuu2E</a></p>&mdash; Diego ST (@e__migrante) <a href="https://twitter.com/e__migrante/status/1377440003991592960?ref_src=twsrc%5Etfw">April 1, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 16. BASE Layers: Simplifying Training of Large, Sparse Models

Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, Luke Zettlemoyer

- retweets: 46, favorites: 55 (04/02/2021 10:43:12)

- links: [abs](https://arxiv.org/abs/2103.16716) | [pdf](https://arxiv.org/pdf/2103.16716)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

We introduce a new balanced assignment of experts (BASE) layer for large language models that greatly simplifies existing high capacity sparse layers. Sparse layers can dramatically improve the efficiency of training and inference by routing each token to specialized expert modules that contain only a small fraction of the model parameters. However, it can be difficult to learn balanced routing functions that make full use of the available experts; existing approaches typically use routing heuristics or auxiliary expert-balancing loss functions. In contrast, we formulate token-to-expert allocation as a linear assignment problem, allowing an optimal assignment in which each expert receives an equal number of tokens. This optimal assignment scheme improves efficiency by guaranteeing balanced compute loads, and also simplifies training by not requiring any new hyperparameters or auxiliary losses. Code is publicly released at https://github.com/pytorch/fairseq/

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">BASE Layers: Simplifying Training of Large, Sparse Models<br><br>Simplifies the loading of sparse MoE by formulating token-to-expert allocation as a linear assignment problem, which outperforms Switch Transformer.<a href="https://t.co/RneE13TCEu">https://t.co/RneE13TCEu</a> <a href="https://t.co/FSJNwXysZy">pic.twitter.com/FSJNwXysZy</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1377430796630335489?ref_src=twsrc%5Etfw">April 1, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 17. Rethinking Self-supervised Correspondence Learning: A Video Frame-level  Similarity Perspective

Jiarui Xu, Xiaolong Wang

- retweets: 42, favorites: 45 (04/02/2021 10:43:12)

- links: [abs](https://arxiv.org/abs/2103.17263) | [pdf](https://arxiv.org/pdf/2103.17263)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Learning a good representation for space-time correspondence is the key for various computer vision tasks, including tracking object bounding boxes and performing video object pixel segmentation. To learn generalizable representation for correspondence in large-scale, a variety of self-supervised pretext tasks are proposed to explicitly perform object-level or patch-level similarity learning. Instead of following the previous literature, we propose to learn correspondence using Video Frame-level Similarity (VFS) learning, i.e, simply learning from comparing video frames. Our work is inspired by the recent success in image-level contrastive learning and similarity learning for visual recognition. Our hypothesis is that if the representation is good for recognition, it requires the convolutional features to find correspondence between similar objects or parts. Our experiments show surprising results that VFS surpasses state-of-the-art self-supervised approaches for both OTB visual object tracking and DAVIS video object segmentation. We perform detailed analysis on what matters in VFS and reveals new properties on image and frame level similarity learning. Project page is available at https://jerryxu.net/VFS.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Rethinking Self-supervised Correspondence Learning: A Video Frame-level Similarity Perspective<br>pdf: <a href="https://t.co/pGyVepM7Eg">https://t.co/pGyVepM7Eg</a><br>abs: <a href="https://t.co/tcIqTXOfqq">https://t.co/tcIqTXOfqq</a><br>project page: <a href="https://t.co/TCMn90RbDs">https://t.co/TCMn90RbDs</a> <a href="https://t.co/jxwnGDKAOI">pic.twitter.com/jxwnGDKAOI</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1377443785995616256?ref_src=twsrc%5Etfw">April 1, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 18. Learning Spatio-Temporal Transformer for Visual Tracking

Bin Yan, Houwen Peng, Jianlong Fu, Dong Wang, Huchuan Lu

- retweets: 42, favorites: 40 (04/02/2021 10:43:12)

- links: [abs](https://arxiv.org/abs/2103.17154) | [pdf](https://arxiv.org/pdf/2103.17154)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

In this paper, we present a new tracking architecture with an encoder-decoder transformer as the key component. The encoder models the global spatio-temporal feature dependencies between target objects and search regions, while the decoder learns a query embedding to predict the spatial positions of the target objects. Our method casts object tracking as a direct bounding box prediction problem, without using any proposals or predefined anchors. With the encoder-decoder transformer, the prediction of objects just uses a simple fully-convolutional network, which estimates the corners of objects directly. The whole method is end-to-end, does not need any postprocessing steps such as cosine window and bounding box smoothing, thus largely simplifying existing tracking pipelines. The proposed tracker achieves state-of-the-art performance on five challenging short-term and long-term benchmarks, while running at real-time speed, being 6x faster than Siam R-CNN. Code and models are open-sourced at https://github.com/researchmm/Stark.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Learning Spatio-Temporal Transformer for Visual Tracking<br>pdf: <a href="https://t.co/OMXCIhMHxm">https://t.co/OMXCIhMHxm</a><br>abs: <a href="https://t.co/2JDwHTwI95">https://t.co/2JDwHTwI95</a><br>github: <a href="https://t.co/RwBZaWVcRm">https://t.co/RwBZaWVcRm</a><br>proposed tracker achieves SOTA performance on five challenging short-term and long-term benchmarks, while running at real-time speed <a href="https://t.co/KzoFKJNfbJ">pic.twitter.com/KzoFKJNfbJ</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1377442519949135872?ref_src=twsrc%5Etfw">April 1, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 19. ReMix: Towards Image-to-Image Translation with Limited Data

Jie Cao, Luanxuan Hou, Ming-Hsuan Yang, Ran He, Zhenan Sun

- retweets: 49, favorites: 27 (04/02/2021 10:43:12)

- links: [abs](https://arxiv.org/abs/2103.16835) | [pdf](https://arxiv.org/pdf/2103.16835)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Image-to-image (I2I) translation methods based on generative adversarial networks (GANs) typically suffer from overfitting when limited training data is available. In this work, we propose a data augmentation method (ReMix) to tackle this issue. We interpolate training samples at the feature level and propose a novel content loss based on the perceptual relations among samples. The generator learns to translate the in-between samples rather than memorizing the training set, and thereby forces the discriminator to generalize. The proposed approach effectively reduces the ambiguity of generation and renders content-preserving results. The ReMix method can be easily incorporated into existing GAN models with minor modifications. Experimental results on numerous tasks demonstrate that GAN models equipped with the ReMix method achieve significant improvements.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">ReMix: Towards Image-to-Image Translation with Limited Data<br>pdf: <a href="https://t.co/PQn8c8wH23">https://t.co/PQn8c8wH23</a><br>abs: <a href="https://t.co/jP8Wmz0IsY">https://t.co/jP8Wmz0IsY</a> <a href="https://t.co/e5f9UImQ9Z">pic.twitter.com/e5f9UImQ9Z</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1377433653744005122?ref_src=twsrc%5Etfw">April 1, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 20. A genuinely natural information measure

Andreas Winter

- retweets: 14, favorites: 61 (04/02/2021 10:43:12)

- links: [abs](https://arxiv.org/abs/2103.16662) | [pdf](https://arxiv.org/pdf/2103.16662)
- [cs.IT](https://arxiv.org/list/cs.IT/recent) | [quant-ph](https://arxiv.org/list/quant-ph/recent)

The theoretical measuring of information was famously initiated by Shannon in his mathematical theory of communication, in which he proposed a now widely used quantity, the entropy, measured in bits. Yet, in the same paper, Shannon also chose to measure the information in continuous systems in nats, which differ from bits by the use of the natural rather than the binary logarithm.   We point out that there is nothing natural about the choice of logarithm basis, rather it is arbitrary. We remedy this problematic state of affairs by proposing a genuinely natural measure of information, which we dub gnats. We show that gnats have many advantages in information theory, and propose to adopt the underlying methodology throughout science, arts and everyday life.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Today&#39;s preprint &quot;A genuinely natural information measure&quot; (<a href="https://t.co/4xSUL0YenQ">https://t.co/4xSUL0YenQ</a>) by Andreas Winter from <a href="https://twitter.com/GIQ_BCN?ref_src=twsrc%5Etfw">@GIQ_BCN</a>, may (or may not) revolutionise information theory. <a href="https://t.co/pbm96X31JQ">pic.twitter.com/pbm96X31JQ</a></p>&mdash; Quantum Info at UAB (@GIQ_BCN) <a href="https://twitter.com/GIQ_BCN/status/1377545102155988993?ref_src=twsrc%5Etfw">April 1, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



