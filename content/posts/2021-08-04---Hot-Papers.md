---
title: Hot Papers 2021-08-04
date: 2021-08-05T08:26:21.Z
template: "post"
draft: false
slug: "hot-papers-2021-08-04"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2021-08-04"
socialImage: "/media/flying-marine.jpg"

---

# 1. Accountability and Forensics in Blockchains: XDC Consensus Engine DPoS  2.0

Gerui Wang, Jerome Wang, Liam Lai, Fisher Yu

- retweets: 2905, favorites: 313 (08/05/2021 08:26:21)

- links: [abs](https://arxiv.org/abs/2108.01420) | [pdf](https://arxiv.org/pdf/2108.01420)
- [cs.CR](https://arxiv.org/list/cs.CR/recent)

This document introduces XinFin DPoS 2.0, the proposed next generation decentralized consensus engine for the XinFin XDC Network. Built upon the most advanced BFT consensus protocol, this upgrade will empower the XDC Network with military-grade security and performance while consuming extremely low resources, and will be fully backwards-compatible in terms of APIs. It will also pave the road to the future evolution of the XDC Network.   The core invention is the holistic integration of accountability and forensics in blockchains: the ability to identify malicious actors with cryptographic integrity directly from the blockchain records, incorporating the latest peer-reviewed academic research with state of the art engineering designs and implementation plans.

<blockquote class="twitter-tweet"><p lang="und" dir="ltr"><a href="https://t.co/c6VrebqWwq">https://t.co/c6VrebqWwq</a></p>&mdash; Atul Khekade (@atulkhekade) <a href="https://twitter.com/atulkhekade/status/1422902799063425026?ref_src=twsrc%5Etfw">August 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Accountability and Forensics in Blockchains: <a href="https://twitter.com/search?q=%24XDC&amp;src=ctag&amp;ref_src=twsrc%5Etfw">$XDC</a> Consensus Engine DPoS 2.0<a href="https://twitter.com/hashtag/XDCNetwork?src=hash&amp;ref_src=twsrc%5Etfw">#XDCNetwork</a><a href="https://twitter.com/hashtag/Cryptography?src=hash&amp;ref_src=twsrc%5Etfw">#Cryptography</a> <a href="https://twitter.com/hashtag/Security?src=hash&amp;ref_src=twsrc%5Etfw">#Security</a> <a href="https://t.co/iE9LTE4NvN">https://t.co/iE9LTE4NvN</a> <a href="https://t.co/qBoemeCHad">pic.twitter.com/qBoemeCHad</a></p>&mdash; Sal M #WeAreXDC ü™êüî• (@Mr_Blockchain22) <a href="https://twitter.com/Mr_Blockchain22/status/1422907095721127945?ref_src=twsrc%5Etfw">August 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Accountability and Forensics in Blockchains: XDC Consensus Engine DPoS 2.0<a href="https://twitter.com/hashtag/XDC?src=hash&amp;ref_src=twsrc%5Etfw">#XDC</a> Network <a href="https://twitter.com/hashtag/XDPOS?src=hash&amp;ref_src=twsrc%5Etfw">#XDPOS</a> 2.0  by <a href="https://twitter.com/hashtag/XinFin?src=hash&amp;ref_src=twsrc%5Etfw">#XinFin</a> Team <a href="https://twitter.com/hashtag/Growth?src=hash&amp;ref_src=twsrc%5Etfw">#Growth</a>, <a href="https://twitter.com/hashtag/progress?src=hash&amp;ref_src=twsrc%5Etfw">#progress</a>, <a href="https://twitter.com/hashtag/interoperability?src=hash&amp;ref_src=twsrc%5Etfw">#interoperability</a>, <a href="https://twitter.com/hashtag/security?src=hash&amp;ref_src=twsrc%5Etfw">#security</a>, <a href="https://twitter.com/hashtag/innovation?src=hash&amp;ref_src=twsrc%5Etfw">#innovation</a> <br><br>This is <a href="https://twitter.com/hashtag/XDC?src=hash&amp;ref_src=twsrc%5Etfw">#XDC</a>  <a href="https://t.co/pntVFhBHkJ">https://t.co/pntVFhBHkJ</a></p>&mdash; ChainGamer #WeAreXDC (@BlockNewb) <a href="https://twitter.com/BlockNewb/status/1422901713611145219?ref_src=twsrc%5Etfw">August 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. SphereFace2: Binary Classification is All You Need for Deep Face  Recognition

Yandong Wen, Weiyang Liu, Adrian Weller, Bhiksha Raj, Rita Singh

- retweets: 323, favorites: 81 (08/05/2021 08:26:21)

- links: [abs](https://arxiv.org/abs/2108.01513) | [pdf](https://arxiv.org/pdf/2108.01513)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

State-of-the-art deep face recognition methods are mostly trained with a softmax-based multi-class classification framework. Despite being popular and effective, these methods still have a few shortcomings that limit empirical performance. In this paper, we first identify the discrepancy between training and evaluation in the existing multi-class classification framework and then discuss the potential limitations caused by the "competitive" nature of softmax normalization. Motivated by these limitations, we propose a novel binary classification training framework, termed SphereFace2. In contrast to existing methods, SphereFace2 circumvents the softmax normalization, as well as the corresponding closed-set assumption. This effectively bridges the gap between training and evaluation, enabling the representations to be improved individually by each binary classification task. Besides designing a specific well-performing loss function, we summarize a few general principles for this "one-vs-all" binary classification framework so that it can outperform current competitive methods. We conduct comprehensive experiments on popular benchmarks to demonstrate that SphereFace2 can consistently outperform current state-of-the-art deep face recognition methods.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">SphereFace2: Binary Classification is All You Need for Deep Face Recognition<br>pdf: <a href="https://t.co/EasbGEvgit">https://t.co/EasbGEvgit</a><br>abs: <a href="https://t.co/h6WolgsqRP">https://t.co/h6WolgsqRP</a> <a href="https://t.co/0Kg6kU9p6z">pic.twitter.com/0Kg6kU9p6z</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1422747677616398337?ref_src=twsrc%5Etfw">August 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Toward Spatially Unbiased Generative Models

Jooyoung Choi, Jungbeom Lee, Yonghyun Jeong, Sungroh Yoon

- retweets: 272, favorites: 85 (08/05/2021 08:26:22)

- links: [abs](https://arxiv.org/abs/2108.01285) | [pdf](https://arxiv.org/pdf/2108.01285)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent)

Recent image generation models show remarkable generation performance. However, they mirror strong location preference in datasets, which we call spatial bias. Therefore, generators render poor samples at unseen locations and scales. We argue that the generators rely on their implicit positional encoding to render spatial content. From our observations, the generator's implicit positional encoding is translation-variant, making the generator spatially biased. To address this issue, we propose injecting explicit positional encoding at each scale of the generator. By learning the spatially unbiased generator, we facilitate the robust use of generators in multiple tasks, such as GAN inversion, multi-scale generation, generation of arbitrary sizes and aspect ratios. Furthermore, we show that our method can also be applied to denoising diffusion probabilistic models.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Toward Spatially Unbiased Generative Models<br>pdf: <a href="https://t.co/TPlWZmqJiy">https://t.co/TPlWZmqJiy</a><br>abs: <a href="https://t.co/YI7IWqAoJ6">https://t.co/YI7IWqAoJ6</a><br>github: <a href="https://t.co/HWYfnPrutK">https://t.co/HWYfnPrutK</a> <a href="https://t.co/AlAAWY9cWd">pic.twitter.com/AlAAWY9cWd</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1422723811229700101?ref_src=twsrc%5Etfw">August 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. A Benchmarking Initiative for Audio-Domain Music Generation Using the  Freesound Loop Dataset

Tun-Min Hung, Bo-Yu Chen, Yen-Tung Yeh, Yi-Hsuan Yang

- retweets: 235, favorites: 113 (08/05/2021 08:26:22)

- links: [abs](https://arxiv.org/abs/2108.01576) | [pdf](https://arxiv.org/pdf/2108.01576)
- [cs.SD](https://arxiv.org/list/cs.SD/recent) | [eess.AS](https://arxiv.org/list/eess.AS/recent)

This paper proposes a new benchmark task for generat-ing musical passages in the audio domain by using thedrum loops from the FreeSound Loop Dataset, which arepublicly re-distributable. Moreover, we use a larger col-lection of drum loops from Looperman to establish fourmodel-based objective metrics for evaluation, releasingthese metrics as a library for quantifying and facilitatingthe progress of musical audio generation. Under this eval-uation framework, we benchmark the performance of threerecent deep generative adversarial network (GAN) mod-els we customize to generate loops, including StyleGAN,StyleGAN2, and UNAGAN. We also report a subjectiveevaluation of these models. Our evaluation shows that theone based on StyleGAN2 performs the best in both objec-tive and subjective metrics.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">A Benchmarking Initiative for Audio-Domain Music Generation Using the Freesound Loop Dataset<br>pdf: <a href="https://t.co/K9SPhGgIYf">https://t.co/K9SPhGgIYf</a><br>github: <a href="https://t.co/qW9TjxvUXA">https://t.co/qW9TjxvUXA</a> <a href="https://t.co/ueFRJft6iu">pic.twitter.com/ueFRJft6iu</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1422729014716911616?ref_src=twsrc%5Etfw">August 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Preprint of our <a href="https://twitter.com/hashtag/ismir2021?src=hash&amp;ref_src=twsrc%5Etfw">#ismir2021</a> paper on audio-domain loop generation benchmarking, showing StyleGAN2&gt;UNAGAN&gt;StyleGAN. <br>üêãpaper - <a href="https://t.co/rTa3C6Ak7E">https://t.co/rTa3C6Ak7E</a><br>üêãdemo - <a href="https://t.co/PmZCC9KWGh">https://t.co/PmZCC9KWGh</a><br>üêãcode (incl. metrics) - <a href="https://t.co/UhsxjTsleK">https://t.co/UhsxjTsleK</a> <br>(<a href="https://twitter.com/AllenHu75466923?ref_src=twsrc%5Etfw">@AllenHu75466923</a>, <a href="https://twitter.com/Chen_Paul_u?ref_src=twsrc%5Etfw">@Chen_Paul_u</a> , <a href="https://twitter.com/YenTung11?ref_src=twsrc%5Etfw">@YenTung11</a>) <a href="https://t.co/oTIgKIMf8N">https://t.co/oTIgKIMf8N</a></p>&mdash; Yi-Hsuan Yang (@affige_yang) <a href="https://twitter.com/affige_yang/status/1422748057607700482?ref_src=twsrc%5Etfw">August 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Cycle-Consistent Inverse GAN for Text-to-Image Synthesis

Hao Wang, Guosheng Lin, Steven C. H. Hoi, Chunyan Miao

- retweets: 64, favorites: 46 (08/05/2021 08:26:22)

- links: [abs](https://arxiv.org/abs/2108.01361) | [pdf](https://arxiv.org/pdf/2108.01361)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

This paper investigates an open research task of text-to-image synthesis for automatically generating or manipulating images from text descriptions. Prevailing methods mainly use the text as conditions for GAN generation, and train different models for the text-guided image generation and manipulation tasks. In this paper, we propose a novel unified framework of Cycle-consistent Inverse GAN (CI-GAN) for both text-to-image generation and text-guided image manipulation tasks. Specifically, we first train a GAN model without text input, aiming to generate images with high diversity and quality. Then we learn a GAN inversion model to convert the images back to the GAN latent space and obtain the inverted latent codes for each image, where we introduce the cycle-consistency training to learn more robust and consistent inverted latent codes. We further uncover the latent space semantics of the trained GAN model, by learning a similarity model between text representations and the latent codes. In the text-guided optimization module, we generate images with the desired semantic attributes by optimizing the inverted latent codes. Extensive experiments on the Recipe1M and CUB datasets validate the efficacy of our proposed framework.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Cycle-Consistent Inverse GAN for Text-to-Image Synthesis<br>pdf: <a href="https://t.co/tllnLlnoKO">https://t.co/tllnLlnoKO</a><br>abs: <a href="https://t.co/0h79k6yfMT">https://t.co/0h79k6yfMT</a> <a href="https://t.co/aLofeRfkBL">pic.twitter.com/aLofeRfkBL</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1422728106188627972?ref_src=twsrc%5Etfw">August 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. Domain Generalization via Gradient Surgery

Lucas Mansilla, Rodrigo Echeveste, Diego H. Milone, Enzo Ferrante

- retweets: 58, favorites: 41 (08/05/2021 08:26:22)

- links: [abs](https://arxiv.org/abs/2108.01621) | [pdf](https://arxiv.org/pdf/2108.01621)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [eess.IV](https://arxiv.org/list/eess.IV/recent)

In real-life applications, machine learning models often face scenarios where there is a change in data distribution between training and test domains. When the aim is to make predictions on distributions different from those seen at training, we incur in a domain generalization problem. Methods to address this issue learn a model using data from multiple source domains, and then apply this model to the unseen target domain. Our hypothesis is that when training with multiple domains, conflicting gradients within each mini-batch contain information specific to the individual domains which is irrelevant to the others, including the test domain. If left untouched, such disagreement may degrade generalization performance. In this work, we characterize the conflicting gradients emerging in domain shift scenarios and devise novel gradient agreement strategies based on gradient surgery to alleviate their effect. We validate our approach in image classification tasks with three multi-domain datasets, showing the value of the proposed agreement strategy in enhancing the generalization capability of deep learning models in domain shift scenarios.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">üö®‚ÄúDomain generalization via gradient surgery‚Äù‚úÇÔ∏èwas accepted at @iccv2021 üéâ<br><br>We study conflicting gradients emerging in domain shift<br>scenarios and devise novel gradient agreement strategies to improve generalization performance on unseen domains<br><br>Paper‚û°Ô∏è <a href="https://t.co/eLNVqeW79x">https://t.co/eLNVqeW79x</a> <a href="https://t.co/eoaXHQJJnr">pic.twitter.com/eoaXHQJJnr</a></p>&mdash; Enzo Ferrante (@enzoferrante) <a href="https://twitter.com/enzoferrante/status/1422958089184190465?ref_src=twsrc%5Etfw">August 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. Where do Models go Wrong? Parameter-Space Saliency Maps for  Explainability

Roman Levin, Manli Shu, Eitan Borgnia, Furong Huang, Micah Goldblum, Tom Goldstein

- retweets: 51, favorites: 40 (08/05/2021 08:26:22)

- links: [abs](https://arxiv.org/abs/2108.01335) | [pdf](https://arxiv.org/pdf/2108.01335)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Conventional saliency maps highlight input features to which neural network predictions are highly sensitive. We take a different approach to saliency, in which we identify and analyze the network parameters, rather than inputs, which are responsible for erroneous decisions. We find that samples which cause similar parameters to malfunction are semantically similar. We also show that pruning the most salient parameters for a wrongly classified sample often improves model behavior. Furthermore, fine-tuning a small number of the most salient parameters on a single sample results in error correction on other samples that are misclassified for similar reasons. Based on our parameter saliency method, we also introduce an input-space saliency technique that reveals how image features cause specific network components to malfunction. Further, we rigorously validate the meaningfulness of our saliency maps on both the dataset and case-study levels.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Where do Models go Wrong? Parameter-Space Saliency Maps for Explainability<br>pdf: <a href="https://t.co/dFPHas5hq3">https://t.co/dFPHas5hq3</a><br>abs: <a href="https://t.co/dBM4SHzVma">https://t.co/dBM4SHzVma</a> <a href="https://t.co/hmLH0cvBqo">pic.twitter.com/hmLH0cvBqo</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1422745331670953984?ref_src=twsrc%5Etfw">August 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. Large-Scale Differentially Private BERT

Rohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, Pasin Manurangsi

- retweets: 56, favorites: 29 (08/05/2021 08:26:22)

- links: [abs](https://arxiv.org/abs/2108.01624) | [pdf](https://arxiv.org/pdf/2108.01624)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.CR](https://arxiv.org/list/cs.CR/recent)

In this work, we study the large-scale pretraining of BERT-Large with differentially private SGD (DP-SGD). We show that combined with a careful implementation, scaling up the batch size to millions (i.e., mega-batches) improves the utility of the DP-SGD step for BERT; we also enhance its efficiency by using an increasing batch size schedule. Our implementation builds on the recent work of [SVK20], who demonstrated that the overhead of a DP-SGD step is minimized with effective use of JAX [BFH+18, FJL18] primitives in conjunction with the XLA compiler [XLA17]. Our implementation achieves a masked language model accuracy of 60.5% at a batch size of 2M, for $\epsilon = 5.36$. To put this number in perspective, non-private BERT models achieve an accuracy of $\sim$70%.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Large-Scale Differentially Private BERT<br>pdf: <a href="https://t.co/S3WwsMUEr6">https://t.co/S3WwsMUEr6</a><br>abs: <a href="https://t.co/jn803y67sI">https://t.co/jn803y67sI</a><br><br>achieves a masked language model accuracy of 60.5% at a batch size of 2M <a href="https://t.co/u5xm4HyfzV">pic.twitter.com/u5xm4HyfzV</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1422724544486428678?ref_src=twsrc%5Etfw">August 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. EMOPIA: A Multi-Modal Pop Piano Dataset For Emotion Recognition and  Emotion-based Music Generation

Hsiao-Tzu Hung, Joann Ching, Seungheon Doh, Nabin Kim, Juhan Nam, Yi-Hsuan Yang

- retweets: 42, favorites: 26 (08/05/2021 08:26:23)

- links: [abs](https://arxiv.org/abs/2108.01374) | [pdf](https://arxiv.org/pdf/2108.01374)
- [cs.SD](https://arxiv.org/list/cs.SD/recent) | [cs.MM](https://arxiv.org/list/cs.MM/recent) | [eess.AS](https://arxiv.org/list/eess.AS/recent)

While there are many music datasets with emotion labels in the literature, they cannot be used for research on symbolic-domain music analysis or generation, as there are usually audio files only. In this paper, we present the EMOPIA (pronounced `yee-m\`{o}-pi-uh') dataset, a shared multi-modal (audio and MIDI) database focusing on perceived emotion in pop piano music, to facilitate research on various tasks related to music emotion. The dataset contains 1,087 music clips from 387 songs and clip-level emotion labels annotated by four dedicated annotators. Since the clips are not restricted to one clip per song, they can also be used for song-level analysis. We present the methodology for building the dataset, covering the song list curation, clip selection, and emotion annotation processes. Moreover, we prototype use cases on clip-level music emotion classification and emotion-based symbolic music generation by training and evaluating corresponding models using the dataset. The result demonstrates the potential of EMOPIA for being used in future exploration on piano emotion-related MIR tasks.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">EMOPIA: A Multi-Modal Pop Piano Dataset For Emotion Recognition and Emotion-based Music Generation<br>paper: <a href="https://t.co/A9T5TULIxG">https://t.co/A9T5TULIxG</a><br>github: <a href="https://t.co/57yl1rCnXj">https://t.co/57yl1rCnXj</a><br><br>dataset contains 1,087 music clips from 387 songs, clip-level emotion labels annotated by four dedicated annotators <a href="https://t.co/Mi0I24W7HW">pic.twitter.com/Mi0I24W7HW</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1422722893016875016?ref_src=twsrc%5Etfw">August 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 10. Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer

Yifan Xu, Zhijie Zhang, Mengdan Zhang, Kekai Sheng, Ke Li, Weiming Dong, Liqing Zhang, Changsheng Xu, Xing Sun

- retweets: 32, favorites: 20 (08/05/2021 08:26:23)

- links: [abs](https://arxiv.org/abs/2108.01390) | [pdf](https://arxiv.org/pdf/2108.01390)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Vision transformers have recently received explosive popularity, but huge computational cost is still a severe issue. Recent efficient designs for vision transformers follow two pipelines, namely, structural compression based on local spatial prior and non-structural token pruning. However, rough token pruning breaks the spatial structure that is indispensable for local spatial prior. To take advantage of both two pipelines, this work seeks to dynamically identify uninformative tokens for each instance and trim down both the training and inference complexity while maintain complete spatial structure and information flow. To achieve this goal, we propose Evo-ViT, a self-motivated slow-fast token evolution method for vision transformers. Specifically, we conduct unstructured instance-wise token selection by taking advantage of the global class attention that is unique to vision transformers. Then, we propose to update information tokens and placeholder tokens that contribute little to the final prediction with different computational properties, namely, slow-fast updating. Thanks to the slow-fast updating mechanism that guarantees information flow and spatial structure, our Evo-ViT can accelerate vanilla transformers of both flat and deep-narrow structures from the very beginning of the training process. Experimental results demonstrate that the proposed method can significantly reduce the computational costs of vision transformers while maintaining comparable performance on image classification. For example, our method accelerates DeiT-S by over 60% throughput while only sacrificing 0.4% top-1 accuracy.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer<br>pdf: <a href="https://t.co/Hk6haYZdH8">https://t.co/Hk6haYZdH8</a><br>abs: <a href="https://t.co/FaFbXbXV7d">https://t.co/FaFbXbXV7d</a><br><br>accelerates DeiTS by over 60% throughput while only sacrificing 0.4% top-1 accuracy <a href="https://t.co/RFpf87zjb1">pic.twitter.com/RFpf87zjb1</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1422745751210299392?ref_src=twsrc%5Etfw">August 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 11. Consistent Depth of Moving Objects in Video

Zhoutong Zhang, Forrester Cole, Richard Tucker, William T. Freeman, Tali Dekel

- retweets: 20, favorites: 31 (08/05/2021 08:26:23)

- links: [abs](https://arxiv.org/abs/2108.01166) | [pdf](https://arxiv.org/pdf/2108.01166)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.GR](https://arxiv.org/list/cs.GR/recent)

We present a method to estimate depth of a dynamic scene, containing arbitrary moving objects, from an ordinary video captured with a moving camera. We seek a geometrically and temporally consistent solution to this underconstrained problem: the depth predictions of corresponding points across frames should induce plausible, smooth motion in 3D. We formulate this objective in a new test-time training framework where a depth-prediction CNN is trained in tandem with an auxiliary scene-flow prediction MLP over the entire input video. By recursively unrolling the scene-flow prediction MLP over varying time steps, we compute both short-range scene flow to impose local smooth motion priors directly in 3D, and long-range scene flow to impose multi-view consistency constraints with wide baselines. We demonstrate accurate and temporally coherent results on a variety of challenging videos containing diverse moving objects (pets, people, cars), as well as camera motion. Our depth maps give rise to a number of depth-and-motion aware video editing effects such as object and lighting insertion.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Consistent Depth of Moving Objects in Video<br>pdf: <a href="https://t.co/S63yrlca9B">https://t.co/S63yrlca9B</a><br>abs: <a href="https://t.co/LYb476h5Fo">https://t.co/LYb476h5Fo</a><br><br>a method to estimate depth of a dynamic scene, containing arbitrary moving objects, from an ordinary video captured with a moving camera <a href="https://t.co/eET8CTq5R7">pic.twitter.com/eET8CTq5R7</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1422725594517803009?ref_src=twsrc%5Etfw">August 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



