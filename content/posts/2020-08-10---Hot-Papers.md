---
title: Hot Papers 2020-08-10
date: 2020-08-11T13:46:44.Z
template: "post"
draft: false
slug: "hot-papers-2020-08-10"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-08-10"
socialImage: "/media/flying-marine.jpg"

---

# 1. Benign Overfitting and Noisy Features

Zhu Li, Weijie Su, Dino Sejdinovic

- retweets: 8, favorites: 74 (08/11/2020 13:46:44)

- links: [abs](https://arxiv.org/abs/2008.02901) | [pdf](https://arxiv.org/pdf/2008.02901)
- [stat.ML](https://arxiv.org/list/stat.ML/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Modern machine learning often operates in the regime where the number of parameters is much higher than the number of data points, with zero training loss and yet good generalization, thereby contradicting the classical bias-variance trade-off. This \textit{benign overfitting} phenomenon has recently been characterized using so called \textit{double descent} curves where the risk undergoes another descent (in addition to the classical U-shaped learning curve when the number of parameters is small) as we increase the number of parameters beyond a certain threshold. In this paper, we examine the conditions under which \textit{Benign Overfitting} occurs in the random feature (RF) models, i.e. in a two-layer neural network with fixed first layer weights. We adopt a new view of random feature and show that \textit{benign overfitting} arises due to the noise which resides in such features (the noise may already be present in the data and propagate to the features or it may be added by the user to the features directly) and plays an important implicit regularization role in the phenomenon.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">New interpretation of the *double descent* phenomenon: noise in features is ubiquitous, and we show using a random feature model that noise can lead to benign overfitting. Paper: <a href="https://t.co/nK7iCNO2ba">https://t.co/nK7iCNO2ba</a>. w/ Zhu Li and Dino Sejdinovic. <a href="https://t.co/JvBbv9BxEe">pic.twitter.com/JvBbv9BxEe</a></p>&mdash; Weijie Su (@weijie444) <a href="https://twitter.com/weijie444/status/1292854888968474625?ref_src=twsrc%5Etfw">August 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. From Connectomic to Task-evoked Fingerprints: Individualized Prediction  of Task Contrasts from Resting-state Functional Connectivity

Gia H. Ngo, Meenakshi Khosla, Keith Jamison, Amy Kuceyeski, Mert R. Sabuncu

- retweets: 16, favorites: 49 (08/11/2020 13:46:44)

- links: [abs](https://arxiv.org/abs/2008.02961) | [pdf](https://arxiv.org/pdf/2008.02961)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [q-bio.NC](https://arxiv.org/list/q-bio.NC/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Resting-state functional MRI (rsfMRI) yields functional connectomes that can serve as cognitive fingerprints of individuals. Connectomic fingerprints have proven useful in many machine learning tasks, such as predicting subject-specific behavioral traits or task-evoked activity. In this work, we propose a surface-based convolutional neural network (BrainSurfCNN) model to predict individual task contrasts from their resting-state fingerprints. We introduce a reconstructive-contrastive loss that enforces subject-specificity of model outputs while minimizing predictive error. The proposed approach significantly improves the accuracy of predicted contrasts over a well-established baseline. Furthermore, BrainSurfCNN's prediction also surpasses test-retest benchmark in a subject identification task.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">From Connectomic to Task-evoked Fingerprints: Individualized Prediction of Task Contrasts from Resting-state Functional Connectivity. Our paper w/ <a href="https://twitter.com/ngohgia?ref_src=twsrc%5Etfw">@ngohgia</a> <a href="https://twitter.com/AmyKuceyeski?ref_src=twsrc%5Etfw">@AmyKuceyeski</a> <a href="https://twitter.com/meenakshik93?ref_src=twsrc%5Etfw">@meenakshik93</a>: <a href="https://t.co/ZFWRDIkhF2">https://t.co/ZFWRDIkhF2</a></p>&mdash; Mert R. Sabuncu (@mertrory) <a href="https://twitter.com/mertrory/status/1292842851387269124?ref_src=twsrc%5Etfw">August 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Unsupervised Cross-Domain Singing Voice Conversion

Adam Polyak, Lior Wolf, Yossi Adi, Yaniv Taigman

- retweets: 8, favorites: 51 (08/11/2020 13:46:45)

- links: [abs](https://arxiv.org/abs/2008.02830) | [pdf](https://arxiv.org/pdf/2008.02830)
- [eess.AS](https://arxiv.org/list/eess.AS/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.SD](https://arxiv.org/list/cs.SD/recent)

We present a wav-to-wav generative model for the task of singing voice conversion from any identity. Our method utilizes both an acoustic model, trained for the task of automatic speech recognition, together with melody extracted features to drive a waveform-based generator. The proposed generative architecture is invariant to the speaker's identity and can be trained to generate target singers from unlabeled training data, using either speech or singing sources. The model is optimized in an end-to-end fashion without any manual supervision, such as lyrics, musical notes or parallel samples. The proposed approach is fully-convolutional and can generate audio in real-time. Experiments show that our method significantly outperforms the baseline methods while generating convincingly better audio samples than alternative attempts.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Unsupervised Cross-Domain Singing Voice Conversion<br>pdf: <a href="https://t.co/FtxxGzuFp3">https://t.co/FtxxGzuFp3</a><br>abs: <a href="https://t.co/ckDw3kSlcu">https://t.co/ckDw3kSlcu</a><br>samples: <a href="https://t.co/ukfmaccEKw">https://t.co/ukfmaccEKw</a> <a href="https://t.co/AQ7aB12ZF1">pic.twitter.com/AQ7aB12ZF1</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1292628845284208640?ref_src=twsrc%5Etfw">August 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Physics-Based Dexterous Manipulations with Estimated Hand Poses and  Residual Reinforcement Learning

Guillermo Garcia-Hernando, Edward Johns, Tae-Kyun Kim

- retweets: 11, favorites: 47 (08/11/2020 13:46:45)

- links: [abs](https://arxiv.org/abs/2008.03285) | [pdf](https://arxiv.org/pdf/2008.03285)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.RO](https://arxiv.org/list/cs.RO/recent)

Dexterous manipulation of objects in virtual environments with our bare hands, by using only a depth sensor and a state-of-the-art 3D hand pose estimator (HPE), is challenging. While virtual environments are ruled by physics, e.g. object weights and surface frictions, the absence of force feedback makes the task challenging, as even slight inaccuracies on finger tips or contact points from HPE may make the interactions fail. Prior arts simply generate contact forces in the direction of the fingers' closures, when finger joints penetrate virtual objects. Although useful for simple grasping scenarios, they cannot be applied to dexterous manipulations such as in-hand manipulation. Existing reinforcement learning (RL) and imitation learning (IL) approaches train agents that learn skills by using task-specific rewards, without considering any online user input. In this work, we propose to learn a model that maps noisy input hand poses to target virtual poses, which introduces the needed contacts to accomplish the tasks on a physics simulator. The agent is trained in a residual setting by using a model-free hybrid RL+IL approach. A 3D hand pose estimation reward is introduced leading to an improvement on HPE accuracy when the physics-guided corrected target poses are remapped to the input space. As the model corrects HPE errors by applying minor but crucial joint displacements for contacts, this helps to keep the generated motion visually close to the user input. Since HPE sequences performing successful virtual interactions do not exist, a data generation scheme to train and evaluate the system is proposed. We test our framework in two applications that use hand pose estimates for dexterous manipulations: hand-object interactions in VR and hand-object motion reconstruction in-the-wild.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Physics-Based Dexterous Manipulations with Estimated Hand Poses and Residual Reinforcement Learning<a href="https://t.co/JE6xAGaWKU">https://t.co/JE6xAGaWKU</a> <a href="https://t.co/5TLwxCUHTP">pic.twitter.com/5TLwxCUHTP</a></p>&mdash; sim2real (@sim2realAIorg) <a href="https://twitter.com/sim2realAIorg/status/1292625820884717568?ref_src=twsrc%5Etfw">August 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



