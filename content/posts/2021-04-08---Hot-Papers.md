---
title: Hot Papers 2021-04-08
date: 2021-04-09T09:17:45.Z
template: "post"
draft: false
slug: "hot-papers-2021-04-08"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2021-04-08"
socialImage: "/media/flying-marine.jpg"

---

# 1. Scaling Scaling Laws with Board Games

Andrew L. Jones

- retweets: 1194, favorites: 176 (04/09/2021 09:17:45)

- links: [abs](https://arxiv.org/abs/2104.03113) | [pdf](https://arxiv.org/pdf/2104.03113)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.MA](https://arxiv.org/list/cs.MA/recent)

The largest experiments in machine learning now require resources far beyond the budget of all but a few institutions. Fortunately, it has recently been shown that the results of these huge experiments can often be extrapolated from the results of a sequence of far smaller, cheaper experiments. In this work, we show that not only can the extrapolation be done based on the size of the model, but on the size of the problem as well. By conducting a sequence of experiments using AlphaZero and Hex, we show that the performance achievable with a fixed amount of compute degrades predictably as the game gets larger and harder. Along with our main result, we further show that increasing the test-time compute available to an agent can substitute for reduced train-time compute, and vice versa.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">üö® I&#39;ve a paper out today: Scaling Scaling Laws with Board Games! üö®<a href="https://t.co/GEXhooTnAO">https://t.co/GEXhooTnAO</a><br><br>Principle result is that by studying a sequence of small problems in ML, I could predict the outcome of experiments on orders-of-magnitude larger problems  ü§Ø <a href="https://t.co/inlMMCXFWW">pic.twitter.com/inlMMCXFWW</a></p>&mdash; Andy Jones (@andy_l_jones) <a href="https://twitter.com/andy_l_jones/status/1380049754458034176?ref_src=twsrc%5Etfw">April 8, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Modern Hopfield Networks for Few- and Zero-Shot Reaction Prediction

Philipp Seidl, Philipp Renz, Natalia Dyubankova, Paulo Neves, Jonas Verhoeven, J√∂rg K. Wegner, Sepp Hochreiter, G√ºnter Klambauer

- retweets: 1088, favorites: 160 (04/09/2021 09:17:45)

- links: [abs](https://arxiv.org/abs/2104.03279) | [pdf](https://arxiv.org/pdf/2104.03279)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [q-bio.BM](https://arxiv.org/list/q-bio.BM/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

An essential step in the discovery of new drugs and materials is the synthesis of a molecule that exists so far only as an idea to test its biological and physical properties. While computer-aided design of virtual molecules has made large progress, computer-assisted synthesis planning (CASP) to realize physical molecules is still in its infancy and lacks a performance level that would enable large-scale molecule discovery. CASP supports the search for multi-step synthesis routes, which is very challenging due to high branching factors in each synthesis step and the hidden rules that govern the reactions. The central and repeatedly applied step in CASP is reaction prediction, for which machine learning methods yield the best performance. We propose a novel reaction prediction approach that uses a deep learning architecture with modern Hopfield networks (MHNs) that is optimized by contrastive learning. An MHN is an associative memory that can store and retrieve chemical reactions in each layer of a deep learning architecture. We show that our MHN contrastive learning approach enables few- and zero-shot learning for reaction prediction which, in contrast to previous methods, can deal with rare, single, or even no training example(s) for a reaction. On a well established benchmark, our MHN approach pushes the state-of-the-art performance up by a large margin as it improves the predictive top-100 accuracy from $0.858\pm0.004$ to $0.959\pm0.004$. This advance might pave the way to large-scale molecule discovery.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Modern Hopfield Networks for Few- and Zero-Shot Reaction Prediction<br><br>Pushes the SotA up by a large margin on computer-assisted molecular synthesis planning<a href="https://t.co/Boxkanl2Ma">https://t.co/Boxkanl2Ma</a> <a href="https://t.co/Ls2wuDTIiG">pic.twitter.com/Ls2wuDTIiG</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1379958465041367040?ref_src=twsrc%5Etfw">April 8, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Regularizing Generative Adversarial Networks under Limited Data

Hung-Yu Tseng, Lu Jiang, Ce Liu, Ming-Hsuan Yang, Weilong Yang

- retweets: 930, favorites: 131 (04/09/2021 09:17:46)

- links: [abs](https://arxiv.org/abs/2104.03310) | [pdf](https://arxiv.org/pdf/2104.03310)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent)

Recent years have witnessed the rapid progress of generative adversarial networks (GANs). However, the success of the GAN models hinges on a large amount of training data. This work proposes a regularization approach for training robust GAN models on limited data. We theoretically show a connection between the regularized loss and an f-divergence called LeCam-divergence, which we find is more robust under limited training data. Extensive experiments on several benchmark datasets demonstrate that the proposed regularization scheme 1) improves the generalization performance and stabilizes the learning dynamics of GAN models under limited training data, and 2) complements the recent data augmentation methods. These properties facilitate training GAN models to achieve state-of-the-art performance when only limited training data of the ImageNet benchmark is available.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Regularizing Generative Adversarial Networks under Limited Data<br>pdf: <a href="https://t.co/oBmu2v1yyp">https://t.co/oBmu2v1yyp</a><br>abs: <a href="https://t.co/dRpZkvKnt4">https://t.co/dRpZkvKnt4</a><br>github: <a href="https://t.co/OkV7RZ3xoC">https://t.co/OkV7RZ3xoC</a><br><br>training GAN models to achieve sota performance when only limited training data of the ImageNet benchmark is available <a href="https://t.co/AQkCMH3J8g">pic.twitter.com/AQkCMH3J8g</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1379970886724370435?ref_src=twsrc%5Etfw">April 8, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. GrammarTagger: A Multilingual, Minimally-Supervised Grammar Profiler for  Language Education

Masato Hagiwara, Joshua Tanner, Keisuke Sakaguchi

- retweets: 460, favorites: 135 (04/09/2021 09:17:46)

- links: [abs](https://arxiv.org/abs/2104.03190) | [pdf](https://arxiv.org/pdf/2104.03190)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

We present GrammarTagger, an open-source grammar profiler which, given an input text, identifies grammatical features useful for language education. The model architecture enables it to learn from a small amount of texts annotated with spans and their labels, which 1) enables easier and more intuitive annotation, 2) supports overlapping spans, and 3) is less prone to error propagation, compared to complex hand-crafted rules defined on constituency/dependency parses. We show that we can bootstrap a grammar profiler model with $F_1 \approx 0.6$ from only a couple hundred sentences both in English and Chinese, which can be further boosted via learning a multilingual model. With GrammarTagger, we also build Octanove Learn, a search engine of language learning materials indexed by their reading difficulty and grammatical features. The code and pretrained models are publicly available at \url{https://github.com/octanove/grammartagger}.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">üéâ We are happy to announce GrammarTagger, an open-source toolkit for grammatical profiling for language learning!<br><br>Github: <a href="https://t.co/RlcZlqL5fX">https://t.co/RlcZlqL5fX</a><br>Paper: <a href="https://t.co/zoBaR0IdYN">https://t.co/zoBaR0IdYN</a><br>Blog post: <a href="https://t.co/7eBHWhjOaP">https://t.co/7eBHWhjOaP</a><br><br>Joint work w/ Joshua Tanner and Keisuke Sakaguchi <a href="https://twitter.com/KeisukeS_?ref_src=twsrc%5Etfw">@KeisukeS_</a></p>&mdash; Masato Hagiwara (@mhagiwara) <a href="https://twitter.com/mhagiwara/status/1379971615828545537?ref_src=twsrc%5Etfw">April 8, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„Å´„Çà„Å£„Å¶„ÉÜ„Ç≠„Çπ„Éà‰∏≠„ÅÆÊñáÊ≥ïÈ†ÖÁõÆ„ÇíËß£Êûê„Åô„Çã„Ç™„Éº„Éó„É≥„ÇΩ„Éº„Çπ„ÅÆ„ÉÑ„Éº„É´„ÄÅGrammarTagger„Çí„É™„É™„Éº„Çπ„Åó„Åæ„Åó„ÅüÔºÅ<br><br>„Ç≥„Éº„Éâ: <a href="https://t.co/mMHVvSJZYc">https://t.co/mMHVvSJZYc</a><br>Ë´ñÊñá: <a href="https://t.co/LDItGarYoQ">https://t.co/LDItGarYoQ</a><br>„Éñ„É≠„Ç∞Ë®ò‰∫ã: <a href="https://t.co/xAE68BmdIu">https://t.co/xAE68BmdIu</a><br><br>„ÉØ„Ç∑„É≥„Éà„É≥Â§ß Tanner „Åï„Çì„ÄÅAI2 „ÅÆÂùÇÂè£„Åï„Çì <a href="https://twitter.com/KeisukeS_?ref_src=twsrc%5Etfw">@KeisukeS_</a> „Å®„ÅÆÂÖ±ÂêåÁ†îÁ©∂„Åß„Åô</p>&mdash; „Çπ„ÉÜ„Éº„Éà„Éª„Ç™„Éñ„ÉªAI „Ç¨„Ç§„Éâ (@stateofai_ja) <a href="https://twitter.com/stateofai_ja/status/1379974308592721921?ref_src=twsrc%5Etfw">April 8, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Efficient transfer learning for NLP with ELECTRA

Fran√ßois Mercier

- retweets: 162, favorites: 72 (04/09/2021 09:17:46)

- links: [abs](https://arxiv.org/abs/2104.02756) | [pdf](https://arxiv.org/pdf/2104.02756)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Clark et al. [2020] claims that the ELECTRA approach is highly efficient in NLP performances relative to computation budget. As such, this reproducibility study focus on this claim, summarized by the following question: Can we use ELECTRA to achieve close to SOTA performances for NLP in low-resource settings, in term of compute cost?

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Efficient transfer learning for NLP with ELECTRA ‚Ä¢ Can we use ELECTRA to achieve close to SOTA performances for NLP in low-resource settings in term of compute cost?<br><br>Paper <a href="https://t.co/UPULOBXI8I">https://t.co/UPULOBXI8I</a><br>Model <a href="https://t.co/901Alsqcf0">https://t.co/901Alsqcf0</a><br><br>_<a href="https://twitter.com/hashtag/NLP?src=hash&amp;ref_src=twsrc%5Etfw">#NLP</a> <a href="https://twitter.com/hashtag/NLProc?src=hash&amp;ref_src=twsrc%5Etfw">#NLProc</a> <a href="https://t.co/9s8pJP5GIn">pic.twitter.com/9s8pJP5GIn</a></p>&mdash; Philip Vollet (@philipvollet) <a href="https://twitter.com/philipvollet/status/1380044359610068993?ref_src=twsrc%5Etfw">April 8, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. Neural Articulated Radiance Field

Atsuhiro Noguchi, Xiao Sun, Stephen Lin, Tatsuya Harada

- retweets: 143, favorites: 90 (04/09/2021 09:17:46)

- links: [abs](https://arxiv.org/abs/2104.03110) | [pdf](https://arxiv.org/pdf/2104.03110)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

We present Neural Articulated Radiance Field (NARF), a novel deformable 3D representation for articulated objects learned from images. While recent advances in 3D implicit representation have made it possible to learn models of complex objects, learning pose-controllable representations of articulated objects remains a challenge, as current methods require 3D shape supervision and are unable to render appearance. In formulating an implicit representation of 3D articulated objects, our method considers only the rigid transformation of the most relevant object part in solving for the radiance field at each 3D location. In this way, the proposed method represents pose-dependent changes without significantly increasing the computational complexity. NARF is fully differentiable and can be trained from images with pose annotations. Moreover, through the use of an autoencoder, it can learn appearance variations over multiple instances of an object class. Experiments show that the proposed method is efficient and can generalize well to novel poses. We make the code, model and demo available for research purposes at https://github.com/nogu-atsu/NARF

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Neural Articulated Radiance Field<br>pdf: <a href="https://t.co/rxB9gj5CLO">https://t.co/rxB9gj5CLO</a><br>abs: <a href="https://t.co/XWQPSM7wpq">https://t.co/XWQPSM7wpq</a> <a href="https://t.co/EDcJeytQXt">pic.twitter.com/EDcJeytQXt</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1379962651938291713?ref_src=twsrc%5Etfw">April 8, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language  Representation Learning

Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, Jianlong Fu

- retweets: 129, favorites: 91 (04/09/2021 09:17:46)

- links: [abs](https://arxiv.org/abs/2104.03135) | [pdf](https://arxiv.org/pdf/2104.03135)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

We study joint learning of Convolutional Neural Network (CNN) and Transformer for vision-language pre-training (VLPT) which aims to learn cross-modal alignments from millions of image-text pairs. State-of-the-art approaches extract salient image regions and align regions with words step-by-step. As region-based visual features usually represent parts of an image, it is challenging for existing vision-language models to fully understand the semantics from paired natural languages. In this paper, we propose SOHO to "See Out of tHe bOx" that takes a whole image as input, and learns vision-language representation in an end-to-end manner. SOHO does not require bounding box annotations which enables inference 10 times faster than region-based approaches. In particular, SOHO learns to extract comprehensive yet compact image features through a visual dictionary (VD) that facilitates cross-modal understanding. VD is designed to represent consistent visual abstractions of similar semantics. It is updated on-the-fly and utilized in our proposed pre-training task Masked Visual Modeling (MVM). We conduct experiments on four well-established vision-language tasks by following standard VLPT settings. In particular, SOHO achieves absolute gains of 2.0% R@1 score on MSCOCO text retrieval 5k test split, 1.5% accuracy on NLVR$^2$ test-P split, 6.7% accuracy on SNLI-VE test split, respectively.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning<br>pdf: <a href="https://t.co/9L4kYkdZyz">https://t.co/9L4kYkdZyz</a><br>abs: <a href="https://t.co/DBpBzJN9ZG">https://t.co/DBpBzJN9ZG</a> <a href="https://t.co/998ltZHTv6">pic.twitter.com/998ltZHTv6</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1379960265010188288?ref_src=twsrc%5Etfw">April 8, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning<br><br>Proposes SOHO, which achieves SotA performance on various vision-language tasks.  <a href="https://t.co/je8Iw6G1NY">https://t.co/je8Iw6G1NY</a> <a href="https://t.co/jJVGLe6EGV">pic.twitter.com/jJVGLe6EGV</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1379962581398347776?ref_src=twsrc%5Etfw">April 8, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. On Self-Contact and Human Pose

Lea M√ºller, Ahmed A. A. Osman, Siyu Tang, Chun-Hao P. Huang, Michael J. Black

- retweets: 96, favorites: 121 (04/09/2021 09:17:47)

- links: [abs](https://arxiv.org/abs/2104.03176) | [pdf](https://arxiv.org/pdf/2104.03176)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

People touch their face 23 times an hour, they cross their arms and legs, put their hands on their hips, etc. While many images of people contain some form of self-contact, current 3D human pose and shape (HPS) regression methods typically fail to estimate this contact. To address this, we develop new datasets and methods that significantly improve human pose estimation with self-contact. First, we create a dataset of 3D Contact Poses (3DCP) containing SMPL-X bodies fit to 3D scans as well as poses from AMASS, which we refine to ensure good contact. Second, we leverage this to create the Mimic-The-Pose (MTP) dataset of images, collected via Amazon Mechanical Turk, containing people mimicking the 3DCP poses with selfcontact. Third, we develop a novel HPS optimization method, SMPLify-XMC, that includes contact constraints and uses the known 3DCP body pose during fitting to create near ground-truth poses for MTP images. Fourth, for more image variety, we label a dataset of in-the-wild images with Discrete Self-Contact (DSC) information and use another new optimization method, SMPLify-DC, that exploits discrete contacts during pose optimization. Finally, we use our datasets during SPIN training to learn a new 3D human pose regressor, called TUCH (Towards Understanding Contact in Humans). We show that the new self-contact training data significantly improves 3D human pose estimates on withheld test data and existing datasets like 3DPW. Not only does our method improve results for self-contact poses, but it also improves accuracy for non-contact poses. The code and data are available for research purposes at https://tuch.is.tue.mpg.de.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">TUCH: training human pose and shape regression with novel *self-contact* losses improves accuracy even on poses without self-contact. Mimic-The-Pose (MTP): novel crowd-sourced, high-quality, 3D reference data with self-contact. <a href="https://twitter.com/CVPR?ref_src=twsrc%5Etfw">@CVPR</a> (<a href="https://twitter.com/hashtag/CVPR2021?src=hash&amp;ref_src=twsrc%5Etfw">#CVPR2021</a> oral). <a href="https://t.co/ynr4cmqD6b">https://t.co/ynr4cmqD6b</a> <a href="https://t.co/BvHW5tTA85">pic.twitter.com/BvHW5tTA85</a></p>&mdash; Michael Black (@Michael_J_Black) <a href="https://twitter.com/Michael_J_Black/status/1380099716571136000?ref_src=twsrc%5Etfw">April 8, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">On Self-Contact and Human Pose<br>pdf: <a href="https://t.co/zptiMINVvb">https://t.co/zptiMINVvb</a><br>abs: <a href="https://t.co/GCTfe9qrh9">https://t.co/GCTfe9qrh9</a><br>project page: <a href="https://t.co/YW5qKwBNmm">https://t.co/YW5qKwBNmm</a> <a href="https://t.co/jFaPO5UEgu">pic.twitter.com/jFaPO5UEgu</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1380016782757523457?ref_src=twsrc%5Etfw">April 8, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. NeuMIP: Multi-Resolution Neural Materials

Alexandr Kuznetsov, Krishna Mullia, Zexiang Xu, Milo≈° Ha≈°an, Ravi Ramamoorthi

- retweets: 156, favorites: 41 (04/09/2021 09:17:47)

- links: [abs](https://arxiv.org/abs/2104.02789) | [pdf](https://arxiv.org/pdf/2104.02789)
- [cs.GR](https://arxiv.org/list/cs.GR/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [eess.IV](https://arxiv.org/list/eess.IV/recent)

We propose NeuMIP, a neural method for representing and rendering a variety of material appearances at different scales. Classical prefiltering (mipmapping) methods work well on simple material properties such as diffuse color, but fail to generalize to normals, self-shadowing, fibers or more complex microstructures and reflectances. In this work, we generalize traditional mipmap pyramids to pyramids of neural textures, combined with a fully connected network. We also introduce neural offsets, a novel method which allows rendering materials with intricate parallax effects without any tessellation. This generalizes classical parallax mapping, but is trained without supervision by any explicit heightfield. Neural materials within our system support a 7-dimensional query, including position, incoming and outgoing direction, and the desired filter kernel size. The materials have small storage (on the order of standard mipmapping except with more texture channels), and can be integrated within common Monte-Carlo path tracing systems. We demonstrate our method on a variety of materials, resulting in complex appearance across levels of detail, with accurate parallax, self-shadowing, and other effects.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Cool new Multi-Resolution Neural Material research from Adobe Research. <a href="https://t.co/W6LJj3J6QI">https://t.co/W6LJj3J6QI</a> <a href="https://t.co/krbn95wU1v">pic.twitter.com/krbn95wU1v</a></p>&mdash; Jonathan Granskog (@jongranskog) <a href="https://twitter.com/jongranskog/status/1380052349570400276?ref_src=twsrc%5Etfw">April 8, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 10. Visual Vibration Tomography: Estimating Interior Material Properties  from Monocular Video

Berthy Feng, Alexander C. Ogren, Chiara Daraio, Katherine L. Bouman

- retweets: 90, favorites: 33 (04/09/2021 09:17:47)

- links: [abs](https://arxiv.org/abs/2104.02735) | [pdf](https://arxiv.org/pdf/2104.02735)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [eess.IV](https://arxiv.org/list/eess.IV/recent)

An object's interior material properties, while invisible to the human eye, determine motion observed on its surface. We propose an approach that estimates heterogeneous material properties of an object directly from a monocular video of its surface vibrations. Specifically, we estimate Young's modulus and density throughout a 3D object with known geometry. Knowledge of how these values change across the object is useful for characterizing defects and simulating how the object will interact with different environments. Traditional non-destructive testing approaches, which generally estimate homogenized material properties or the presence of defects, are expensive and use specialized instruments. We propose an approach that leverages monocular video to (1) measure and object's sub-pixel motion and decompose this motion into image-space modes, and (2) directly infer spatially-varying Young's modulus and density values from the observed image-space modes. On both simulated and real videos, we demonstrate that our approach is able to image material properties simply by analyzing surface motion. In particular, our method allows us to identify unseen defects on a 2D drum head from real, high-speed video.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Visual Vibration Tomography: Estimating Interior Material Properties from Monocular Video <a href="https://t.co/LYxbOB7BJU">https://t.co/LYxbOB7BJU</a> <a href="https://twitter.com/hashtag/computervision?src=hash&amp;ref_src=twsrc%5Etfw">#computervision</a> <a href="https://t.co/Roy1KtWfyB">pic.twitter.com/Roy1KtWfyB</a></p>&mdash; Tomasz Malisiewicz (@quantombone) <a href="https://twitter.com/quantombone/status/1379975951954673664?ref_src=twsrc%5Etfw">April 8, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 11. MultiScene: A Large-scale Dataset and Benchmark for Multi-scene  Recognition in Single Aerial Images

Yuansheng Hua, Lichao Mou, Pu Jin, Xiao Xiang Zhu

- retweets: 72, favorites: 38 (04/09/2021 09:17:47)

- links: [abs](https://arxiv.org/abs/2104.02846) | [pdf](https://arxiv.org/pdf/2104.02846)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Aerial scene recognition is a fundamental research problem in interpreting high-resolution aerial imagery. Over the past few years, most studies focus on classifying an image into one scene category, while in real-world scenarios, it is more often that a single image contains multiple scenes. Therefore, in this paper, we investigate a more practical yet underexplored task -- multi-scene recognition in single images. To this end, we create a large-scale dataset, called MultiScene, composed of 100,000 unconstrained high-resolution aerial images. Considering that manually labeling such images is extremely arduous, we resort to low-cost annotations from crowdsourcing platforms, e.g., OpenStreetMap (OSM). However, OSM data might suffer from incompleteness and incorrectness, which introduce noise into image labels. To address this issue, we visually inspect 14,000 images and correct their scene labels, yielding a subset of cleanly-annotated images, named MultiScene-Clean. With it, we can develop and evaluate deep networks for multi-scene recognition using clean data. Moreover, we provide crowdsourced annotations of all images for the purpose of studying network learning with noisy labels. We conduct experiments with extensive baseline models on both MultiScene-Clean and MultiScene to offer benchmarks for multi-scene recognition in single images and learning from noisy labels for this task, respectively. To facilitate progress, we will make our dataset and pre-trained models available.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Interested in a new <a href="https://twitter.com/hashtag/AI4EO?src=hash&amp;ref_src=twsrc%5Etfw">#AI4EO</a> task - multi-scene classification in single aerial images? <br><br>We <a href="https://twitter.com/Zhu_XLab?ref_src=twsrc%5Etfw">@Zhu_XLab</a> <a href="https://twitter.com/ai4eo_de?ref_src=twsrc%5Etfw">@ai4eo_de</a>  are sharing a large-scale benchmark, called <a href="https://twitter.com/hashtag/MultiScene?src=hash&amp;ref_src=twsrc%5Etfw">#MultiScene</a>, composed of 100,000  high-resolution aerial images. Stay tuned!<br><br>Link to paper: <a href="https://t.co/ueM6SzJ1ep">https://t.co/ueM6SzJ1ep</a> <a href="https://t.co/1FiCL1xvQo">pic.twitter.com/1FiCL1xvQo</a></p>&mdash; Xiaoxiang ZHU (@xiaoxiang_zhu) <a href="https://twitter.com/xiaoxiang_zhu/status/1380099795612733442?ref_src=twsrc%5Etfw">April 8, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 12. SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks

Shunsuke Saito, Jinlong Yang, Qianli Ma, Michael J. Black

- retweets: 56, favorites: 54 (04/09/2021 09:17:47)

- links: [abs](https://arxiv.org/abs/2104.03313) | [pdf](https://arxiv.org/pdf/2104.03313)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

We present SCANimate, an end-to-end trainable framework that takes raw 3D scans of a clothed human and turns them into an animatable avatar. These avatars are driven by pose parameters and have realistic clothing that moves and deforms naturally. SCANimate does not rely on a customized mesh template or surface mesh registration. We observe that fitting a parametric 3D body model, like SMPL, to a clothed human scan is tractable while surface registration of the body topology to the scan is often not, because clothing can deviate significantly from the body shape. We also observe that articulated transformations are invertible, resulting in geometric cycle consistency in the posed and unposed shapes. These observations lead us to a weakly supervised learning method that aligns scans into a canonical pose by disentangling articulated deformations without template-based surface registration. Furthermore, to complete missing regions in the aligned scans while modeling pose-dependent deformations, we introduce a locally pose-aware implicit function that learns to complete and model geometry with learned pose correctives. In contrast to commonly used global pose embeddings, our local pose conditioning significantly reduces long-range spurious correlations and improves generalization to unseen poses, especially when training data is limited. Our method can be applied to pose-aware appearance modeling to generate a fully textured avatar. We demonstrate our approach on various clothing types with different amounts of training data, outperforming existing solutions and other variants in terms of fidelity and generality in every setting. The code is available at https://scanimate.is.tue.mpg.de.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks<br>pdf: <a href="https://t.co/zqbd59n3pW">https://t.co/zqbd59n3pW</a><br>abs: <a href="https://t.co/xjK0o2iPSJ">https://t.co/xjK0o2iPSJ</a><br>project page: <a href="https://t.co/EoobL7RxS0">https://t.co/EoobL7RxS0</a> <a href="https://t.co/Ht97evFp61">pic.twitter.com/Ht97evFp61</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1380018124792201217?ref_src=twsrc%5Etfw">April 8, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 13. Creativity and Machine Learning: a Survey

Giorgio Franceschelli, Mirco Musolesi

- retweets: 64, favorites: 33 (04/09/2021 09:17:47)

- links: [abs](https://arxiv.org/abs/2104.02726) | [pdf](https://arxiv.org/pdf/2104.02726)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.CY](https://arxiv.org/list/cs.CY/recent)

There is a growing interest in the area of machine learning and creativity. This survey presents an overview of the history and the state of the art of computational creativity theories, machine learning techniques, including generative deep learning, and corresponding automatic evaluation methods. After presenting a critical discussion of the key contributions in this area, we outline the current research challenges and emerging opportunities in this field.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">New arXiv pre-print: &quot;Creativity and Machine Learning: A Survey&quot;. Machine learning and artistic creation, from Ada Lovelace to GANs and GPT-3 (and beyond). Not only methods, but also metrics for evaluating them.  <a href="https://t.co/dviVmaEDRY">https://t.co/dviVmaEDRY</a> <br><br>\w <a href="https://twitter.com/Gionceschelli?ref_src=twsrc%5Etfw">@Gionceschelli</a> <a href="https://t.co/fj19F9im2P">pic.twitter.com/fj19F9im2P</a></p>&mdash; Mirco Musolesi (@mircomusolesi) <a href="https://twitter.com/mircomusolesi/status/1380125963732856834?ref_src=twsrc%5Etfw">April 8, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 14. The Power of Subsampling in Submodular Maximization

Christopher Harshaw, Ehsan Kazemi, Moran Feldman, Amin Karbasi

- retweets: 6, favorites: 87 (04/09/2021 09:17:48)

- links: [abs](https://arxiv.org/abs/2104.02772) | [pdf](https://arxiv.org/pdf/2104.02772)
- [cs.DS](https://arxiv.org/list/cs.DS/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [math.OC](https://arxiv.org/list/math.OC/recent)

We propose subsampling as a unified algorithmic technique for submodular maximization in centralized and online settings. The idea is simple: independently sample elements from the ground set, and use simple combinatorial techniques (such as greedy or local search) on these sampled elements. We show that this approach leads to optimal/state-of-the-art results despite being much simpler than existing methods. In the usual offline setting, we present SampleGreedy, which obtains a $(p + 2 + o(1))$-approximation for maximizing a submodular function subject to a $p$-extendible system using $O(n + nk/p)$ evaluation and feasibility queries, where $k$ is the size of the largest feasible set. The approximation ratio improves to $p+1$ and $p$ for monotone submodular and linear objectives, respectively. In the streaming setting, we present SampleStreaming, which obtains a $(4p +2 - o(1))$-approximation for maximizing a submodular function subject to a $p$-matchoid using $O(k)$ memory and $O(km/p)$ evaluation and feasibility queries per element, where $m$ is the number of matroids defining the $p$-matchoid. The approximation ratio improves to $4p$ for monotone submodular objectives. We empirically demonstrate the effectiveness of our algorithms on video summarization, location summarization, and movie recommendation tasks.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">In this paper, just accepted to the Mathematics of Operations Research, we study the Nyquist rate of subsampling for submodular maximization. A simple idea that provides the currently best-known approximation guarantee in offline and streaming settings. <a href="https://t.co/BFpNpudTkR">https://t.co/BFpNpudTkR</a> <a href="https://t.co/YfYCnlKJIG">pic.twitter.com/YfYCnlKJIG</a></p>&mdash; Amin Karbasi (@aminkarbasi) <a href="https://twitter.com/aminkarbasi/status/1379981465451122688?ref_src=twsrc%5Etfw">April 8, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 15. Self-supervised Learning of Depth Inference for Multi-view Stereo

Jiayu Yang, Jose M. Alvarez, Miaomiao Liu

- retweets: 37, favorites: 37 (04/09/2021 09:17:48)

- links: [abs](https://arxiv.org/abs/2104.02972) | [pdf](https://arxiv.org/pdf/2104.02972)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Recent supervised multi-view depth estimation networks have achieved promising results. Similar to all supervised approaches, these networks require ground-truth data during training. However, collecting a large amount of multi-view depth data is very challenging. Here, we propose a self-supervised learning framework for multi-view stereo that exploit pseudo labels from the input data. We start by learning to estimate depth maps as initial pseudo labels under an unsupervised learning framework relying on image reconstruction loss as supervision. We then refine the initial pseudo labels using a carefully designed pipeline leveraging depth information inferred from higher resolution images and neighboring views. We use these high-quality pseudo labels as the supervision signal to train the network and improve, iteratively, its performance by self-training. Extensive experiments on the DTU dataset show that our proposed self-supervised learning framework outperforms existing unsupervised multi-view stereo networks by a large margin and performs on par compared to the supervised counterpart. Code is available at https://github.com/JiayuYANG/Self-supervised-CVP-MVSNet.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Self-supervised Learning of Depth Inference for Multi-view Stereo<br>pdf: <a href="https://t.co/15O9lhhjUB">https://t.co/15O9lhhjUB</a><br>abs: <a href="https://t.co/os1BMpJXNJ">https://t.co/os1BMpJXNJ</a> <a href="https://t.co/T8aveFxGrC">pic.twitter.com/T8aveFxGrC</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1379966758304550922?ref_src=twsrc%5Etfw">April 8, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



