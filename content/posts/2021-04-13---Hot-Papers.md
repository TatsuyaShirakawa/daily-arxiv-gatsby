---
title: Hot Papers 2021-04-13
date: 2021-04-14T11:53:41.Z
template: "post"
draft: false
slug: "hot-papers-2021-04-13"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2021-04-13"
socialImage: "/media/flying-marine.jpg"

---

# 1. MobileStyleGAN: A Lightweight Convolutional Neural Network for  High-Fidelity Image Synthesis

Sergei Belousov

- retweets: 11274, favorites: 39 (04/14/2021 11:53:41)

- links: [abs](https://arxiv.org/abs/2104.04767) | [pdf](https://arxiv.org/pdf/2104.04767)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [eess.IV](https://arxiv.org/list/eess.IV/recent)

In recent years, the use of Generative Adversarial Networks (GANs) has become very popular in generative image modeling. While style-based GAN architectures yield state-of-the-art results in high-fidelity image synthesis, computationally, they are highly complex. In our work, we focus on the performance optimization of style-based generative models. We analyze the most computationally hard parts of StyleGAN2, and propose changes in the generator network to make it possible to deploy style-based generative networks in the edge devices. We introduce MobileStyleGAN architecture, which has x3.5 fewer parameters and is x9.5 less computationally complex than StyleGAN2, while providing comparable quality.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">MobileStyleGAN: A Lightweight Convolutional Neural Network for High-Fidelity Image Synthesis<br>pdf: <a href="https://t.co/SILNLZkZqx">https://t.co/SILNLZkZqx</a><br>abs: <a href="https://t.co/eh0sNbBq2G">https://t.co/eh0sNbBq2G</a> <a href="https://t.co/C7M7LgTujo">pic.twitter.com/C7M7LgTujo</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1381786189611593731?ref_src=twsrc%5Etfw">April 13, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Voting-based probabilistic consensuses and their applications in  distributed ledgers

Serguei Popov, Sebastian M√ºller

- retweets: 6226, favorites: 531 (04/14/2021 11:53:42)

- links: [abs](https://arxiv.org/abs/2104.05313) | [pdf](https://arxiv.org/pdf/2104.05313)
- [cs.DC](https://arxiv.org/list/cs.DC/recent)

We review probabilistic models known as majority dynamics (also known as threshold Voter Models) and discuss their possible applications for achieving consensus in cryptocurrency systems. In particular, we show that using this approach straightforwardly for practical consensus in Byzantine setting can be problematic and requires extensive further research. We then discuss the FPC consensus protocol which circumvents the problems mentioned above by using external randomness.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">In our latest  <a href="https://twitter.com/hashtag/IOTA?src=hash&amp;ref_src=twsrc%5Etfw">#IOTA</a> paper, we review voting-based <a href="https://twitter.com/hashtag/consensus?src=hash&amp;ref_src=twsrc%5Etfw">#consensus</a> protocols, explain the <a href="https://twitter.com/hashtag/magic?src=hash&amp;ref_src=twsrc%5Etfw">#magic</a> of <a href="https://twitter.com/hashtag/FPC?src=hash&amp;ref_src=twsrc%5Etfw">#FPC</a>, and why one should always be skeptical. By <a href="https://twitter.com/mthcl_crypto?ref_src=twsrc%5Etfw">@mthcl_crypto</a> and <a href="https://twitter.com/NaitsabesMue?ref_src=twsrc%5Etfw">@NaitsabesMue</a>. Check it here: <a href="https://t.co/9TSdTtuBnh">https://t.co/9TSdTtuBnh</a></p>&mdash; IOTA (@iota) <a href="https://twitter.com/iota/status/1381927313575673858?ref_src=twsrc%5Etfw">April 13, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Representation Learning for Networks in Biology and Medicine:  Advancements, Challenges, and Opportunities

Michelle M. Li, Kexin Huang, Marinka Zitnik

- retweets: 5588, favorites: 347 (04/14/2021 11:53:42)

- links: [abs](https://arxiv.org/abs/2104.04883) | [pdf](https://arxiv.org/pdf/2104.04883)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.SI](https://arxiv.org/list/cs.SI/recent) | [q-bio.BM](https://arxiv.org/list/q-bio.BM/recent) | [q-bio.GN](https://arxiv.org/list/q-bio.GN/recent) | [q-bio.MN](https://arxiv.org/list/q-bio.MN/recent)

With the remarkable success of representation learning in providing powerful predictions and data insights, we have witnessed a rapid expansion of representation learning techniques into modeling, analysis, and learning with networks. Biomedical networks are universal descriptors of systems of interacting elements, from protein interactions to disease networks, all the way to healthcare systems and scientific knowledge. In this review, we put forward an observation that long-standing principles of network biology and medicine -- while often unspoken in machine learning research -- can provide the conceptual grounding for representation learning, explain its current successes and limitations, and inform future advances. We synthesize a spectrum of algorithmic approaches that, at their core, leverage topological features to embed networks into compact vector spaces. We also provide a taxonomy of biomedical areas that are likely to benefit most from algorithmic innovation. Representation learning techniques are becoming essential for identifying causal variants underlying complex traits, disentangling behaviors of single cells and their impact on health, and diagnosing and treating diseases with safe and effective medicines.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Survey on Representation Learning for Networks in Biology and Medicine <a href="https://t.co/1Aby9wNMcV">https://t.co/1Aby9wNMcV</a><br>Long-standing principles of biomed nets (often unspoken in ML) provide grounding for representation learning, explain successes &amp; limitations <a href="https://twitter.com/_michellemli?ref_src=twsrc%5Etfw">@_michellemli</a> <a href="https://twitter.com/KexinHuang5?ref_src=twsrc%5Etfw">@KexinHuang5</a> <a href="https://twitter.com/hashtag/netbio?src=hash&amp;ref_src=twsrc%5Etfw">#netbio</a> <a href="https://twitter.com/hashtag/GNN?src=hash&amp;ref_src=twsrc%5Etfw">#GNN</a> <a href="https://twitter.com/hashtag/ML?src=hash&amp;ref_src=twsrc%5Etfw">#ML</a> <a href="https://t.co/Y71r3WK84l">pic.twitter.com/Y71r3WK84l</a></p>&mdash; Marinka Zitnik (@marinkazitnik) <a href="https://twitter.com/marinkazitnik/status/1381995570647199754?ref_src=twsrc%5Etfw">April 13, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Getting to the Point. Index Sets and Parallelism-Preserving Autodiff for  Pointful Array Programming

Adam Paszke, Daniel Johnson, David Duvenaud, Dimitrios Vytiniotis, Alexey Radul, Matthew Johnson, Jonathan Ragan-Kelley, Dougal Maclaurin

- retweets: 2400, favorites: 255 (04/14/2021 11:53:42)

- links: [abs](https://arxiv.org/abs/2104.05372) | [pdf](https://arxiv.org/pdf/2104.05372)
- [cs.PL](https://arxiv.org/list/cs.PL/recent)

We present a novel programming language design that attempts to combine the clarity and safety of high-level functional languages with the efficiency and parallelism of low-level numerical languages. We treat arrays as eagerly-memoized functions on typed index sets, allowing abstract function manipulations, such as currying, to work on arrays. In contrast to composing primitive bulk-array operations, we argue for an explicit nested indexing style that mirrors application of functions to arguments. We also introduce a fine-grained typed effects system which affords concise and automatically-parallelized in-place updates. Specifically, an associative accumulation effect allows reverse-mode automatic differentiation of in-place updates in a way that preserves parallelism. Empirically, we benchmark against the Futhark array programming language, and demonstrate that aggressive inlining and type-driven compilation allows array programs to be written in an expressive, "pointful" style with little performance penalty.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Curious what we think the future of array computing can look like? Check out our new Dex preprint to see how we‚Äôre designing a language for safe, expressive parallelism (incl. no shape errors!) and efficient AD. Available at <a href="https://t.co/ae4cOQjoZ8">https://t.co/ae4cOQjoZ8</a></p>&mdash; Adam Paszke (@apaszke) <a href="https://twitter.com/apaszke/status/1381941073312952320?ref_src=twsrc%5Etfw">April 13, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Escaping the Big Data Paradigm with Compact Transformers

Ali Hassani, Steven Walton, Nikhil Shah, Abulikemu Abuduweili, Jiachen Li, Humphrey Shi

- retweets: 1247, favorites: 194 (04/14/2021 11:53:42)

- links: [abs](https://arxiv.org/abs/2104.05704) | [pdf](https://arxiv.org/pdf/2104.05704)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

With the rise of Transformers as the standard for language processing, and their advancements in computer vision, along with their unprecedented size and amounts of training data, many have come to believe that they are not suitable for small sets of data. This trend leads to great concerns, including but not limited to: limited availability of data in certain scientific domains and the exclusion of those with limited resource from research in the field. In this paper, we dispel the myth that transformers are "data hungry" and therefore can only be applied to large sets of data. We show for the first time that with the right size and tokenization, transformers can perform head-to-head with state-of-the-art CNNs on small datasets. Our model eliminates the requirement for class token and positional embeddings through a novel sequence pooling strategy and the use of convolutions. We show that compared to CNNs, our compact transformers have fewer parameters and MACs, while obtaining similar accuracies. Our method is flexible in terms of model size, and can have as little as 0.28M parameters and achieve reasonable results. It can reach an accuracy of 94.72% when training from scratch on CIFAR-10, which is comparable with modern CNN based approaches, and a significant improvement over previous Transformer based models. Our simple and compact design democratizes transformers by making them accessible to those equipped with basic computing resources and/or dealing with important small datasets. Our code and pre-trained models will be made publicly available at https://github.com/SHI-Labs/Compact-Transformers.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Escaping the Big Data Paradigm with Compact Transformers<br>pdf: <a href="https://t.co/izACyF4fL3">https://t.co/izACyF4fL3</a><br>abs: <a href="https://t.co/DxMmNbDwgg">https://t.co/DxMmNbDwgg</a><br>github: <a href="https://t.co/0NakpcqZ6G">https://t.co/0NakpcqZ6G</a><br>&quot;that with the right size and tokenization, transformers can perform head-to-head with state-of-the-art CNNs on small datasets&quot; <a href="https://t.co/Ync7fo9DjL">pic.twitter.com/Ync7fo9DjL</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1381775750286704641?ref_src=twsrc%5Etfw">April 13, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Let&#39;s make Transformers accessible and bring them into the hands of everyone -- esp for those dealing with limited computing resources and small datasets! Train Compact Transformers on CIFAR-10 in 30 minutes or less with a single GPU! <a href="https://twitter.com/hashtag/democratizeAI?src=hash&amp;ref_src=twsrc%5Etfw">#democratizeAI</a> <a href="https://t.co/kEmgA2W7UX">https://t.co/kEmgA2W7UX</a> <a href="https://t.co/060AgwWGHt">pic.twitter.com/060AgwWGHt</a></p>&mdash; Humphrey Shi (@humphrey_shi) <a href="https://twitter.com/humphrey_shi/status/1381776290211061762?ref_src=twsrc%5Etfw">April 13, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. Understanding Overparameterization in Generative Adversarial Networks

Yogesh Balaji, Mohammadmahdi Sajedi, Neha Mukund Kalibhat, Mucong Ding, Dominik St√∂ger, Mahdi Soltanolkotabi, Soheil Feizi

- retweets: 1122, favorites: 201 (04/14/2021 11:53:43)

- links: [abs](https://arxiv.org/abs/2104.05605) | [pdf](https://arxiv.org/pdf/2104.05605)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

A broad class of unsupervised deep learning methods such as Generative Adversarial Networks (GANs) involve training of overparameterized models where the number of parameters of the model exceeds a certain threshold. A large body of work in supervised learning have shown the importance of model overparameterization in the convergence of the gradient descent (GD) to globally optimal solutions. In contrast, the unsupervised setting and GANs in particular involve non-convex concave mini-max optimization problems that are often trained using Gradient Descent/Ascent (GDA). The role and benefits of model overparameterization in the convergence of GDA to a global saddle point in non-convex concave problems is far less understood. In this work, we present a comprehensive analysis of the importance of model overparameterization in GANs both theoretically and empirically. We theoretically show that in an overparameterized GAN model with a $1$-layer neural network generator and a linear discriminator, GDA converges to a global saddle point of the underlying non-convex concave min-max problem. To the best of our knowledge, this is the first result for global convergence of GDA in such settings. Our theory is based on a more general result that holds for a broader class of nonlinear generators and discriminators that obey certain assumptions (including deeper generators and random feature discriminators). We also empirically study the role of model overparameterization in GANs using several large-scale experiments on CIFAR-10 and Celeb-A datasets. Our experiments show that overparameterization improves the quality of generated samples across various model architectures and datasets. Remarkably, we observe that overparameterization leads to faster and more stable convergence behavior of GDA across the board.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Deep learning often requires solving non-convex concave min-max problems. <br><br>In our <a href="https://twitter.com/hashtag/ICLR2021?src=hash&amp;ref_src=twsrc%5Etfw">#ICLR2021</a> work, we prove (for the first time) the &quot;global&quot; convergence of gradient descent/ascent for a GAN-based min-max problem in an overparameterized regime. <br><br>Paper: <a href="https://t.co/perPU5Fb3g">https://t.co/perPU5Fb3g</a><br>üëá <a href="https://t.co/c6yBLaa9T7">pic.twitter.com/c6yBLaa9T7</a></p>&mdash; Soheil Feizi (@FeiziSoheil) <a href="https://twitter.com/FeiziSoheil/status/1381959365016616962?ref_src=twsrc%5Etfw">April 13, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. LocalViT: Bringing Locality to Vision Transformers

Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, Luc Van Gool

- retweets: 441, favorites: 133 (04/14/2021 11:53:43)

- links: [abs](https://arxiv.org/abs/2104.05707) | [pdf](https://arxiv.org/pdf/2104.05707)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

We study how to introduce locality mechanisms into vision transformers. The transformer network originates from machine translation and is particularly good at modelling long-range dependencies within a long sequence. Although the global interaction between the token embeddings could be well modelled by the self-attention mechanism of transformers, what is lacking a locality mechanism for information exchange within a local region. Yet, locality is essential for images since it pertains to structures like lines, edges, shapes, and even objects.   We add locality to vision transformers by introducing depth-wise convolution into the feed-forward network. This seemingly simple solution is inspired by the comparison between feed-forward networks and inverted residual blocks. The importance of locality mechanisms is validated in two ways: 1) A wide range of design choices (activation function, layer placement, expansion ratio) are available for incorporating locality mechanisms and all proper choices can lead to a performance gain over the baseline, and 2) The same locality mechanism is successfully applied to 4 vision transformers, which shows the generalization of the locality concept. In particular, for ImageNet2012 classification, the locality-enhanced transformers outperform the baselines DeiT-T and PVT-T by 2.6\% and 3.1\% with a negligible increase in the number of parameters and computational effort. Code is available at \url{https://github.com/ofsoundof/LocalViT}.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">LocalViT: Bringing Locality to Vision Transformers<br><br>Outperforms DeiT-T and PVT-T by 2.6% and 3.1% w/ comparable params and computes by adding locality to vision transformers by introducing depth-wise convs into FFN.<br><br>abs: <a href="https://t.co/gOFr6QXfPf">https://t.co/gOFr6QXfPf</a><br>code: <a href="https://t.co/1kvGZy7pEC">https://t.co/1kvGZy7pEC</a> <a href="https://t.co/BpcxsWrEwT">pic.twitter.com/BpcxsWrEwT</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1381773878507139072?ref_src=twsrc%5Etfw">April 13, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">LocalViT: Bringing Locality to Vision Transformers<br>pdf: <a href="https://t.co/UkIpEiaRme">https://t.co/UkIpEiaRme</a><br>abs: <a href="https://t.co/HfSdnR1LNS">https://t.co/HfSdnR1LNS</a><br>&quot;We add locality to vision transformers by introducing<br>depth-wise convolution into the feed-forward network&quot; <a href="https://t.co/rMcw62bOjR">pic.twitter.com/rMcw62bOjR</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1381776496101109769?ref_src=twsrc%5Etfw">April 13, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. Neural RGB-D Surface Reconstruction

Dejan Azinoviƒá, Ricardo Martin-Brualla, Dan B Goldman, Matthias Nie√üner, Justus Thies

- retweets: 244, favorites: 95 (04/14/2021 11:53:43)

- links: [abs](https://arxiv.org/abs/2104.04532) | [pdf](https://arxiv.org/pdf/2104.04532)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

In this work, we explore how to leverage the success of implicit novel view synthesis methods for surface reconstruction. Methods which learn a neural radiance field have shown amazing image synthesis results, but the underlying geometry representation is only a coarse approximation of the real geometry. We demonstrate how depth measurements can be incorporated into the radiance field formulation to produce more detailed and complete reconstruction results than using methods based on either color or depth data alone. In contrast to a density field as the underlying geometry representation, we propose to learn a deep neural network which stores a truncated signed distance field. Using this representation, we show that one can still leverage differentiable volume rendering to estimate color values of the observed images during training to compute a reconstruction loss. This is beneficial for learning the signed distance field in regions with missing depth measurements. Furthermore, we correct misalignment errors of the camera, improving the overall reconstruction quality. In several experiments, we showcase our method and compare to existing works on classical RGB-D fusion and learned representations.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Neural RGB-D Surface Reconstruction<br>pdf: <a href="https://t.co/LDn6dOSZWB">https://t.co/LDn6dOSZWB</a><br>abs: <a href="https://t.co/TR6x4yc2w5">https://t.co/TR6x4yc2w5</a><br>project page: <a href="https://t.co/BEES8OVKFE">https://t.co/BEES8OVKFE</a> <a href="https://t.co/W7g12MvFVY">pic.twitter.com/W7g12MvFVY</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1381782726404100096?ref_src=twsrc%5Etfw">April 13, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. Action-Conditioned 3D Human Motion Synthesis with Transformer VAE

Mathis Petrovich, Michael J. Black, G√ºl Varol

- retweets: 210, favorites: 105 (04/14/2021 11:53:43)

- links: [abs](https://arxiv.org/abs/2104.05670) | [pdf](https://arxiv.org/pdf/2104.05670)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

We tackle the problem of action-conditioned generation of realistic and diverse human motion sequences. In contrast to methods that complete, or extend, motion sequences, this task does not require an initial pose or sequence. Here we learn an action-aware latent representation for human motions by training a generative variational autoencoder (VAE). By sampling from this latent space and querying a certain duration through a series of positional encodings, we synthesize variable-length motion sequences conditioned on a categorical action. Specifically, we design a Transformer-based architecture, ACTOR, for encoding and decoding a sequence of parametric SMPL human body models estimated from action recognition datasets. We evaluate our approach on the NTU RGB+D, HumanAct12 and UESTC datasets and show improvements over the state of the art. Furthermore, we present two use cases: improving action recognition through adding our synthesized data to training, and motion denoising. Our code and models will be made available.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Action-Conditioned 3D Human Motion Synthesis with Transformer VAE<br>pdf: <a href="https://t.co/4bG5PHP1wx">https://t.co/4bG5PHP1wx</a><br>abs: <a href="https://t.co/2AQdYNrw4s">https://t.co/2AQdYNrw4s</a><br>project page: <a href="https://t.co/COHaXkGf4l">https://t.co/COHaXkGf4l</a> <a href="https://t.co/Ro2cLvhGhn">pic.twitter.com/Ro2cLvhGhn</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1381779003380482052?ref_src=twsrc%5Etfw">April 13, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 10. Machine Translation Decoding beyond Beam Search

R√©mi Leblond, Jean-Baptiste Alayrac, Laurent Sifre, Miruna Pislar, Jean-Baptiste Lespiau, Ioannis Antonoglou, Karen Simonyan, Oriol Vinyals

- retweets: 169, favorites: 80 (04/14/2021 11:53:44)

- links: [abs](https://arxiv.org/abs/2104.05336) | [pdf](https://arxiv.org/pdf/2104.05336)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Beam search is the go-to method for decoding auto-regressive machine translation models. While it yields consistent improvements in terms of BLEU, it is only concerned with finding outputs with high model likelihood, and is thus agnostic to whatever end metric or score practitioners care about. Our aim is to establish whether beam search can be replaced by a more powerful metric-driven search technique. To this end, we explore numerous decoding algorithms, including some which rely on a value function parameterised by a neural network, and report results on a variety of metrics. Notably, we introduce a Monte-Carlo Tree Search (MCTS) based method and showcase its competitiveness. We provide a blueprint for how to use MCTS fruitfully in language applications, which opens promising future directions. We find that which algorithm is best heavily depends on the characteristics of the goal metric; we believe that our extensive experiments and analysis will inform further research in this area.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Machine Translation Decoding beyond Beam Search<br><br>Proposes a MCTS-based decoding as an alternative to Beam Search. It performs competitively. <a href="https://t.co/QyCyQ398Q2">https://t.co/QyCyQ398Q2</a> <a href="https://t.co/w8NcL42tDk">pic.twitter.com/w8NcL42tDk</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1381778654850547715?ref_src=twsrc%5Etfw">April 13, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 11. Pixel Codec Avatars

Shugao Ma, Tomas Simon, Jason Saragih, Dawei Wang, Yuecheng Li, Fernando De La Torre, Yaser Sheikh

- retweets: 121, favorites: 58 (04/14/2021 11:53:44)

- links: [abs](https://arxiv.org/abs/2104.04638) | [pdf](https://arxiv.org/pdf/2104.04638)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Telecommunication with photorealistic avatars in virtual or augmented reality is a promising path for achieving authentic face-to-face communication in 3D over remote physical distances. In this work, we present the Pixel Codec Avatars (PiCA): a deep generative model of 3D human faces that achieves state of the art reconstruction performance while being computationally efficient and adaptive to the rendering conditions during execution. Our model combines two core ideas: (1) a fully convolutional architecture for decoding spatially varying features, and (2) a rendering-adaptive per-pixel decoder. Both techniques are integrated via a dense surface representation that is learned in a weakly-supervised manner from low-topology mesh tracking over training images. We demonstrate that PiCA improves reconstruction over existing techniques across testing expressions and views on persons of different gender and skin tone. Importantly, we show that the PiCA model is much smaller than the state-of-art baseline model, and makes multi-person telecommunicaiton possible: on a single Oculus Quest 2 mobile VR headset, 5 avatars are rendered in realtime in the same scene.

<blockquote class="twitter-tweet"><p lang="fr" dir="ltr">Pixel Codec Avatars<br>pdf: <a href="https://t.co/esLrBbT8dr">https://t.co/esLrBbT8dr</a><br>abs: <a href="https://t.co/Q5e1F0jei9">https://t.co/Q5e1F0jei9</a> <a href="https://t.co/7fpqgXNXfH">pic.twitter.com/7fpqgXNXfH</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1381810756581216259?ref_src=twsrc%5Etfw">April 13, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 12. High-performance, Distributed Training of Large-scale Deep Learning  Recommendation Models

Dheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Andrew Tulloch, Srinivas Sridharan, Xing Liu, Mustafa Ozdal, Jade Nie, Jongsoo Park, Liang Luo, Jie Amy Yang, Leon Gao, Dmytro Ivchenko, Aarti Basant, Yuxi Hu, Jiyan Yang, Ehsan K. Ardestani, Xiaodong Wang, Rakesh Komuravelli, Ching-Hsiang Chu, Serhat Yilmaz, Huayu Li, Jiyuan Qian, Zhuobo Feng, Yinbin Ma, Junjie Yang, Ellie Wen, Hong Li, Lin Yang, Chonglin Sun, Whitney Zhao, Krishna Dhulipala, KR Kishore, Tyler Graf, Assaf Eisenman, Kiran Kumar Matam, Adi Gangidi, Pallab Bhattacharya, Guoqiang Jerry Chen, Manoj Krishnan, Krishnakumar Nair, Petr Lapukhov, Maxim Naumov, Lin Qiao, Mikhail Smelyanskiy, Bill Jia, Vijay Rao

- retweets: 121, favorites: 38 (04/14/2021 11:53:44)

- links: [abs](https://arxiv.org/abs/2104.05158) | [pdf](https://arxiv.org/pdf/2104.05158)
- [cs.DC](https://arxiv.org/list/cs.DC/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.PF](https://arxiv.org/list/cs.PF/recent)

Deep learning recommendation models (DLRMs) are used across many business-critical services at Facebook and are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper we discuss the SW/HW co-designed solution for high-performance distributed training of large-scale DLRMs. We introduce a high-performance scalable software stack based on PyTorch and pair it with the new evolution of Zion platform, namely ZionEX. We demonstrate the capability to train very large DLRMs with up to 12 Trillion parameters and show that we can attain 40X speedup in terms of time to solution over previous systems. We achieve this by (i) designing the ZionEX platform with dedicated scale-out network, provisioned with high bandwidth, optimal topology and efficient transport (ii) implementing an optimized PyTorch-based training stack supporting both model and data parallelism (iii) developing sharding algorithms capable of hierarchical partitioning of the embedding tables along row, column dimensions and load balancing them across multiple workers; (iv) adding high-performance core operators while retaining flexibility to support optimizers with fully deterministic updates (v) leveraging reduced precision communications, multi-level memory hierarchy (HBM+DDR+SSD) and pipelining. Furthermore, we develop and briefly comment on distributed data ingestion and other supporting services that are required for the robust and efficient end-to-end training in production environments.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">High-performance, Distributed Training of Large-scale Deep Learning Recommendation Models<br>pdf: <a href="https://t.co/f0miQlgq2s">https://t.co/f0miQlgq2s</a><br>abs: <a href="https://t.co/R3qCT62l17">https://t.co/R3qCT62l17</a><br><br>&quot;We demonstrate the capability to train very large DLRMs with up to 12 Trillion parameters and show that we can attain 40√ó speedup&quot; <a href="https://t.co/PAnLvQ1Fxn">pic.twitter.com/PAnLvQ1Fxn</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1381789039234285568?ref_src=twsrc%5Etfw">April 13, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 13. Rethinking and Improving the Robustness of Image Style Transfer

Pei Wang, Yijun Li, Nuno Vasconcelos

- retweets: 50, favorites: 85 (04/14/2021 11:53:44)

- links: [abs](https://arxiv.org/abs/2104.05623) | [pdf](https://arxiv.org/pdf/2104.05623)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [eess.IV](https://arxiv.org/list/eess.IV/recent)

Extensive research in neural style transfer methods has shown that the correlation between features extracted by a pre-trained VGG network has a remarkable ability to capture the visual style of an image. Surprisingly, however, this stylization quality is not robust and often degrades significantly when applied to features from more advanced and lightweight networks, such as those in the ResNet family. By performing extensive experiments with different network architectures, we find that residual connections, which represent the main architectural difference between VGG and ResNet, produce feature maps of small entropy, which are not suitable for style transfer. To improve the robustness of the ResNet architecture, we then propose a simple yet effective solution based on a softmax transformation of the feature activations that enhances their entropy. Experimental results demonstrate that this small magic can greatly improve the quality of stylization results, even for networks with random weights. This suggests that the architecture used for feature extraction is more important than the use of learned weights for the task of style transfer.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">Rethinking and Improving the Robustness of Image Style Transfer (CVPR2021)<a href="https://t.co/wlaJPurZ7O">https://t.co/wlaJPurZ7O</a><br>Style-transfer„ÅåVGGÁ≥ª„Å†„Å®(„É©„É≥„ÉÄ„É†ÂàùÊúüÂåñ„Åß„ÇÇ„Åù„Åì„Åù„Åì)„ÅÜ„Åæ„Åè„ÅÑ„Åè„ÅÆ„Å´ResNetÁ≥ª„Å†„Å®„ÅÜ„Åæ„Åè„ÅÑ„Åã„Å™„ÅÑ„Å®„ÅÑ„ÅÜÔºå„ÅÇ„Çã„ÅÇ„ÇãË©±„ÇíÁúüÈù¢ÁõÆ„Å´Ëß£Êûê„Åó„Å¶ÂéüÂõ†„ÇíÁô∫Ë¶ã„Åô„Çã„ÇÑ„Å§ÔºåÈù¢ÁôΩ„ÅÑ</p>&mdash; Naoto Inoue (@naoto_inoue_) <a href="https://twitter.com/naoto_inoue_/status/1381993799488995333?ref_src=twsrc%5Etfw">April 13, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Rethinking and Improving the Robustness of Image Style Transfer<br>pdf: <a href="https://t.co/JvZCMAfWSe">https://t.co/JvZCMAfWSe</a><br>abs: <a href="https://t.co/TQztSVesnw">https://t.co/TQztSVesnw</a> <a href="https://t.co/LiQ8nHjkz0">pic.twitter.com/LiQ8nHjkz0</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1381813268826705922?ref_src=twsrc%5Etfw">April 13, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 14. Meta-tuning Language Models to Answer Prompts Better

Ruiqi Zhong, Kristy Lee, Zheng Zhang, Dan Klein

- retweets: 76, favorites: 38 (04/14/2021 11:53:44)

- links: [abs](https://arxiv.org/abs/2104.04670) | [pdf](https://arxiv.org/pdf/2104.04670)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

Large pretrained language models like GPT-3 have acquired a surprising ability to perform zero-shot classification (ZSC). For example, to classify review sentiments, we can "prompt" the language model with the review and the question "Is the review positive?" as the context, and ask it to predict whether the next word is "Yes" or "No". However, these models are not specialized for answering these prompts. To address this weakness, we propose meta-tuning, which trains the model to specialize in answering prompts but still generalize to unseen tasks. To create the training data, we aggregated 43 existing datasets, annotated 441 label descriptions in total, and unified them into the above question answering (QA) format. After meta-tuning, our model outperforms a same-sized QA model for most labels on unseen tasks, and we forecast that the performance would improve for even larger models. Therefore, measuring ZSC performance on non-specialized language models might underestimate their true capability, and community-wide efforts on aggregating datasets and unifying their formats can help build models that understand prompts better.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Meta-tuning Language Models to Answer Prompts Better<br>pdf: <a href="https://t.co/zyugMr2Vkr">https://t.co/zyugMr2Vkr</a><br>abs: <a href="https://t.co/lh0pM4PmoB">https://t.co/lh0pM4PmoB</a><br>After metatuning, our model outperforms a same-sized<br>QA model for most labels on unseen tasks, and<br>we forecast that the performance would improve for even larger models <a href="https://t.co/Gy26aUmvDj">pic.twitter.com/Gy26aUmvDj</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1381773462335848448?ref_src=twsrc%5Etfw">April 13, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 15. Adversarial Open Domain Adaption for Sketch-to-Photo Synthesis

Xiaoyu Xiang, Ding Liu, Xiao Yang, Yiheng Zhu, Xiaohui Shen, Jan P. Allebach

- retweets: 72, favorites: 42 (04/14/2021 11:53:44)

- links: [abs](https://arxiv.org/abs/2104.05703) | [pdf](https://arxiv.org/pdf/2104.05703)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

In this paper, we explore the open-domain sketch-to-photo translation, which aims to synthesize a realistic photo from a freehand sketch with its class label, even if the sketches of that class are missing in the training data. It is challenging due to the lack of training supervision and the large geometry distortion between the freehand sketch and photo domains. To synthesize the absent freehand sketches from photos, we propose a framework that jointly learns sketch-to-photo and photo-to-sketch generation. However, the generator trained from fake sketches might lead to unsatisfying results when dealing with sketches of missing classes, due to the domain gap between synthesized sketches and real ones. To alleviate this issue, we further propose a simple yet effective open-domain sampling and optimization strategy to "fool" the generator into treating fake sketches as real ones. Our method takes advantage of the learned sketch-to-photo and photo-to-sketch mapping of in-domain data and generalizes them to the open-domain classes. We validate our method on the Scribble and SketchyCOCO datasets. Compared with the recent competing methods, our approach shows impressive results in synthesizing realistic color, texture, and maintaining the geometric composition for various categories of open-domain sketches.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Adversarial Open Domain Adaption for Sketch-to-Photo Synthesis<br>pdf: <a href="https://t.co/1i35S8a9wF">https://t.co/1i35S8a9wF</a><br>abs: <a href="https://t.co/mY5C7GhSba">https://t.co/mY5C7GhSba</a> <a href="https://t.co/dtLkdASbN9">pic.twitter.com/dtLkdASbN9</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1381825462914125824?ref_src=twsrc%5Etfw">April 13, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 16. Hausdorff approximations and volume of tubes of singular algebraic sets

Saugata Basu, Antonio Lerario

- retweets: 54, favorites: 34 (04/14/2021 11:53:44)

- links: [abs](https://arxiv.org/abs/2104.05053) | [pdf](https://arxiv.org/pdf/2104.05053)
- [math.AG](https://arxiv.org/list/math.AG/recent) | [math.NA](https://arxiv.org/list/math.NA/recent) | [math.OC](https://arxiv.org/list/math.OC/recent) | [math.PR](https://arxiv.org/list/math.PR/recent)

We prove bounds for the volume of neighborhoods of algebraic sets, in the euclidean space or the sphere, in terms of the degree of the defining polynomials, the number of variables and the dimension of the algebraic set, without any smoothness assumption. This generalizes previous work of Lotz on smooth complete intersections in the euclidean space and of B\"urgisser, Cucker and Lotz on hypersurfaces in the sphere, and gives a complete solution to Problem 17 in the book titled "Condition" by B\"urgisser and Cucker.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I like it when the non-standard world  (non-archimedean extensions, infinitesimals) meets the standard (integrals, Gauss maps,  condition numbers).  New preprint on &quot;volume of tubes&quot; with Antonio Lerario. Comments most welcome.<a href="https://t.co/obkiAIQBdS">https://t.co/obkiAIQBdS</a> <a href="https://t.co/I8yqDkdh3t">pic.twitter.com/I8yqDkdh3t</a></p>&mdash; Saugata Basu (@SaugataBasu4) <a href="https://twitter.com/SaugataBasu4/status/1381774372331999233?ref_src=twsrc%5Etfw">April 13, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 17. StereoPIFu: Depth Aware Clothed Human Digitization via Stereo Vision

Yang Hong, Juyong Zhang, Boyi Jiang, Yudong Guo, Ligang Liu, Hujun Bao

- retweets: 42, favorites: 21 (04/14/2021 11:53:45)

- links: [abs](https://arxiv.org/abs/2104.05289) | [pdf](https://arxiv.org/pdf/2104.05289)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

In this paper, we propose StereoPIFu, which integrates the geometric constraints of stereo vision with implicit function representation of PIFu, to recover the 3D shape of the clothed human from a pair of low-cost rectified images. First, we introduce the effective voxel-aligned features from a stereo vision-based network to enable depth-aware reconstruction. Moreover, the novel relative z-offset is employed to associate predicted high-fidelity human depth and occupancy inference, which helps restore fine-level surface details. Second, a network structure that fully utilizes the geometry information from the stereo images is designed to improve the human body reconstruction quality. Consequently, our StereoPIFu can naturally infer the human body's spatial location in camera space and maintain the correct relative position of different parts of the human body, which enables our method to capture human performance. Compared with previous works, our StereoPIFu significantly improves the robustness, completeness, and accuracy of the clothed human reconstruction, which is demonstrated by extensive experimental results.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">StereoPIFu: Depth Aware Clothed Human Digitization via Stereo Vision<br>pdf: <a href="https://t.co/0oR10kCnOt">https://t.co/0oR10kCnOt</a><br>abs: <a href="https://t.co/i4n1pQtHaP">https://t.co/i4n1pQtHaP</a> <a href="https://t.co/zepRzs981j">pic.twitter.com/zepRzs981j</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1382029285087711236?ref_src=twsrc%5Etfw">April 13, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 18. Drafting and Revision: Laplacian Pyramid Network for Fast High-Quality  Artistic Style Transfer

Tianwei Lin, Zhuoqi Ma, Fu Li, Dongliang He, Xin Li, Errui Ding, Nannan Wang, Jie Li, Xinbo Gao

- retweets: 25, favorites: 32 (04/14/2021 11:53:45)

- links: [abs](https://arxiv.org/abs/2104.05376) | [pdf](https://arxiv.org/pdf/2104.05376)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [eess.IV](https://arxiv.org/list/eess.IV/recent)

Artistic style transfer aims at migrating the style from an example image to a content image. Currently, optimization-based methods have achieved great stylization quality, but expensive time cost restricts their practical applications. Meanwhile, feed-forward methods still fail to synthesize complex style, especially when holistic global and local patterns exist. Inspired by the common painting process of drawing a draft and revising the details, we introduce a novel feed-forward method named Laplacian Pyramid Network (LapStyle). LapStyle first transfers global style patterns in low-resolution via a Drafting Network. It then revises the local details in high-resolution via a Revision Network, which hallucinates a residual image according to the draft and the image textures extracted by Laplacian filtering. Higher resolution details can be easily generated by stacking Revision Networks with multiple Laplacian pyramid levels. The final stylized image is obtained by aggregating outputs of all pyramid levels. %We also introduce a patch discriminator to better learn local patterns adversarially. Experiments demonstrate that our method can synthesize high quality stylized images in real time, where holistic style patterns are properly transferred.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Drafting and Revision: Laplacian Pyramid Network for Fast High-Quality Artistic Style Transfer<br>pdf: <a href="https://t.co/MPiiRYbCaf">https://t.co/MPiiRYbCaf</a><br>abs: <a href="https://t.co/XoJn3Nnoyh">https://t.co/XoJn3Nnoyh</a> <a href="https://t.co/BN60raRklp">pic.twitter.com/BN60raRklp</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1381808558346547203?ref_src=twsrc%5Etfw">April 13, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 19. Joint Universal Syntactic and Semantic Parsing

Elias Stengel-Eskin, Kenton Murray, Sheng Zhang, Aaron Steven White, Benjamin Van Durme

- retweets: 30, favorites: 27 (04/14/2021 11:53:45)

- links: [abs](https://arxiv.org/abs/2104.05696) | [pdf](https://arxiv.org/pdf/2104.05696)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

While numerous attempts have been made to jointly parse syntax and semantics, high performance in one domain typically comes at the price of performance in the other. This trade-off contradicts the large body of research focusing on the rich interactions at the syntax-semantics interface. We explore multiple model architectures which allow us to exploit the rich syntactic and semantic annotations contained in the Universal Decompositional Semantics (UDS) dataset, jointly parsing Universal Dependencies and UDS to obtain state-of-the-art results in both formalisms. We analyze the behaviour of a joint model of syntax and semantics, finding patterns supported by linguistic theory at the syntax-semantics interface. We then investigate to what degree joint modeling generalizes to a multilingual setting, where we find similar trends across 8 languages.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Actual link to the paper: <a href="https://t.co/J2S9idikZf">https://t.co/J2S9idikZf</a> <br><br>I‚Äôm really excited to finally share this work where our joint model improves both syntactic and semantic parsing! Ever since I saw the preliminary results last year, I‚Äôve been eagerly awaiting being able to share them. <a href="https://t.co/FV2jWLRPOW">https://t.co/FV2jWLRPOW</a></p>&mdash; Kenton Murray (@kentonmurray) <a href="https://twitter.com/kentonmurray/status/1381959132194934785?ref_src=twsrc%5Etfw">April 13, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 20. Fool Me Twice: Entailment from Wikipedia Gamification

Julian Martin Eisenschlos, Bhuwan Dhingra, Jannis Bulian, Benjamin B√∂rschinger, Jordan Boyd-Graber

- retweets: 42, favorites: 14 (04/14/2021 11:53:45)

- links: [abs](https://arxiv.org/abs/2104.04725) | [pdf](https://arxiv.org/pdf/2104.04725)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

We release FoolMeTwice (FM2 for short), a large dataset of challenging entailment pairs collected through a fun multi-player game. Gamification encourages adversarial examples, drastically lowering the number of examples that can be solved using "shortcuts" compared to other popular entailment datasets. Players are presented with two tasks. The first task asks the player to write a plausible claim based on the evidence from a Wikipedia page. The second one shows two plausible claims written by other players, one of which is false, and the goal is to identify it before the time runs out. Players "pay" to see clues retrieved from the evidence pool: the more evidence the player needs, the harder the claim. Game-play between motivated players leads to diverse strategies for crafting claims, such as temporal inference and diverting to unrelated evidence, and results in higher quality data for the entailment and evidence retrieval tasks. We open source the dataset and the game code.




# 21. Robust Egocentric Photo-realistic Facial Expression Transfer for Virtual  Reality

Amin Jourabloo, Fernando De la Torre, Jason Saragih, Shih-En Wei, Te-Li Wang, Stephen Lombardi, Danielle Belko, Autumn Trimble, Hernan Badino

- retweets: 30, favorites: 21 (04/14/2021 11:53:45)

- links: [abs](https://arxiv.org/abs/2104.04794) | [pdf](https://arxiv.org/pdf/2104.04794)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Social presence, the feeling of being there with a real person, will fuel the next generation of communication systems driven by digital humans in virtual reality (VR). The best 3D video-realistic VR avatars that minimize the uncanny effect rely on person-specific (PS) models. However, these PS models are time-consuming to build and are typically trained with limited data variability, which results in poor generalization and robustness. Major sources of variability that affects the accuracy of facial expression transfer algorithms include using different VR headsets (e.g., camera configuration, slop of the headset), facial appearance changes over time (e.g., beard, make-up), and environmental factors (e.g., lighting, backgrounds). This is a major drawback for the scalability of these models in VR. This paper makes progress in overcoming these limitations by proposing an end-to-end multi-identity architecture (MIA) trained with specialized augmentation strategies. MIA drives the shape component of the avatar from three cameras in the VR headset (two eyes, one mouth), in untrained subjects, using minimal personalized information (i.e., neutral 3D mesh shape). Similarly, if the PS texture decoder is available, MIA is able to drive the full avatar (shape+texture) robustly outperforming PS models in challenging scenarios. Our key contribution to improve robustness and generalization, is that our method implicitly decouples, in an unsupervised manner, the facial expression from nuisance factors (e.g., headset, environment, facial appearance). We demonstrate the superior performance and robustness of the proposed method versus state-of-the-art PS approaches in a variety of experiments.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Robust Egocentric Photo-realistic Facial Expression Transfer for Virtual Reality<br>pdf: <a href="https://t.co/FGBR45Ynwb">https://t.co/FGBR45Ynwb</a><br>abs: <a href="https://t.co/mpDwM5T5ak">https://t.co/mpDwM5T5ak</a> <a href="https://t.co/yCD6L2bOyI">pic.twitter.com/yCD6L2bOyI</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1381795700044931072?ref_src=twsrc%5Etfw">April 13, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



