---
title: Hot Papers 2021-06-22
date: 2021-06-23T10:32:04.Z
template: "post"
draft: false
slug: "hot-papers-2021-06-22"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2021-06-22"
socialImage: "/media/flying-marine.jpg"

---

# 1. GRAND: Graph Neural Diffusion

Benjamin Paul Chamberlain, James Rowbottom, Maria Gorinova, Stefan Webb, Emanuele Rossi, Michael M. Bronstein

- retweets: 7779, favorites: 89 (06/23/2021 10:32:04)

- links: [abs](https://arxiv.org/abs/2106.10934) | [pdf](https://arxiv.org/pdf/2106.10934)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

We present Graph Neural Diffusion (GRAND) that approaches deep learning on graphs as a continuous diffusion process and treats Graph Neural Networks (GNNs) as discretisations of an underlying PDE. In our model, the layer structure and topology correspond to the discretisation choices of temporal and spatial operators. Our approach allows a principled development of a broad new class of GNNs that are able to address the common plights of graph learning models such as depth, oversmoothing, and bottlenecks. Key to the success of our models are stability with respect to perturbations in the data and this is addressed for both implicit and explicit discretisation schemes. We develop linear and nonlinear versions of GRAND, which achieve competitive results on many standard graph benchmarks.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">New paper from Twitter GraphML at <a href="https://twitter.com/hashtag/ICML2021?src=hash&amp;ref_src=twsrc%5Etfw">#ICML2021</a> now on arxiv. <br>We develop links between Partial Differential Equations &amp; GNNs -&gt; new GNNs + new theory<br>Blog post: <a href="https://t.co/dNaibcliBR">https://t.co/dNaibcliBR</a><br>Paper: <a href="https://t.co/wSyHNemQ9x">https://t.co/wSyHNemQ9x</a><br>Code: <a href="https://t.co/xQ7wj92aAA">https://t.co/xQ7wj92aAA</a><a href="https://twitter.com/hashtag/MachineLearning?src=hash&amp;ref_src=twsrc%5Etfw">#MachineLearning</a> <a href="https://t.co/cwYusZXaXs">https://t.co/cwYusZXaXs</a></p>&mdash; Ben Chamberlain (@b_p_chamberlain) <a href="https://twitter.com/b_p_chamberlain/status/1407259486352445443?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/GNNs?src=hash&amp;ref_src=twsrc%5Etfw">#GNNs</a> are related to PDEs governing information diffusion on graphs. In a new paper with <a href="https://twitter.com/b_p_chamberlain?ref_src=twsrc%5Etfw">@b_p_chamberlain</a> James Rowbottom <a href="https://twitter.com/migorinova?ref_src=twsrc%5Etfw">@migorinova</a> <a href="https://twitter.com/stefan_webb?ref_src=twsrc%5Etfw">@stefan_webb</a> <a href="https://twitter.com/emaros96?ref_src=twsrc%5Etfw">@emaros96</a>  we study a new class of Neural Graph Diffusion PDEs<br><br>Blog post: <a href="https://t.co/sxVcS1pWmK">https://t.co/sxVcS1pWmK</a><br><br>Paper: <a href="https://t.co/upMNI0EyW8">https://t.co/upMNI0EyW8</a> <a href="https://t.co/SYNWeRjP4z">pic.twitter.com/SYNWeRjP4z</a></p>&mdash; Michael Bronstein (@mmbronstein) <a href="https://twitter.com/mmbronstein/status/1407247333499285510?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Regularization is all you Need: Simple Neural Nets can Excel on Tabular  Data

Arlind Kadra, Marius Lindauer, Frank Hutter, Josif Grabocka

- retweets: 6604, favorites: 68 (06/23/2021 10:32:04)

- links: [abs](https://arxiv.org/abs/2106.11189) | [pdf](https://arxiv.org/pdf/2106.11189)
- [cs.LG](https://arxiv.org/list/cs.LG/recent)

Tabular datasets are the last "unconquered castle" for deep learning, with traditional ML methods like Gradient-Boosted Decision Trees still performing strongly even against recent specialized neural architectures. In this paper, we hypothesize that the key to boosting the performance of neural networks lies in rethinking the joint and simultaneous application of a large set of modern regularization techniques. As a result, we propose regularizing plain Multilayer Perceptron (MLP) networks by searching for the optimal combination/cocktail of 13 regularization techniques for each dataset using a joint optimization over the decision on which regularizers to apply and their subsidiary hyperparameters. We empirically assess the impact of these regularization cocktails for MLPs on a large-scale empirical study comprising 40 tabular datasets and demonstrate that (i) well-regularized plain MLPs significantly outperform recent state-of-the-art specialized neural network architectures, and (ii) they even outperform strong traditional ML methods, such as XGBoost.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Regularization is all you Need: Simple Neural Nets can Excel on Tabular Data<br>pdf: <a href="https://t.co/rcRbf6H3tf">https://t.co/rcRbf6H3tf</a><br>abs: <a href="https://t.co/voMqog8zct">https://t.co/voMqog8zct</a><br>well-regularized plain MLPs significantly outperform sota specialized neural network architectures <a href="https://t.co/L3jcM0hbgc">pic.twitter.com/L3jcM0hbgc</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1407142424800075779?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Lossy Compression for Lossless Prediction

Yann Dubois, Benjamin Bloem-Reddy, Karen Ullrich, Chris J. Maddison

- retweets: 4098, favorites: 339 (06/23/2021 10:32:05)

- links: [abs](https://arxiv.org/abs/2106.10800) | [pdf](https://arxiv.org/pdf/2106.10800)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.IT](https://arxiv.org/list/cs.IT/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Most data is automatically collected and only ever "seen" by algorithms. Yet, data compressors preserve perceptual fidelity rather than just the information needed by algorithms performing downstream tasks. In this paper, we characterize the bit-rate required to ensure high performance on all predictive tasks that are invariant under a set of transformations, such as data augmentations. Based on our theory, we design unsupervised objectives for training neural compressors. Using these objectives, we train a generic image compressor that achieves substantial rate savings (more than $1000\times$ on ImageNet) compared to JPEG on 8 datasets, without decreasing downstream classification performance.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Most data is processed by algorithms, but compressors (eg JPEG) are for human eyes.<br><br>ü§ìOur fix: formalize lossy compression that ensures perfect downstream predictions<br><br>üî•1000x gains vs JPEG on ImageNetüî•<a href="https://t.co/8r3OJyCILj">https://t.co/8r3OJyCILj</a><br>w. Ben Bloem-Reddy <a href="https://twitter.com/karen_ullrich?ref_src=twsrc%5Etfw">@karen_ullrich</a> <a href="https://twitter.com/cjmaddison?ref_src=twsrc%5Etfw">@cjmaddison</a><br>1/9 <a href="https://t.co/dJoQA3zdOW">pic.twitter.com/dJoQA3zdOW</a></p>&mdash; Yann Dubois (@yanndubs) <a href="https://twitter.com/yanndubs/status/1407370279450447878?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Improving security for users of decentralized exchanges through  multiparty computation

Robert Annessi, Ethan Fast

- retweets: 2466, favorites: 299 (06/23/2021 10:32:05)

- links: [abs](https://arxiv.org/abs/2106.10972) | [pdf](https://arxiv.org/pdf/2106.10972)
- [cs.CR](https://arxiv.org/list/cs.CR/recent)

Decentralized cryptocurrency exchanges offer compelling security benefits over centralized exchanges: users control their funds and avoid the risk of an exchange hack or malicious operator. However, because user assets are fully accessible by a secret key, decentralized exchanges pose significant internal security risks for trading firms and automated trading systems, where a compromised system can result in total loss of funds. Centralized exchanges mitigate this risk through API key based security policies that allow professional users to give individual traders or automated systems specific and customizable access rights such as trading or withdrawal limits. Such policies, however, are not compatible with decentralized exchanges, where all exchange operations require a signature generated by the owner's secret key. This paper introduces a protocol based upon multiparty computation that allows for the creation of API keys and security policies that can be applied to any existing decentralized exchange. Our protocol works with both ECDSA and EdDSA signature schemes and prioritizes efficient computation and communication. We have deployed this protocol on Nash exchange, as well as around several Ethereum-based automated market maker smart contracts, where it secures the trading accounts and wallets of thousands of users.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Multi-party computation (MPC) offers major improvements to non-custodial wallet security and lets decentralized exchanges offer traders API keys with security policies. Read our research team‚Äôs technical paper on Nash‚Äôs MPC implementation here: <a href="https://t.co/RylGraR37e">https://t.co/RylGraR37e</a></p>&mdash; Nash (@nashsocial) <a href="https://twitter.com/nashsocial/status/1407257979431305218?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Excited to have a paper out describing the MPC protocol we developed at Nash. The <a href="https://twitter.com/nashsocial?ref_src=twsrc%5Etfw">@nashsocial</a> mobile app uses this protocol to secure all user blockchain interactions, including with integrated DeFi applications like <a href="https://twitter.com/Uniswap?ref_src=twsrc%5Etfw">@Uniswap</a> and <a href="https://twitter.com/1inchNetwork?ref_src=twsrc%5Etfw">@1inchNetwork</a> <a href="https://t.co/udC5bNkifC">https://t.co/udC5bNkifC</a></p>&mdash; Ethan Fast (@unignorant) <a href="https://twitter.com/unignorant/status/1407368126291877888?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Calliar: An Online Handwritten Dataset for Arabic Calligraphy

Zaid Alyafeai, Maged S. Al-shaibani, Mustafa Ghaleb, Yousif Ahmed Al-Wajih

- retweets: 1295, favorites: 157 (06/23/2021 10:32:05)

- links: [abs](https://arxiv.org/abs/2106.10745) | [pdf](https://arxiv.org/pdf/2106.10745)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

Calligraphy is an essential part of the Arabic heritage and culture. It has been used in the past for the decoration of houses and mosques. Usually, such calligraphy is designed manually by experts with aesthetic insights. In the past few years, there has been a considerable effort to digitize such type of art by either taking a photo of decorated buildings or drawing them using digital devices. The latter is considered an online form where the drawing is tracked by recording the apparatus movement, an electronic pen for instance, on a screen. In the literature, there are many offline datasets collected with a diversity of Arabic styles for calligraphy. However, there is no available online dataset for Arabic calligraphy. In this paper, we illustrate our approach for the collection and annotation of an online dataset for Arabic calligraphy called Calliar that consists of 2,500 sentences. Calliar is annotated for stroke, character, word and sentence level prediction.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Pleased to announce Calliar,  the first online dataset for Arabic Calligraphy. Joint work with <a href="https://twitter.com/_MagedSaeed_?ref_src=twsrc%5Etfw">@_MagedSaeed_</a> <a href="https://twitter.com/alwaridi?ref_src=twsrc%5Etfw">@alwaridi</a> and Yousif Al-Wajih. <br>Paper: <a href="https://t.co/q5mwcP6roa">https://t.co/q5mwcP6roa</a><br>Code &amp; data: <a href="https://t.co/tDCPJRYUcl">https://t.co/tDCPJRYUcl</a><br>Colab:  <a href="https://t.co/46yD0V3wht">https://t.co/46yD0V3wht</a> <a href="https://t.co/qbbb4tZJ6l">pic.twitter.com/qbbb4tZJ6l</a></p>&mdash; Zaid ÿ≤ŸäÿØ (@zaidalyafeai) <a href="https://twitter.com/zaidalyafeai/status/1407383686786527234?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. VIMPAC: Video Pre-Training via Masked Token Prediction and Contrastive  Learning

Hao Tan, Jie Lei, Thomas Wolf, Mohit Bansal

- retweets: 1170, favorites: 202 (06/23/2021 10:32:05)

- links: [abs](https://arxiv.org/abs/2106.11250) | [pdf](https://arxiv.org/pdf/2106.11250)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Video understanding relies on perceiving the global content and modeling its internal connections (e.g., causality, movement, and spatio-temporal correspondence). To learn these interactions, we apply a mask-then-predict pre-training task on discretized video tokens generated via VQ-VAE. Unlike language, where the text tokens are more independent, neighboring video tokens typically have strong correlations (e.g., consecutive video frames usually look very similar), and hence uniformly masking individual tokens will make the task too trivial to learn useful representations. To deal with this issue, we propose a block-wise masking strategy where we mask neighboring video tokens in both spatial and temporal domains. We also add an augmentation-free contrastive learning method to further capture the global content by predicting whether the video clips are sampled from the same video. We pre-train our model on uncurated videos and show that our pre-trained model can reach state-of-the-art results on several video understanding datasets (e.g., SSV2, Diving48). Lastly, we provide detailed analyses on model scalability and pre-training method design. Code is released at https://github.com/airsplay/vimpac.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Excited to share ‚ÄúVIMPAC: Video Pre-Training via Masked Token Prediction and Contrastive Learning‚Äù, combining language modeling and contrastive learning for video pre-training. <a href="https://t.co/UxgVjs5R5d">https://t.co/UxgVjs5R5d</a><br><br>Work done w/ <a href="https://twitter.com/jayleicn?ref_src=twsrc%5Etfw">@jayleicn</a> <a href="https://twitter.com/Thom_Wolf?ref_src=twsrc%5Etfw">@thom_wolf</a> <a href="https://twitter.com/mohitban47?ref_src=twsrc%5Etfw">@mohitban47</a>  <br>(<a href="https://twitter.com/huggingface?ref_src=twsrc%5Etfw">@huggingface</a> + <a href="https://twitter.com/uncnlp?ref_src=twsrc%5Etfw">@uncnlp</a>)<br>1/4 <a href="https://t.co/DUpoKHVajg">pic.twitter.com/DUpoKHVajg</a></p>&mdash; Hao Tan (@HaoTan5) <a href="https://twitter.com/HaoTan5/status/1407346561961578504?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">VIMPAC: Video Pre-Training via Masked Token Prediction and Contrastive Learning<br>pdf: <a href="https://t.co/9RJH4ovBiP">https://t.co/9RJH4ovBiP</a><br>github: <a href="https://t.co/inPZwIuR4q">https://t.co/inPZwIuR4q</a><br><br>pre-train model on uncurated videos and pre-trained<br>model can reach sota results on several video understanding datasets <a href="https://t.co/TamganjV9t">pic.twitter.com/TamganjV9t</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1407146356251181059?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. Nested Variational Inference

Heiko Zimmermann, Hao Wu, Babak Esmaeili, Jan-Willem van de Meent

- retweets: 506, favorites: 71 (06/23/2021 10:32:06)

- links: [abs](https://arxiv.org/abs/2106.11302) | [pdf](https://arxiv.org/pdf/2106.11302)
- [stat.ML](https://arxiv.org/list/stat.ML/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

We develop nested variational inference (NVI), a family of methods that learn proposals for nested importance samplers by minimizing an forward or reverse KL divergence at each level of nesting. NVI is applicable to many commonly-used importance sampling strategies and provides a mechanism for learning intermediate densities, which can serve as heuristics to guide the sampler. Our experiments apply NVI to (a) sample from a multimodal distribution using a learned annealing path (b) learn heuristics that approximate the likelihood of future observations in a hidden Markov model and (c) to perform amortized inference in hierarchical deep generative models. We observe that optimizing nested objectives leads to improved sample quality in terms of log average weight and effective sample size.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">New on ArXiv: Nested Variational Inference<a href="https://t.co/wkAESSNfiu">https://t.co/wkAESSNfiu</a><br><br>Work by Heiko Zimmermann (<a href="https://twitter.com/zmheiko?ref_src=twsrc%5Etfw">@zmheiko</a>), Hao Wu (<a href="https://twitter.com/Hao_Wu_?ref_src=twsrc%5Etfw">@Hao_Wu_</a>), Babak Esmaeili (<a href="https://twitter.com/bob_smiley_?ref_src=twsrc%5Etfw">@bob_smiley_</a>), and myself.<br><br>(this is an extended version of our work at AABI this year; <a href="https://t.co/hzLJ2IpIWm">https://t.co/hzLJ2IpIWm</a>)   [1/] <a href="https://t.co/U4D5vvhQEd">pic.twitter.com/U4D5vvhQEd</a></p>&mdash; Jan-Willem van de Meent (@jwvdm) <a href="https://twitter.com/jwvdm/status/1407344458597871631?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. Boundary Graph Neural Networks for 3D Simulations

Andreas Mayr, Sebastian Lehner, Arno Mayrhofer, Christoph Kloss, Sepp Hochreiter, Johannes Brandstetter

- retweets: 462, favorites: 43 (06/23/2021 10:32:06)

- links: [abs](https://arxiv.org/abs/2106.11299) | [pdf](https://arxiv.org/pdf/2106.11299)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

The abundance of data has given machine learning huge momentum in natural sciences and engineering. However, the modeling of simulated physical processes remains difficult. A key problem in doing so is the correct handling of geometric boundaries. While triangularized geometric boundaries are very common in engineering applications, they are notoriously difficult to model by machine learning approaches due to their heterogeneity with respect to size and orientation. In this work, we introduce Boundary Graph Neural Networks (BGNNs), which dynamically modify graph structures to address boundary conditions. Boundary graph structures are constructed via modifying edges, augmenting node features, and dynamically inserting virtual nodes. The new BGNNs are tested on complex 3D granular flow processes of hoppers and rotating drums which are standard parts of industrial machinery. Using precise simulations that are obtained by an expensive and complex discrete element method, BGNNs are evaluated in terms of computational efficiency as well as prediction accuracy of particle flows and mixing entropies. Even if complex boundaries are present, BGNNs are able to accurately reproduce 3D granular flows within simulation uncertainties over hundreds of thousands of simulation timesteps, and most notably particles completely stay within the geometric objects without using handcrafted conditions or restrictions.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Great work lead by <a href="https://twitter.com/AndreasMayr11?ref_src=twsrc%5Etfw">@AndreasMayr11</a>. Granular flows learn to move inside complex geometric objects without any handcrafted restrictions. Paper: <a href="https://t.co/qKIBDKZVJZ">https://t.co/qKIBDKZVJZ</a> Blog post: <a href="https://t.co/eWL1A7Awuv">https://t.co/eWL1A7Awuv</a> <a href="https://t.co/okLWDpAiD1">pic.twitter.com/okLWDpAiD1</a></p>&mdash; Johannes Brandstetter (@jo_brandstetter) <a href="https://twitter.com/jo_brandstetter/status/1407226931263770625?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. Say Their Names: Resurgence in the collective attention toward Black  victims of fatal police violence following the death of George Floyd

Henry H. Wu, Ryan J. Gallagher, Thayer Alshaabi, Jane L. Adams, Joshua R. Minot, Michael V. Arnold, Brooke Foucault Welles, Randall Harp, Peter Sheridan Dodds, Christopher M. Danforth

- retweets: 430, favorites: 32 (06/23/2021 10:32:06)

- links: [abs](https://arxiv.org/abs/2106.10281) | [pdf](https://arxiv.org/pdf/2106.10281)
- [cs.SI](https://arxiv.org/list/cs.SI/recent) | [cs.CY](https://arxiv.org/list/cs.CY/recent) | [physics.soc-ph](https://arxiv.org/list/physics.soc-ph/recent)

The murder of George Floyd by police in May 2020 sparked international protests and renewed attention in the Black Lives Matter movement. Here, we characterize ways in which the online activity following George Floyd's death was unparalleled in its volume and intensity, including setting records for activity on Twitter, prompting the saddest day in the platform's history, and causing George Floyd's name to appear among the ten most frequently used phrases in a day, where he is the only individual to have ever received that level of attention who was not known to the public earlier that same week. Further, we find this attention extended beyond George Floyd and that more Black victims of fatal police violence received attention following his death than during other past moments in Black Lives Matter's history. We place that attention within the context of prior online racial justice activism by showing how the names of Black victims of police violence have been lifted and memorialized over the last 12 years on Twitter. Our results suggest that the 2020 wave of attention to the Black Lives Matter movement centered past instances of police violence in an unprecedented way, demonstrating the impact of the movement's rhetorical strategy to "say their names."

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">‚ÄúSay Their Names: Resurgence in the collective attention toward Black¬†victims of fatal police violence following the death of George Floyd‚Äù<br><br>New study from our group in collaboration w/<a href="https://twitter.com/reharp?ref_src=twsrc%5Etfw">@reharp</a> <a href="https://twitter.com/foucaultwelles?ref_src=twsrc%5Etfw">@foucaultwelles</a> @ryanjgallager.<a href="https://t.co/ZGJ5lsry4G">https://t.co/ZGJ5lsry4G</a><br><br>1/15 <a href="https://t.co/yQriHjbOCZ">pic.twitter.com/yQriHjbOCZ</a></p>&mdash; Computational Story Lab (@compstorylab) <a href="https://twitter.com/compstorylab/status/1407299020247158792?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 10. Pay Better Attention to Attention: Head Selection in Multilingual and  Multi-Domain Sequence Modeling

Hongyu Gong, Yun Tang, Juan Pino, Xian Li

- retweets: 196, favorites: 61 (06/23/2021 10:32:06)

- links: [abs](https://arxiv.org/abs/2106.10840) | [pdf](https://arxiv.org/pdf/2106.10840)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

Multi-head attention has each of the attention heads collect salient information from different parts of an input sequence, making it a powerful mechanism for sequence modeling. Multilingual and multi-domain learning are common scenarios for sequence modeling, where the key challenge is to maximize positive transfer and mitigate negative transfer across languages and domains. In this paper, we find that non-selective attention sharing is sub-optimal for achieving good generalization across all languages and domains. We further propose attention sharing strategies to facilitate parameter sharing and specialization in multilingual and multi-domain sequence modeling. Our approach automatically learns shared and specialized attention heads for different languages and domains to mitigate their interference. Evaluated in various tasks including speech recognition, text-to-text and speech-to-text translation, the proposed attention sharing strategies consistently bring gains to sequence models built upon multi-head attention. For speech-to-text translation, our approach yields an average of $+2.0$ BLEU over $13$ language directions in multilingual setting and $+2.0$ BLEU over $3$ domains in multi-domain setting.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Pay Better Attention to Attention: Head Selection in<br>Multilingual and Multi-Domain Sequence Modeling<br>pdf: <a href="https://t.co/TRKWh374g0">https://t.co/TRKWh374g0</a><br><br>For speech-to-text translation, yields an average of +2.0 BLEU over 13 language directions in multilingual setting and +2.0 BLEU over 3 domains <a href="https://t.co/Vg6FSpJ3ea">pic.twitter.com/Vg6FSpJ3ea</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1407203452799881219?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 11. Multiplying Matrices Without Multiplying

Davis Blalock, John Guttag

- retweets: 170, favorites: 60 (06/23/2021 10:32:06)

- links: [abs](https://arxiv.org/abs/2106.10860) | [pdf](https://arxiv.org/pdf/2106.10860)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AR](https://arxiv.org/list/cs.AR/recent) | [cs.PF](https://arxiv.org/list/cs.PF/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Multiplying matrices is among the most fundamental and compute-intensive operations in machine learning. Consequently, there has been significant work on efficiently approximating matrix multiplies. We introduce a learning-based algorithm for this task that greatly outperforms existing methods. Experiments using hundreds of matrices from diverse domains show that it often runs $100\times$ faster than exact matrix products and $10\times$ faster than current approximate methods. In the common case that one matrix is known ahead of time, our method also has the interesting property that it requires zero multiply-adds. These results suggest that a mixture of hashing, averaging, and byte shuffling$-$the core operations of our method$-$could be a more promising building block for machine learning than the sparsified, factorized, and/or scalar quantized matrix products that have recently been the focus of substantial research and hardware investment.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Excited to announce our ICML paper, ‚ÄúMultiplying Matrices Without Multiplying‚Äù !<br>TL;DR: 10x better sparsity + quantization, no multiply-adds<br>Paper: <a href="https://t.co/nsPiiwVbC0">https://t.co/nsPiiwVbC0</a><br>Code: <a href="https://t.co/xXEfRpAEZR">https://t.co/xXEfRpAEZR</a> [1/n] <a href="https://t.co/vlsLPJPSCB">pic.twitter.com/vlsLPJPSCB</a></p>&mdash; Davis Blalock (@davisblalock) <a href="https://twitter.com/davisblalock/status/1407384407808221187?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 12. CLIP2Video: Mastering Video-Text Retrieval via Image CLIP

Han Fang, Pengfei Xiong, Luhui Xu, Yu Chen

- retweets: 143, favorites: 45 (06/23/2021 10:32:06)

- links: [abs](https://arxiv.org/abs/2106.11097) | [pdf](https://arxiv.org/pdf/2106.11097)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

We present CLIP2Video network to transfer the image-language pre-training model to video-text retrieval in an end-to-end manner. Leading approaches in the domain of video-and-language learning try to distill the spatio-temporal video features and multi-modal interaction between videos and languages from a large-scale video-text dataset. Different from them, we leverage pretrained image-language model, simplify it as a two-stage framework with co-learning of image-text and enhancing temporal relations between video frames and video-text respectively, make it able to train on comparatively small datasets. Specifically, based on the spatial semantics captured by Contrastive Language-Image Pretraining (CLIP) model, our model involves a Temporal Difference Block to capture motions at fine temporal video frames, and a Temporal Alignment Block to re-align the tokens of video clips and phrases and enhance the multi-modal correlation. We conduct thorough ablation studies, and achieve state-of-the-art performance on major text-to-video and video-to-text retrieval benchmarks, including new records of retrieval accuracy on MSR-VTT, MSVD and VATEX.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">CLIP2Video: Mastering Video-Text Retrieval via Image CLIP<br>pdf: <a href="https://t.co/3EgkVlPLtE">https://t.co/3EgkVlPLtE</a><br>abs: <a href="https://t.co/l9oIq1jqie">https://t.co/l9oIq1jqie</a><br><br>sota performance on text-to-video and video-to-text retrieval benchmarks, including new records of retrieval accuracy on MSRVTT, MSVD and VATEX <a href="https://t.co/kVyUXAeG1J">pic.twitter.com/kVyUXAeG1J</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1407141768706023425?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 13. One Million Scenes for Autonomous Driving: ONCE Dataset

Jiageng Mao, Minzhe Niu, Chenhan Jiang, Hanxue Liang, Xiaodan Liang, Yamin Li, Chaoqiang Ye, Wei Zhang, Zhenguo Li, Jie Yu, Hang Xu, Chunjing Xu

- retweets: 144, favorites: 43 (06/23/2021 10:32:07)

- links: [abs](https://arxiv.org/abs/2106.11037) | [pdf](https://arxiv.org/pdf/2106.11037)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Current perception models in autonomous driving have become notorious for greatly relying on a mass of annotated data to cover unseen cases and address the long-tail problem. On the other hand, learning from unlabeled large-scale collected data and incrementally self-training powerful recognition models have received increasing attention and may become the solutions of next-generation industry-level powerful and robust perception models in autonomous driving. However, the research community generally suffered from data inadequacy of those essential real-world scene data, which hampers the future exploration of fully/semi/self-supervised methods for 3D perception. In this paper, we introduce the ONCE (One millioN sCenEs) dataset for 3D object detection in the autonomous driving scenario. The ONCE dataset consists of 1 million LiDAR scenes and 7 million corresponding camera images. The data is selected from 144 driving hours, which is 20x longer than the largest 3D autonomous driving dataset available (e.g. nuScenes and Waymo), and it is collected across a range of different areas, periods and weather conditions. To facilitate future research on exploiting unlabeled data for 3D detection, we additionally provide a benchmark in which we reproduce and evaluate a variety of self-supervised and semi-supervised methods on the ONCE dataset. We conduct extensive analyses on those methods and provide valuable observations on their performance related to the scale of used data. Data, code, and more information are available at https://once-for-auto-driving.github.io/index.html.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">One Million Scenes for Autonomous Driving: ONCE Dataset<br>pdf: <a href="https://t.co/4ITaZ5dRdS">https://t.co/4ITaZ5dRdS</a><br>abs: <a href="https://t.co/oV9Gl4l7u9">https://t.co/oV9Gl4l7u9</a><br>project page: <a href="https://t.co/tLcOYgMuBt">https://t.co/tLcOYgMuBt</a><br><br>consists of 1 million LiDAR scenes and 7 million corresponding camera images <a href="https://t.co/2WCmC8TAwF">pic.twitter.com/2WCmC8TAwF</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1407148582180954114?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 14. NeuS: Learning Neural Implicit Surfaces by Volume Rendering for  Multi-view Reconstruction

Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, Wenping Wang

- retweets: 90, favorites: 60 (06/23/2021 10:32:07)

- links: [abs](https://arxiv.org/abs/2106.10689) | [pdf](https://arxiv.org/pdf/2106.10689)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.GR](https://arxiv.org/list/cs.GR/recent)

We present a novel neural surface reconstruction method, called NeuS, for reconstructing objects and scenes with high fidelity from 2D image inputs. Existing neural surface reconstruction approaches, such as DVR and IDR, require foreground mask as supervision, easily get trapped in local minima, and therefore struggle with the reconstruction of objects with severe self-occlusion or thin structures. Meanwhile, recent neural methods for novel view synthesis, such as NeRF and its variants, use volume rendering to produce a neural scene representation with robustness of optimization, even for highly complex objects. However, extracting high-quality surfaces from this learned implicit representation is difficult because there are not sufficient surface constraints in the representation. In NeuS, we propose to represent a surface as the zero-level set of a signed distance function (SDF) and develop a new volume rendering method to train a neural SDF representation. We observe that the conventional volume rendering method causes inherent geometric errors (i.e. bias) for surface reconstruction, and therefore propose a new formulation that is free of bias in the first order of approximation, thus leading to more accurate surface reconstruction even without the mask supervision. Experiments on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the state-of-the-arts in high-quality surface reconstruction, especially for objects and scenes with complex structures and self-occlusion.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction<br>pdf: <a href="https://t.co/LPA78L4KFO">https://t.co/LPA78L4KFO</a><br><br>multiview surface reconstruction, represents 3D surfaces as neural SDF and developed a new volume rendering method for training the implicit SDF<br>representation <a href="https://t.co/k6hBlbV9mP">pic.twitter.com/k6hBlbV9mP</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1407150495517626369?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 15. Understanding Object Dynamics for Interactive Image-to-Video Synthesis

Andreas Blattmann, Timo Milbich, Michael Dorkenwald, Bj√∂rn Ommer

- retweets: 98, favorites: 35 (06/23/2021 10:32:07)

- links: [abs](https://arxiv.org/abs/2106.11303) | [pdf](https://arxiv.org/pdf/2106.11303)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

What would be the effect of locally poking a static scene? We present an approach that learns naturally-looking global articulations caused by a local manipulation at a pixel level. Training requires only videos of moving objects but no information of the underlying manipulation of the physical scene. Our generative model learns to infer natural object dynamics as a response to user interaction and learns about the interrelations between different object body regions. Given a static image of an object and a local poking of a pixel, the approach then predicts how the object would deform over time. In contrast to existing work on video prediction, we do not synthesize arbitrary realistic videos but enable local interactive control of the deformation. Our model is not restricted to particular object categories and can transfer dynamics onto novel unseen object instances. Extensive experiments on diverse objects demonstrate the effectiveness of our approach compared to common video prediction frameworks. Project page is available at https://bit.ly/3cxfA2L .

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Understanding Object Dynamics for Interactive Image-to-Video Synthesis<br>pdf: <a href="https://t.co/kOfQrMw69K">https://t.co/kOfQrMw69K</a><br>abs: <a href="https://t.co/LvXna6pQuU">https://t.co/LvXna6pQuU</a><br>project page: <a href="https://t.co/gYrMQjgkYr">https://t.co/gYrMQjgkYr</a><br><br>static image of an object and a local poking of a pixel, approach predicts how the object would deform over time <a href="https://t.co/ITLDPKDCte">pic.twitter.com/ITLDPKDCte</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1407163224504639489?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 16. Scenic4RL: Programmatic Modeling and Generation of Reinforcement  Learning Environments

Abdus Salam Azad, Edward Kim, Qiancheng Wu, Kimin Lee, Ion Stoica, Pieter Abbeel, Sanjit A. Seshia

- retweets: 72, favorites: 34 (06/23/2021 10:32:07)

- links: [abs](https://arxiv.org/abs/2106.10365) | [pdf](https://arxiv.org/pdf/2106.10365)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

The capability of reinforcement learning (RL) agent directly depends on the diversity of learning scenarios the environment generates and how closely it captures real-world situations. However, existing environments/simulators lack the support to systematically model distributions over initial states and transition dynamics. Furthermore, in complex domains such as soccer, the space of possible scenarios is infinite, which makes it impossible for one research group to provide a comprehensive set of scenarios to train, test, and benchmark RL algorithms. To address this issue, for the first time, we adopt an existing formal scenario specification language, SCENIC, to intuitively model and generate interactive scenarios. We interfaced SCENIC to Google Research Soccer environment to create a platform called SCENIC4RL. Using this platform, we provide a dataset consisting of 36 scenario programs encoded in SCENIC and demonstration data generated from a subset of them. We share our experimental results to show the effectiveness of our dataset and the platform to train, test, and benchmark RL algorithms. More importantly, we open-source our platform to enable RL community to collectively contribute to constructing a comprehensive set of scenarios.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Scenic4RL: Programmatic Modeling and Generation of Reinforcement Learning Environments<br>pdf: <a href="https://t.co/7ZnzOIOSrj">https://t.co/7ZnzOIOSrj</a><br><br>platform for generation of diverse scenarios for Reinforcement Learning programmatically, using the SCENIC scenario specification language <a href="https://t.co/TFAJjrvhFN">pic.twitter.com/TFAJjrvhFN</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1407166927357853696?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 17. DiGS : Divergence guided shape implicit neural representation for  unoriented point clouds

Yizhak Ben-Shabat, Chamin Hewa Koneputugodage, Stephen Gould

- retweets: 36, favorites: 50 (06/23/2021 10:32:07)

- links: [abs](https://arxiv.org/abs/2106.10811) | [pdf](https://arxiv.org/pdf/2106.10811)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Neural shape representations have recently shown to be effective in shape analysis and reconstruction tasks. Existing neural network methods require point coordinates and corresponding normal vectors to learn the implicit level sets of the shape. Normal vectors are often not provided as raw data, therefore, approximation and reorientation are required as pre-processing stages, both of which can introduce noise. In this paper, we propose a divergence guided shape representation learning approach that does not require normal vectors as input. We show that incorporating a soft constraint on the divergence of the distance function favours smooth solutions that reliably orients gradients to match the unknown normal at each point, in some cases even better than approaches that use ground truth normal vectors directly. Additionally, we introduce a novel geometric initialization method for sinusoidal shape representation networks that further improves convergence to the desired solution. We evaluate the effectiveness of our approach on the task of surface reconstruction and show state-of-the-art performance compared to other unoriented methods and on-par performance compared to oriented methods.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">DiGS : Divergence guided shape implicit neural representation for unoriented point clouds<br>pdf: <a href="https://t.co/HfT8LN7dpj">https://t.co/HfT8LN7dpj</a><br>abs: <a href="https://t.co/cxYS5WYvd1">https://t.co/cxYS5WYvd1</a> <a href="https://t.co/zzxHij5tnD">pic.twitter.com/zzxHij5tnD</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1407180133639802881?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 18. Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval

Xiao Wang, Craig Macdonald, Nicola Tonellotto, Iadh Ounis

- retweets: 64, favorites: 19 (06/23/2021 10:32:07)

- links: [abs](https://arxiv.org/abs/2106.11251) | [pdf](https://arxiv.org/pdf/2106.11251)
- [cs.IR](https://arxiv.org/list/cs.IR/recent)

Pseudo-relevance feedback mechanisms, from Rocchio to the relevance models, have shown the usefulness of expanding and reweighting the users' initial queries using information occurring in an initial set of retrieved documents, known as the pseudo-relevant set. Recently, dense retrieval -- through the use of neural contextual language models such as BERT for analysing the documents' and queries' contents and computing their relevance scores -- has shown a promising performance on several information retrieval tasks still relying on the traditional inverted index for identifying documents relevant to a query. Two different dense retrieval families have emerged: the use of single embedded representations for each passage and query (e.g. using BERT's [CLS] token), or via multiple representations (e.g. using an embedding for each token of the query and document). In this work, we conduct the first study into the potential for multiple representation dense retrieval to be enhanced using pseudo-relevance feedback. In particular, based on the pseudo-relevant set of documents identified using a first-pass dense retrieval, we extract representative feedback embeddings - while ensuring that these embeddings discriminate among passages -- which are then added to the query representation. These additional feedback embeddings are shown to both enhance the effectiveness of a reranking as well as an additional dense retrieval operation. Indeed, experiments on the MSMARCO passage ranking dataset show that MAP can be improved by upto 26% on the TREC 2019 query set and 10% on the TREC 2020 query set by the application of our proposed ColBERT-PRF method on a ColBERT dense retrieval approach.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Happy to share the ùóΩùóøùó≤ùóΩùóøùó∂ùóªùòÅ of our <a href="https://twitter.com/hashtag/ictir2021?src=hash&amp;ref_src=twsrc%5Etfw">#ictir2021</a> paper ‚ÄúPseudo-Relevance Feedback for Multiple Representation Dense Retrieval‚Äù with <a href="https://twitter.com/craig_macdonald?ref_src=twsrc%5Etfw">@craig_macdonald</a> <a href="https://twitter.com/ntonellotto?ref_src=twsrc%5Etfw">@ntonellotto</a> and <a href="https://twitter.com/iadh?ref_src=twsrc%5Etfw">@iadh</a>.<br><br>üì∞Preprint: <a href="https://t.co/anjXcNZAKW">https://t.co/anjXcNZAKW</a>. <br>üîóGithub: <a href="https://t.co/vdntN5EIe0">https://t.co/vdntN5EIe0</a> <a href="https://t.co/tuHfvhoNaS">pic.twitter.com/tuHfvhoNaS</a></p>&mdash; Xiao.W (@_XiaoWang_) <a href="https://twitter.com/_XiaoWang_/status/1407357979565957126?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 19. MeshRIR: A Dataset of Room Impulse Responses on Meshed Grid Points For  Evaluating Sound Field Analysis and Synthesis Methods

Shoichi Koyama, Tomoya Nishida, Keisuke Kimura, Takumi Abe, Natsuki Ueno, Jesper Brunnstr√∂m

- retweets: 56, favorites: 22 (06/23/2021 10:32:07)

- links: [abs](https://arxiv.org/abs/2106.10801) | [pdf](https://arxiv.org/pdf/2106.10801)
- [eess.AS](https://arxiv.org/list/eess.AS/recent) | [cs.SD](https://arxiv.org/list/cs.SD/recent)

A new impulse response (IR) dataset called "MeshRIR" is introduced. Currently available datasets usually include IRs at an array of microphones from several source positions under various room conditions, which are basically designed for evaluating speech enhancement and distant speech recognition methods. On the other hand, methods of estimating or controlling spatial sound fields have been extensively investigated in recent years; however, the current IR datasets are not applicable to validating and comparing these methods because of the low spatial resolution of measurement points. MeshRIR consists of IRs measured at positions obtained by finely discretizing a spatial region. Two subdatasets are currently available: one consists of IRs in a three-dimensional cuboidal region from a single source, and the other consists of IRs in a two-dimensional square region from an array of 32 sources. Therefore, MeshRIR is suitable for evaluating sound field analysis and synthesis methods. This dataset is freely available at \url{https://sh01k.github.io/MeshRIR/} with some codes of sample applications.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Published MeshRIR - room impulse response dataset on meshed grid pointsüí• You can visualize wave motionüåä<br>- website: <a href="https://t.co/Dfd0JINQwD">https://t.co/Dfd0JINQwD</a><br>- preprint: <a href="https://t.co/O3qxEH7WRH">https://t.co/O3qxEH7WRH</a> <a href="https://t.co/dxVpBOmZ0h">pic.twitter.com/dxVpBOmZ0h</a></p>&mdash; Shoichi Koyama (@sh01) <a href="https://twitter.com/sh01/status/1407154948295430147?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 20. Moving in a 360 World: Synthesizing Panoramic Parallaxes from a Single  Panorama

Ching-Yu Hsu, Cheng Sun, Hwann-Tzong Chen

- retweets: 42, favorites: 34 (06/23/2021 10:32:08)

- links: [abs](https://arxiv.org/abs/2106.10859) | [pdf](https://arxiv.org/pdf/2106.10859)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

We present Omnidirectional Neural Radiance Fields (OmniNeRF), the first method to the application of parallax-enabled novel panoramic view synthesis. Recent works for novel view synthesis focus on perspective images with limited field-of-view and require sufficient pictures captured in a specific condition. Conversely, OmniNeRF can generate panorama images for unknown viewpoints given a single equirectangular image as training data. To this end, we propose to augment the single RGB-D panorama by projecting back and forth between a 3D world and different 2D panoramic coordinates at different virtual camera positions. By doing so, we are able to optimize an Omnidirectional Neural Radiance Field with visible pixels collecting from omnidirectional viewing angles at a fixed center for the estimation of new viewing angles from varying camera positions. As a result, the proposed OmniNeRF achieves convincing renderings of novel panoramic views that exhibit the parallax effect. We showcase the effectiveness of each of our proposals on both synthetic and real-world datasets.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Moving in a 360 World: Synthesizing Panoramic Parallaxes from a Single Panorama<br>pdf: <a href="https://t.co/4FyesN29ia">https://t.co/4FyesN29ia</a><br>abs: <a href="https://t.co/BknCJii2Kr">https://t.co/BknCJii2Kr</a> <a href="https://t.co/Nv2AIQMdvr">pic.twitter.com/Nv2AIQMdvr</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1407181539990028291?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 21. Towards Long-Form Video Understanding

Chao-Yuan Wu, Philipp Kr√§henb√ºhl

- retweets: 30, favorites: 26 (06/23/2021 10:32:08)

- links: [abs](https://arxiv.org/abs/2106.11310) | [pdf](https://arxiv.org/pdf/2106.11310)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Our world offers a never-ending stream of visual stimuli, yet today's vision systems only accurately recognize patterns within a few seconds. These systems understand the present, but fail to contextualize it in past or future events. In this paper, we study long-form video understanding. We introduce a framework for modeling long-form videos and develop evaluation protocols on large-scale datasets. We show that existing state-of-the-art short-term models are limited for long-form tasks. A novel object-centric transformer-based video recognition architecture performs significantly better on 7 diverse tasks. It also outperforms comparable state-of-the-art on the AVA dataset.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Towards Long-Form Video Understanding<br>pdf: <a href="https://t.co/bHyRADq2g3">https://t.co/bHyRADq2g3</a><br>abs: <a href="https://t.co/dyP3cH7We3">https://t.co/dyP3cH7We3</a><br><br>object-centric transformer-based video recognition architecture performs significantly better on 7 diverse tasks <a href="https://t.co/LYGO8653oR">pic.twitter.com/LYGO8653oR</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1407143550987522053?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 22. TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?

Michael S. Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, Anelia Angelova

- retweets: 24, favorites: 28 (06/23/2021 10:32:08)

- links: [abs](https://arxiv.org/abs/2106.11297) | [pdf](https://arxiv.org/pdf/2106.11297)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

In this paper, we introduce a novel visual representation learning which relies on a handful of adaptively learned tokens, and which is applicable to both image and video understanding tasks. Instead of relying on hand-designed splitting strategies to obtain visual tokens and processing a large number of densely sampled patches for attention, our approach learns to mine important tokens in visual data. This results in efficiently and effectively finding a few important visual tokens and enables modeling of pairwise attention between such tokens, over a longer temporal horizon for videos, or the spatial content in images. Our experiments demonstrate strong performance on several challenging benchmarks for both image and video recognition tasks. Importantly, due to our tokens being adaptive, we accomplish competitive results at significantly reduced compute amount.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?<br>pdf: <a href="https://t.co/eDZTwj9vLk">https://t.co/eDZTwj9vLk</a><br>abs: <a href="https://t.co/9rfY9Lkdce">https://t.co/9rfY9Lkdce</a><br><br>visual representation learning which relies on<br>a handful of adaptively learned tokens, and which is applicable to both image and video understanding tasks <a href="https://t.co/Bzxq2aC2DV">pic.twitter.com/Bzxq2aC2DV</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1407186234548572161?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



