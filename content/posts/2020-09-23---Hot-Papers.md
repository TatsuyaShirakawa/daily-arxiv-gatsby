---
title: Hot Papers 2020-09-23
date: 2020-09-24T09:22:01.Z
template: "post"
draft: false
slug: "hot-papers-2020-09-23"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-09-23"
socialImage: "/media/flying-marine.jpg"

---

# 1. Ethical Machine Learning in Health

Irene Y. Chen, Emma Pierson, Sherri Rose, Shalmali Joshi, Kadija Ferryman, Marzyeh Ghassemi

- retweets: 10533, favorites: 5 (09/24/2020 09:22:01)

- links: [abs](https://arxiv.org/abs/2009.10576) | [pdf](https://arxiv.org/pdf/2009.10576)
- [cs.CY](https://arxiv.org/list/cs.CY/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

The use of machine learning (ML) in health care raises numerous ethical concerns, especially as models can amplify existing health inequities. Here, we outline ethical considerations for equitable ML in the advancement of health care. Specifically, we frame ethics of ML in health care through the lens of social justice. We describe ongoing efforts and outline challenges in a proposed pipeline of ethical ML in health, ranging from problem selection to post-deployment considerations. We close by summarizing recommendations to address these challenges.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Dream come true to coauthor a review article on ethical machine learning in health care with this team of brilliant women: <a href="https://twitter.com/irenetrampoline?ref_src=twsrc%5Etfw">@irenetrampoline</a>, <a href="https://twitter.com/2plus2make5?ref_src=twsrc%5Etfw">@2plus2make5</a>, Shalmali Joshi,  <a href="https://twitter.com/KadijaFerryman?ref_src=twsrc%5Etfw">@KadijaFerryman</a> &amp; <a href="https://twitter.com/MarzyehGhassemi?ref_src=twsrc%5Etfw">@MarzyehGhassemi</a>.<a href="https://t.co/OSeqzKaPMf">https://t.co/OSeqzKaPMf</a> <a href="https://t.co/1zw8mToaZR">pic.twitter.com/1zw8mToaZR</a></p>&mdash; Sherri Rose (@sherrirose) <a href="https://twitter.com/sherrirose/status/1308567248571777024?ref_src=twsrc%5Etfw">September 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving  Out-of-Domain Robustness

Nathan Ng, Kyunghyun Cho, Marzyeh Ghassemi

- retweets: 1397, favorites: 180 (09/24/2020 09:22:01)

- links: [abs](https://arxiv.org/abs/2009.10195) | [pdf](https://arxiv.org/pdf/2009.10195)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold. We introduce SSMBA, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold. We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models. In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8% accuracy on OOD Amazon reviews, 1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">in <a href="https://t.co/bfF4KPMb1J">https://t.co/bfF4KPMb1J</a>, <a href="https://twitter.com/learn_ng?ref_src=twsrc%5Etfw">@learn_ng</a> draws a connection between VRM (chapelle &amp; <a href="https://twitter.com/jaseweston?ref_src=twsrc%5Etfw">@jaseweston</a> et al. <a href="https://t.co/3Ro0MHIiXY">https://t.co/3Ro0MHIiXY</a>; in particular ¬ß4.2) and denoising autoencoder, and shows the effectiveness of using BERT-like MLM for data augmentation in 3 tasks with 9 datasets in NLP. <a href="https://t.co/89rIdSq5FF">pic.twitter.com/89rIdSq5FF</a></p>&mdash; Kyunghyun Cho (@kchonyc) <a href="https://twitter.com/kchonyc/status/1308594325182459904?ref_src=twsrc%5Etfw">September 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">New work with <a href="https://twitter.com/kchonyc?ref_src=twsrc%5Etfw">@kchonyc</a> and <a href="https://twitter.com/MarzyehGhassemi?ref_src=twsrc%5Etfw">@MarzyehGhassemi</a>!<br><br>arxiv: <a href="https://t.co/ApaaP3xNkf">https://t.co/ApaaP3xNkf</a><br>code: <a href="https://t.co/th47UxaxLT">https://t.co/th47UxaxLT</a> <br><br>We propose a novel data augmentation scheme based on using a pair of corruption and reconstruction functions to generate new examples along an underlying data manifold. 1/ <a href="https://t.co/oIFJS0VNVu">pic.twitter.com/oIFJS0VNVu</a></p>&mdash; Nathan Ng (@learn_ng) <a href="https://twitter.com/learn_ng/status/1308805309368954881?ref_src=twsrc%5Etfw">September 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. A survey on Kornia: an Open Source Differentiable Computer Vision  Library for PyTorch

E. Riba, D. Mishkin, J. Shi, D. Ponsa, F. Moreno-Noguer, G. Bradski

- retweets: 585, favorites: 130 (09/24/2020 09:22:02)

- links: [abs](https://arxiv.org/abs/2009.10521) | [pdf](https://arxiv.org/pdf/2009.10521)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

This work presents Kornia, an open source computer vision library built upon a set of differentiable routines and modules that aims to solve generic computer vision problems. The package uses PyTorch as its main backend, not only for efficiency but also to take advantage of the reverse auto-differentiation engine to define and compute the gradient of complex functions. Inspired by OpenCV, Kornia is composed of a set of modules containing operators that can be integrated into neural networks to train models to perform a wide range of operations including image transformations,camera calibration, epipolar geometry, and low level image processing techniques, such as filtering and edge detection that operate directly on high dimensional tensor representations on graphical processing units, generating faster systems. Examples of classical vision problems implemented using our framework are provided including a benchmark comparing to existing vision libraries.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">E. Riba, D. Mishkin, J. Shi, D. Ponsa, F. Moreno-Noguer, G. Bradski, A survey on Kornia: an Open Source Differentiable <a href="https://twitter.com/hashtag/ComputerVision?src=hash&amp;ref_src=twsrc%5Etfw">#ComputerVision</a> Library for PyTorch, arXiv, 2020<br><br>Paper: <a href="https://t.co/8S2ZSpNKhI">https://t.co/8S2ZSpNKhI</a><a href="https://twitter.com/kornia_foss?ref_src=twsrc%5Etfw">@kornia_foss</a> <a href="https://t.co/eEHNHuJABQ">pic.twitter.com/eEHNHuJABQ</a></p>&mdash; Kosta Derpanis (@CSProfKGD) <a href="https://twitter.com/CSProfKGD/status/1308587215556169728?ref_src=twsrc%5Etfw">September 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">[ùêçùêÑùêñ ùêèùêÄùêèùêÑùêë] now available in <a href="https://twitter.com/hashtag/arxiv?src=hash&amp;ref_src=twsrc%5Etfw">#arxiv</a><br><br>A survey on Kornia: an Open Source Differentiable Computer Vision Library for <a href="https://twitter.com/PyTorch?ref_src=twsrc%5Etfw">@PyTorch</a> <a href="https://t.co/YDrAK8obBU">https://t.co/YDrAK8obBU</a><a href="https://twitter.com/edgarriba?ref_src=twsrc%5Etfw">@edgarriba</a> <a href="https://twitter.com/ducha_aiki?ref_src=twsrc%5Etfw">@ducha_aiki</a> <a href="https://twitter.com/js_shijian?ref_src=twsrc%5Etfw">@js_shijian</a> <a href="https://twitter.com/PonsaDaniel?ref_src=twsrc%5Etfw">@PonsaDaniel</a> <a href="https://twitter.com/fmorenoguer?ref_src=twsrc%5Etfw">@fmorenoguer</a> <a href="https://twitter.com/grbradsk?ref_src=twsrc%5Etfw">@grbradsk</a> <a href="https://t.co/OlT1KbL3C8">https://t.co/OlT1KbL3C8</a></p>&mdash; Kornia (@kornia_foss) <a href="https://twitter.com/kornia_foss/status/1308679310132178944?ref_src=twsrc%5Etfw">September 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. A narrowing of AI research?

Joel Klinger, Juan Mateos-Garcia, Konstantinos Stathoulopoulos

- retweets: 160, favorites: 29 (09/24/2020 09:22:02)

- links: [abs](https://arxiv.org/abs/2009.10385) | [pdf](https://arxiv.org/pdf/2009.10385)
- [cs.CY](https://arxiv.org/list/cs.CY/recent)

Artificial Intelligence (AI) is being hailed as the latest example of a General Purpose Technology that could transform productivity and help tackle important societal challenges. This outcome is however not guaranteed: a myopic focus on short-term benefits could lock AI into technologies that turn out to be sub-optimal in the longer-run. Recent controversies about the dominance of deep learning methods and private labs in AI research suggest that the field may be getting narrower, but the evidence base is lacking. We seek to address this gap with an analysis of the thematic diversity of AI research in arXiv, a widely used pre-prints site. Having identified 110,000 AI papers in this corpus, we use hierarchical topic modelling to estimate the thematic composition of AI research, and this composition to calculate various metrics of research diversity. Our analysis suggests that diversity in AI research has stagnated in recent years, and that AI research involving private sector organisations tends to be less diverse than research in academia. This appears to be driven by a small number of prolific and narrowly-focused technology companies. Diversity in academia is bolstered by smaller institutions and research groups that may have less incentives to `race' and lower levels of collaboration with the private sector. We also find that private sector AI researchers tend to specialise in data and computationally intensive deep learning methods at the expense of research involving other (symbolic and statistical) AI methods, and of research that considers the societal and ethical implications of AI or applies it in domains like health. Our results suggest that there may be a rationale for policy action to prevent a premature narrowing of AI research that could reduce its societal benefits, but we note the incentive, information and scale hurdles standing in the way of such interventions.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">A narrowing of AI research? <br><br>Our new paper about the evolution of thematic diversity in AI research is live in arXiv now. I will post a thread about it sometime soon. Comments welcome!<a href="https://t.co/tabm03OBTi">https://t.co/tabm03OBTi</a> <a href="https://t.co/1Um63oZkBI">pic.twitter.com/1Um63oZkBI</a></p>&mdash; Juan Mateos Garcia (@JMateosGarcia) <a href="https://twitter.com/JMateosGarcia/status/1308664432671756288?ref_src=twsrc%5Etfw">September 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. On the Theory of Modern Quantum Algorithms

Jacob Biamonte

- retweets: 108, favorites: 75 (09/24/2020 09:22:02)

- links: [abs](https://arxiv.org/abs/2009.10088) | [pdf](https://arxiv.org/pdf/2009.10088)
- [quant-ph](https://arxiv.org/list/quant-ph/recent) | [cs.CC](https://arxiv.org/list/cs.CC/recent) | [math-ph](https://arxiv.org/list/math-ph/recent)

This dissertation unites variational computation with results and techniques appearing in the theory of ground state computation. It should be readable by graduate students.   The topics covered include: Ising model reductions, stochastic versus quantum processes on graphs, quantum gates and circuits as tensor networks, variational quantum algorithms and Hamiltonian gadgets.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Second doctorate: &quot;On the Theory of Modern Quantum Algorithms&quot;<br><br>The dissertation focuses on variational computation and ground state computation. <br><br>It should be readable by graduate students. <a href="https://t.co/2KLXhmow2l">https://t.co/2KLXhmow2l</a></p>&mdash; Jacob D Biamonte (@JacobBiamonte) <a href="https://twitter.com/JacobBiamonte/status/1308656596373909506?ref_src=twsrc%5Etfw">September 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. Proposal of a Novel Bug Bounty Implementation Using Gamification

Jamie O'Hare, Lynsay A. Shepherd

- retweets: 90, favorites: 29 (09/24/2020 09:22:02)

- links: [abs](https://arxiv.org/abs/2009.10158) | [pdf](https://arxiv.org/pdf/2009.10158)
- [cs.CR](https://arxiv.org/list/cs.CR/recent) | [cs.CY](https://arxiv.org/list/cs.CY/recent) | [cs.HC](https://arxiv.org/list/cs.HC/recent)

Despite significant popularity, the bug bounty process has remained broadly unchanged since its inception, with limited implementation of gamification aspects. Existing literature recognises that current methods generate intensive resource demands, and can encounter issues impacting program effectiveness. This paper proposes a novel bug bounty process aiming to alleviate resource demands and mitigate inherent issues. Through the additional crowdsourcing of report verification where fellow hackers perform vulnerability verification and reproduction, the client organisation can reduce overheads at the cost of rewarding more participants. The incorporation of gamification elements provides a substitute for monetary rewards, as well as presenting possible mitigation of bug bounty program effectiveness issues. Collectively, traits of the proposed process appear appropriate for resource and budget-constrained organisations - such Higher Education institutions.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">üì∞ <a href="https://twitter.com/Lynsay?ref_src=twsrc%5Etfw">@Lynsay</a> and I&#39;s <a href="https://twitter.com/hashtag/BugBounty?src=hash&amp;ref_src=twsrc%5Etfw">#BugBounty</a> paper is now live on arXiv. <br><br>We propose a possible bug bounty solution appropriate for resource and economically limited organisations, such as Universities. <br><br>Link üëâ <a href="https://t.co/OqAtMo55lV">https://t.co/OqAtMo55lV</a><br><br>Find out more information in the thread below üëá <a href="https://t.co/k6S8a4mGex">pic.twitter.com/k6S8a4mGex</a></p>&mdash; Jamie O&#39;Hare (@TheHairyJ) <a href="https://twitter.com/TheHairyJ/status/1308697467144335360?ref_src=twsrc%5Etfw">September 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. ALICE: Active Learning with Contrastive Natural Language Explanations

Weixin Liang, James Zou, Zhou Yu

- retweets: 30, favorites: 35 (09/24/2020 09:22:02)

- links: [abs](https://arxiv.org/abs/2009.10259) | [pdf](https://arxiv.org/pdf/2009.10259)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.HC](https://arxiv.org/list/cs.HC/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Training a supervised neural network classifier typically requires many annotated training samples. Collecting and annotating a large number of data points are costly and sometimes even infeasible. Traditional annotation process uses a low-bandwidth human-machine communication interface: classification labels, each of which only provides several bits of information. We propose Active Learning with Contrastive Explanations (ALICE), an expert-in-the-loop training framework that utilizes contrastive natural language explanations to improve data efficiency in learning. ALICE learns to first use active learning to select the most informative pairs of label classes to elicit contrastive natural language explanations from experts. Then it extracts knowledge from these explanations using a semantic parser. Finally, it incorporates the extracted knowledge through dynamically changing the learning model's structure. We applied ALICE in two visual recognition tasks, bird species classification and social relationship classification. We found by incorporating contrastive explanations, our models outperform baseline models that are trained with 40-100% more training data. We found that adding 1 explanation leads to similar performance gain as adding 13-30 labeled training data points.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Our new <a href="https://twitter.com/hashtag/emnlp?src=hash&amp;ref_src=twsrc%5Etfw">#emnlp</a> paper shows how to teach ML via natural language explanation of contrasts between concepts (eg &quot; difference between COVID and flu is ...&quot;).<br><br>It&#39;s much more efficient than using labeled examples. Excited for more human-like learning! <a href="https://t.co/q0dznybPNe">https://t.co/q0dznybPNe</a> <a href="https://t.co/eeV58VkdJl">pic.twitter.com/eeV58VkdJl</a></p>&mdash; James Zou (@james_y_zou) <a href="https://twitter.com/james_y_zou/status/1308767903370547200?ref_src=twsrc%5Etfw">September 23, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



