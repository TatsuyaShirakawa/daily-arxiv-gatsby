---
title: Hot Papers 2021-05-10
date: 2021-05-11T09:06:35.Z
template: "post"
draft: false
slug: "hot-papers-2021-05-10"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2021-05-10"
socialImage: "/media/flying-marine.jpg"

---

# 1. A Survey of Data Augmentation Approaches for NLP

Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, Eduard Hovy

- retweets: 12395, favorites: 43 (05/11/2021 09:06:35)

- links: [abs](https://arxiv.org/abs/2105.03075) | [pdf](https://arxiv.org/pdf/2105.03075)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Data augmentation has recently seen increased interest in NLP due to more work in low-resource domains, new tasks, and the popularity of large-scale neural networks that require large amounts of training data. Despite this recent upsurge, this area is still relatively underexplored, perhaps due to the challenges posed by the discrete nature of language data. In this paper, we present a comprehensive and unifying survey of data augmentation for NLP by summarizing the literature in a structured manner. We first introduce and motivate data augmentation for NLP, and then discuss major methodologically representative approaches. Next, we highlight techniques that are used for popular NLP applications and tasks. We conclude by outlining current challenges and directions for future research. Overall, our paper aims to clarify the landscape of existing literature in data augmentation for NLP and motivate additional work in this area.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Data augmentation is useful in machine learning as more needs for increasing data diversity emerge.<br><br>It&#39;s nice to see this recent survey paper summarizing the latest data augmentation approaches for NLP.<br><br>A must-read for ML/NLP students and practitioners.<a href="https://t.co/djbnXyHRGT">https://t.co/djbnXyHRGT</a> <a href="https://t.co/PYYdqkbzVX">pic.twitter.com/PYYdqkbzVX</a></p>&mdash; elvis (@omarsar0) <a href="https://twitter.com/omarsar0/status/1391729605766262785?ref_src=twsrc%5Etfw">May 10, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Are Pre-trained Convolutions Better than Pre-trained Transformers?

Yi Tay, Mostafa Dehghani, Jai Gupta, Dara Bahri, Vamsi Aribandi, Zhen Qin, Donald Metzler

- retweets: 6087, favorites: 407 (05/11/2021 09:06:36)

- links: [abs](https://arxiv.org/abs/2105.03322) | [pdf](https://arxiv.org/pdf/2105.03322)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

In the era of pre-trained language models, Transformers are the de facto choice of model architectures. While recent research has shown promise in entirely convolutional, or CNN, architectures, they have not been explored using the pre-train-fine-tune paradigm. In the context of language models, are convolutional models competitive to Transformers when pre-trained? This paper investigates this research question and presents several interesting findings. Across an extensive set of experiments on 8 datasets/tasks, we find that CNN-based pre-trained models are competitive and outperform their Transformer counterpart in certain scenarios, albeit with caveats. Overall, the findings outlined in this paper suggest that conflating pre-training and architectural advances is misguided and that both advances should be considered independently. We believe our research paves the way for a healthy amount of optimism in alternative architectures.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">2月：「Transformer is All you Need！」<a href="https://t.co/XGmFWuxAgF">https://t.co/XGmFWuxAgF</a><br>3月：「Attention is not All you Need！」<a href="https://t.co/4lByFg3EhM">https://t.co/4lByFg3EhM</a><br>今日：「Convolutionの方がTransformerより強いのでは?」<a href="https://t.co/Q9Yz6wYgF0">https://t.co/Q9Yz6wYgF0</a><br>Transformer周りの界隈，魔境すぎでは</p>&mdash; えるエル (@ImAI_Eruel) <a href="https://twitter.com/ImAI_Eruel/status/1391710077548077063?ref_src=twsrc%5Etfw">May 10, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Are Pre-trained Convolutions Better than Pre-trained Transformers?<br>pdf: <a href="https://t.co/8L06XiPM1C">https://t.co/8L06XiPM1C</a><br>abs: <a href="https://t.co/gIAq2Od5GA">https://t.co/gIAq2Od5GA</a><br><br>experimental results show that convolutions can outperform Transformers in both pretrain and non-pre-trained setups <a href="https://t.co/IZtYlBXvkc">pic.twitter.com/IZtYlBXvkc</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1391556921325408258?ref_src=twsrc%5Etfw">May 10, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. ResMLP: Feedforward networks for image classification with  data-efficient training

Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard Grave, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, Hervé Jégou

- retweets: 1195, favorites: 392 (05/11/2021 09:06:36)

- links: [abs](https://arxiv.org/abs/2105.03404) | [pdf](https://arxiv.org/pdf/2105.03404)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

We present ResMLP, an architecture built entirely upon multi-layer perceptrons for image classification. It is a simple residual network that alternates (i) a linear layer in which image patches interact, independently and identically across channels, and (ii) a two-layer feed-forward network in which channels interact independently per patch. When trained with a modern training strategy using heavy data-augmentation and optionally distillation, it attains surprisingly good accuracy/complexity trade-offs on ImageNet. We will share our code based on the Timm library and pre-trained models.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">ResMLP: Feedforward networks for image classification with data-efficient training<br><br>- proposes ResMLP, an architecture built entirely upon MLPs for image classification. <br><br>- attains surprisingly good accuracy/complexity trade-offs on ImageNet. <a href="https://t.co/R9bGmszryB">https://t.co/R9bGmszryB</a> <a href="https://t.co/cihkPrbkWy">pic.twitter.com/cihkPrbkWy</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1391553324361519106?ref_src=twsrc%5Etfw">May 10, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Repeat after me, another day, another MLP architecture<a href="https://t.co/XdOGML9Vmy">https://t.co/XdOGML9Vmy</a> <a href="https://t.co/7EtombxELq">pic.twitter.com/7EtombxELq</a></p>&mdash; Kosta Derpanis (@CSProfKGD) <a href="https://twitter.com/CSProfKGD/status/1391578954432139267?ref_src=twsrc%5Etfw">May 10, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">ResMLP: Feedforward networks for image classification with data-efficient training<br>pdf: <a href="https://t.co/0cTkeTwwne">https://t.co/0cTkeTwwne</a><br>abs: <a href="https://t.co/DA1zkiYq0S">https://t.co/DA1zkiYq0S</a><br><br>an architecture built entirely upon multi-layer perceptrons for image classification <a href="https://t.co/7VmEyovIkW">pic.twitter.com/7VmEyovIkW</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1391559126816370701?ref_src=twsrc%5Etfw">May 10, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. What Kinds of Functions do Deep Neural Networks Learn? Insights from  Variational Spline Theory

Rahul Parhi, Robert D. Nowak

- retweets: 1082, favorites: 232 (05/11/2021 09:06:36)

- links: [abs](https://arxiv.org/abs/2105.03361) | [pdf](https://arxiv.org/pdf/2105.03361)
- [stat.ML](https://arxiv.org/list/stat.ML/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

We develop a variational framework to understand the properties of functions learned by deep neural networks with ReLU activation functions fit to data. We propose a new function space, which is reminiscent of classical bounded variation spaces, that captures the compositional structure associated with deep neural networks. We derive a representer theorem showing that deep ReLU networks are solutions to regularized data fitting problems in this function space. The function space consists of compositions of functions from the (non-reflexive) Banach spaces of second-order bounded variation in the Radon domain. These are Banach spaces with sparsity-promoting norms, giving insight into the role of sparsity in deep neural networks. The neural network solutions have skip connections and rank bounded weight matrices, providing new theoretical support for these common architectural choices. The variational problem we study can be recast as a finite-dimensional neural network training problem with regularization schemes related to the notions of weight decay and path-norm regularization. Finally, our analysis builds on techniques from variational spline theory, providing new connections between deep neural networks and splines.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">What kinds of functions do deep neural networks learn?  Now we know!<a href="https://t.co/MHuSPPMd23">https://t.co/MHuSPPMd23</a><br>Our paper presents a new representer theorem for deep ReLU networks and provides theoretical insights into  weight decay, sparsity, skip connections, and low-rank weight matrices.</p>&mdash; Rob Nowak (@rdnowak) <a href="https://twitter.com/rdnowak/status/1391829045889863683?ref_src=twsrc%5Etfw">May 10, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Learning Controllable Content Generators

Sam Earle, Maria Edwards, Ahmed Khalifa, Philip Bontrager, Julian Togelius

- retweets: 247, favorites: 130 (05/11/2021 09:06:37)

- links: [abs](https://arxiv.org/abs/2105.02993) | [pdf](https://arxiv.org/pdf/2105.02993)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

It has recently been shown that reinforcement learning can be used to train generators capable of producing high-quality game levels, with quality defined in terms of some user-specified heuristic. To ensure that these generators' output is sufficiently diverse (that is, not amounting to the reproduction of a single optimal level configuration), the generation process is constrained such that the initial seed results in some variance in the generator's output. However, this results in a loss of control over the generated content for the human user. We propose to train generators capable of producing controllably diverse output, by making them "goal-aware." To this end, we add conditional inputs representing how close a generator is to some heuristic, and also modify the reward mechanism to incorporate that value. Testing on multiple domains, we show that the resulting level generators are capable of exploring the space of possible levels in a targeted, controllable manner, producing levels of comparable quality as their goal-unaware counterparts, that are diverse along designer-specified dimensions.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">In &quot;Learning Controllable Content Generators,&quot; with Maria Edwards, <a href="https://twitter.com/Amidos2006?ref_src=twsrc%5Etfw">@Amidos2006</a>, <a href="https://twitter.com/FilipoGiovanni?ref_src=twsrc%5Etfw">@FilipoGiovanni</a>, and <a href="https://twitter.com/togelius?ref_src=twsrc%5Etfw">@togelius</a>, we train an RL agent to generate game levels with specific features by feeding the agent targets as input and rewarding it for approaching them.<a href="https://t.co/VaKxX3TVMz">https://t.co/VaKxX3TVMz</a> <a href="https://t.co/5YhmSvkn0l">pic.twitter.com/5YhmSvkn0l</a></p>&mdash; Sam Earle (@Smearle_RH) <a href="https://twitter.com/Smearle_RH/status/1391592454206406657?ref_src=twsrc%5Etfw">May 10, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Learning Controllable Content Generators<br>pdf: <a href="https://t.co/SkkFy1SlRX">https://t.co/SkkFy1SlRX</a><br>abs: <a href="https://t.co/qStOBsLM7R">https://t.co/qStOBsLM7R</a><br><br>resulting level generators explore the space of possible levels in a targeted, controllable manner, producing levels of comparable quality as their goal-unaware counterparts <a href="https://t.co/8NJZi7b9o9">pic.twitter.com/8NJZi7b9o9</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1391565640448757762?ref_src=twsrc%5Etfw">May 10, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. LASR: Learning Articulated Shape Reconstruction from a Monocular Video

Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic, Forrester Cole, Huiwen Chang, Deva Ramanan, William T. Freeman, Ce Liu

- retweets: 229, favorites: 99 (05/11/2021 09:06:37)

- links: [abs](https://arxiv.org/abs/2105.02976) | [pdf](https://arxiv.org/pdf/2105.02976)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.GR](https://arxiv.org/list/cs.GR/recent)

Remarkable progress has been made in 3D reconstruction of rigid structures from a video or a collection of images. However, it is still challenging to reconstruct nonrigid structures from RGB inputs, due to its under-constrained nature. While template-based approaches, such as parametric shape models, have achieved great success in modeling the "closed world" of known object categories, they cannot well handle the "open-world" of novel object categories or outlier shapes. In this work, we introduce a template-free approach to learn 3D shapes from a single video. It adopts an analysis-by-synthesis strategy that forward-renders object silhouette, optical flow, and pixel values to compare with video observations, which generates gradients to adjust the camera, shape and motion parameters. Without using a category-specific shape template, our method faithfully reconstructs nonrigid 3D structures from videos of human, animals, and objects of unknown classes. Code will be available at lasr-google.github.io .

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">LASR: Learning Articulated Shape Reconstruction from a Monocular Video<br>pdf: <a href="https://t.co/0oB93bnY6j">https://t.co/0oB93bnY6j</a><br>abs: <a href="https://t.co/gi1lfVL1q4">https://t.co/gi1lfVL1q4</a><br>project page: <a href="https://t.co/AK9BxWgMqa">https://t.co/AK9BxWgMqa</a> <a href="https://t.co/gUforGguA6">pic.twitter.com/gUforGguA6</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1391560893343932418?ref_src=twsrc%5Etfw">May 10, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. Contrastive Learning for Unsupervised Image-to-Image Translation

Hanbit Lee, Jinseok Seol, Sang-goo Lee

- retweets: 135, favorites: 99 (05/11/2021 09:06:37)

- links: [abs](https://arxiv.org/abs/2105.03117) | [pdf](https://arxiv.org/pdf/2105.03117)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Image-to-image translation aims to learn a mapping between different groups of visually distinguishable images. While recent methods have shown impressive ability to change even intricate appearance of images, they still rely on domain labels in training a model to distinguish between distinct visual features. Such dependency on labels often significantly limits the scope of applications since consistent and high-quality labels are expensive. Instead, we wish to capture visual features from images themselves and apply them to enable realistic translation without human-generated labels. To this end, we propose an unsupervised image-to-image translation method based on contrastive learning. The key idea is to learn a discriminator that differentiates between distinctive styles and let the discriminator supervise a generator to transfer those styles across images. During training, we randomly sample a pair of images and train the generator to change the appearance of one towards another while keeping the original structure. Experimental results show that our method outperforms the leading unsupervised baselines in terms of visual quality and translation accuracy.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Contrastive Learning for Unsupervised Image-to-Image Translation<br>pdf: <a href="https://t.co/tiRPgcY0ic">https://t.co/tiRPgcY0ic</a><br>abs: <a href="https://t.co/OoyrZ6ERqQ">https://t.co/OoyrZ6ERqQ</a><br><br>learn a discriminator that differentiates between distinctive styles and let the discriminator supervise a generator to transfer those styles across images <a href="https://t.co/PigLyp2795">pic.twitter.com/PigLyp2795</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1391563832632520707?ref_src=twsrc%5Etfw">May 10, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ko" dir="ltr">아카이브에 논문 뭐 하나 올림 <a href="https://t.co/X964maiUrb">https://t.co/X964maiUrb</a></p>&mdash; 제이미 (@theeluwin) <a href="https://twitter.com/theeluwin/status/1391566690236829699?ref_src=twsrc%5Etfw">May 10, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. Structured dataset documentation: a datasheet for CheXpert

Christian Garbin, Pranav Rajpurkar, Jeremy Irvin, Matthew P. Lungren, Oge Marques

- retweets: 78, favorites: 30 (05/11/2021 09:06:37)

- links: [abs](https://arxiv.org/abs/2105.03020) | [pdf](https://arxiv.org/pdf/2105.03020)
- [eess.IV](https://arxiv.org/list/eess.IV/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Billions of X-ray images are taken worldwide each year. Machine learning, and deep learning in particular, has shown potential to help radiologists triage and diagnose images. However, deep learning requires large datasets with reliable labels. The CheXpert dataset was created with the participation of board-certified radiologists, resulting in the strong ground truth needed to train deep learning networks. Following the structured format of Datasheets for Datasets, this paper expands on the original CheXpert paper and other sources to show the critical role played by radiologists in the creation of reliable labels and to describe the different aspects of the dataset composition in detail. Such structured documentation intends to increase the awareness in the machine learning and medical communities of the strengths, applications, and evolution of CheXpert, thereby advancing the field of medical image analysis. Another objective of this paper is to put forward this dataset datasheet as an example to the community of how to create detailed and structured descriptions of datasets. We believe that clearly documenting the creation process, the contents, and applications of datasets accelerates the creation of useful and reliable models.




# 9. Hierarchical Graph Neural Networks

Stanislav Sobolevsky

- retweets: 42, favorites: 43 (05/11/2021 09:06:37)

- links: [abs](https://arxiv.org/abs/2105.03388) | [pdf](https://arxiv.org/pdf/2105.03388)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [math.CO](https://arxiv.org/list/math.CO/recent) | [physics.data-an](https://arxiv.org/list/physics.data-an/recent)

Over the recent years, Graph Neural Networks have become increasingly popular in network analytic and beyond. With that, their architecture noticeable diverges from the classical multi-layered hierarchical organization of the traditional neural networks. At the same time, many conventional approaches in network science efficiently utilize the hierarchical approaches to account for the hierarchical organization of the networks, and recent works emphasize their critical importance. This paper aims to connect the dots between the traditional Neural Network and the Graph Neural Network architectures as well as the network science approaches, harnessing the power of the hierarchical network organization. A Hierarchical Graph Neural Network architecture is proposed, supplementing the original input network layer with the hierarchy of auxiliary network layers and organizing the computational scheme updating the node features through both - horizontal network connections within each layer as well as the vertical connection between the layers. It enables simultaneous learning of the individual node features along with the aggregated network features at variable resolution and uses them to improve the convergence and stability of the individual node feature learning. The proposed Hierarchical Graph Neural network architecture is successfully evaluated on the network embedding and modeling as well as network classification, node labeling, and community tasks and demonstrates increased efficiency in those.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Over the recent years, Graph <a href="https://twitter.com/hashtag/NeuralNetworks?src=hash&amp;ref_src=twsrc%5Etfw">#NeuralNetworks</a> have become increasingly popular<br><br>Hierarchical Graph Neural Networks aim to connect the dots between traditional Neural Networks &amp; GNN architectures as well as the network science approaches<a href="https://twitter.com/hashtag/EmergingTech?src=hash&amp;ref_src=twsrc%5Etfw">#EmergingTech</a> <a href="https://twitter.com/hashtag/AI?src=hash&amp;ref_src=twsrc%5Etfw">#AI</a><a href="https://t.co/S2quI2tpQi">https://t.co/S2quI2tpQi</a> <a href="https://t.co/eSvJ9XPkZU">pic.twitter.com/eSvJ9XPkZU</a></p>&mdash; The Year of the Graph (@TheYotg) <a href="https://twitter.com/TheYotg/status/1391715394155683841?ref_src=twsrc%5Etfw">May 10, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 10. Emergence in artificial life

Carlos Gershenson

- retweets: 44, favorites: 35 (05/11/2021 09:06:37)

- links: [abs](https://arxiv.org/abs/2105.03216) | [pdf](https://arxiv.org/pdf/2105.03216)
- [physics.gen-ph](https://arxiv.org/list/physics.gen-ph/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

Concepts similar to emergence have been used since antiquity, but we lack an agreed definition of emergence. Still, emergence has been identified as one of the features of complex systems. Most would agree on the statement "life is complex". Thus, understanding emergence and complexity should benefit the study of living systems. It can be said that life emerges from the interactions of complex molecules. But how useful is this to understand living systems? Artificial life (ALife) has been developed in recent decades to study life using a synthetic approach: build it to understand it. ALife systems are not so complex, be them soft (simulations), hard (robots), or wet (protocells). Then, we can aim at first understanding emergence in ALife, for then using this knowledge in biology. I argue that to understand emergence and life, it becomes useful to use information as a framework. In a general sense, emergence can be defined as information that is not present at one scale but is present at another scale. This perspective avoids problems of studying emergence from a materialistic framework, and can be useful to study self-organization and complexity.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Emergence in artificial life.<br>My latest preprint.<br>Emergence can be defined as information that is not present at one scale but is present at another scale.<br>I explore how can this help our understanding of (artificial) life.<a href="https://t.co/KQY08jkyJO">https://t.co/KQY08jkyJO</a></p>&mdash; Carlos Gershenson (@cgershen) <a href="https://twitter.com/cgershen/status/1391742592161828864?ref_src=twsrc%5Etfw">May 10, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 11. On-the-Fly Controlled Text Generation with Experts and Anti-Experts

Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, Yejin Choi

- retweets: 42, favorites: 25 (05/11/2021 09:06:38)

- links: [abs](https://arxiv.org/abs/2105.03023) | [pdf](https://arxiv.org/pdf/2105.03023)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

Despite recent advances in natural language generation, it remains challenging to control attributes of generated text. We propose DExperts: Decoding-time Experts, a decoding-time method for controlled text generation which combines a pretrained language model with experts and/or anti-experts in an ensemble of language models. Intuitively, under our ensemble, output tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts. We apply DExperts to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations. Our work highlights the promise of using LMs trained on text with (un)desired attributes for efficient decoding-time controlled language generation.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">On-the-Fly Controlled Text Generation with Experts and Anti-Experts<br>pdf: <a href="https://t.co/etzYuAzYo5">https://t.co/etzYuAzYo5</a><br>abs: <a href="https://t.co/0MonKgtPRm">https://t.co/0MonKgtPRm</a><br><br>a method of ensembling language models at decoding time to control an attribute of generated text <a href="https://t.co/nCjuHbVaCP">pic.twitter.com/nCjuHbVaCP</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1391556436438786051?ref_src=twsrc%5Etfw">May 10, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 12. A Dataset of Information-Seeking Questions and Answers Anchored in  Research Papers

Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, Matt Gardner

- retweets: 30, favorites: 36 (05/11/2021 09:06:38)

- links: [abs](https://arxiv.org/abs/2105.03011) | [pdf](https://arxiv.org/pdf/2105.03011)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present QASPER, a dataset of 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers<br>pdf: <a href="https://t.co/RsTV9UeHPq">https://t.co/RsTV9UeHPq</a><br>abs: <a href="https://t.co/6g2Xt9vmhW">https://t.co/6g2Xt9vmhW</a><br>project page: <a href="https://t.co/OYjXg8lCCW">https://t.co/OYjXg8lCCW</a><br><br>an information-seeking QA dataset over NLP research papers <a href="https://t.co/UUZ9MzNUdE">pic.twitter.com/UUZ9MzNUdE</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1391588122597765120?ref_src=twsrc%5Etfw">May 10, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 13. SpeechMoE: Scaling to Large Acoustic Models with Dynamic Routing Mixture  of Experts

Zhao You, Shulin Feng, Dan Su, Dong Yu

- retweets: 21, favorites: 43 (05/11/2021 09:06:38)

- links: [abs](https://arxiv.org/abs/2105.03036) | [pdf](https://arxiv.org/pdf/2105.03036)
- [cs.SD](https://arxiv.org/list/cs.SD/recent) | [cs.CL](https://arxiv.org/list/cs.CL/recent) | [eess.AS](https://arxiv.org/list/eess.AS/recent)

Recently, Mixture of Experts (MoE) based Transformer has shown promising results in many domains. This is largely due to the following advantages of this architecture: firstly, MoE based Transformer can increase model capacity without computational cost increasing both at training and inference time. Besides, MoE based Transformer is a dynamic network which can adapt to the varying complexity of input instances in realworld applications. In this work, we explore the MoE based model for speech recognition, named SpeechMoE. To further control the sparsity of router activation and improve the diversity of gate values, we propose a sparsity L1 loss and a mean importance loss respectively. In addition, a new router architecture is used in SpeechMoE which can simultaneously utilize the information from a shared embedding network and the hierarchical representation of different MoE layers. Experimental results show that SpeechMoE can achieve lower character error rate (CER) with comparable computation cost than traditional static networks, providing 7.0%-23.0% relative CER improvements on four evaluation datasets.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">SpeechMoE: Scaling to Large Acoustic Models with Dynamic Routing Mixture of Experts<br><br>Achieves lower character error rate (CER) w/ comparable computation cost than traditional networks, providing 7.0% ∼ 23.0% relative CER gains on four datasets.<a href="https://t.co/yHobg3c6xe">https://t.co/yHobg3c6xe</a> <a href="https://t.co/wrGVfVA4yA">pic.twitter.com/wrGVfVA4yA</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1391559225747251204?ref_src=twsrc%5Etfw">May 10, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



