---
title: Hot Papers 2021-03-23
date: 2021-03-24T09:20:13.Z
template: "post"
draft: false
slug: "hot-papers-2021-03-23"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2021-03-23"
socialImage: "/media/flying-marine.jpg"

---

# 1. MasakhaNER: Named Entity Recognition for African Languages

David Ifeoluwa Adelani, Jade Abbott, Graham Neubig, Daniel D'souza, Julia Kreutzer, Constantine Lignos, Chester Palen-Michel, Happy Buzaaba, Shruti Rijhwani, Sebastian Ruder, Stephen Mayhew, Israel Abebe Azime, Shamsuddeen Muhammad, Chris Chinenye Emezue, Joyce Nakatumba-Nabende, Perez Ogayo, Anuoluwapo Aremu, Catherine Gitau, Derguene Mbaye, Jesujoba Alabi, Seid Muhie Yimam, Tajuddeen Gwadabe, Ignatius Ezeani, Rubungo Andre Niyongabo, Jonathan Mukiibi, Verrah Otiende, Iroro Orife, Davis David, Samba Ngom, Tosin Adewumi, Paul Rayson, Mofetoluwa Adeyemi, Gerald Muriuki, Emmanuel Anebi, Chiamaka Chukwuneke, Nkiruka Odu, Eric Peter Wairagala, Samuel Oyerinde, Clemencia Siro, Tobius Saul Bateesa, Temilola Oloyede, Yvonne Wambui, Victor Akinode, Deborah Nabagereka, Maurice Katusiime, Ayodele Awokoya

- retweets: 12710, favorites: 13 (03/24/2021 09:20:13)

- links: [abs](https://arxiv.org/abs/2103.11811) | [pdf](https://arxiv.org/pdf/2103.11811)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

We take a step towards addressing the under-representation of the African continent in NLP research by creating the first large publicly available high-quality dataset for named entity recognition (NER) in ten African languages, bringing together a variety of stakeholders. We detail characteristics of the languages to help researchers understand the challenges that these languages pose for NER. We analyze our datasets and conduct an extensive empirical evaluation of state-of-the-art methods across both supervised and transfer learning settings. We release the data, code, and models in order to inspire future research on African NLP.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">We&#39;re  SO excited to present the first large publicly available high quality dataset for NER in 10 African languages, bringing together a variety of stakeholders: language speakers, dataset curators, NLP practitioners, and evaluation experts üíïüåçüí™<br> (1/n)<a href="https://t.co/oa5ZbRrMyT">https://t.co/oa5ZbRrMyT</a> <a href="https://t.co/gJ3OlY97sx">pic.twitter.com/gJ3OlY97sx</a></p>&mdash; Masakhane (@MasakhaneNLP) <a href="https://twitter.com/MasakhaneNLP/status/1374300893936504832?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Preliminary Analysis of Potential Harms in the Luca Tracing System

Theresa Stadler, Wouter Lueks, Katharina Kohls, Carmela Troncoso

- retweets: 8707, favorites: 9 (03/24/2021 09:20:13)

- links: [abs](https://arxiv.org/abs/2103.11958) | [pdf](https://arxiv.org/pdf/2103.11958)
- [cs.CR](https://arxiv.org/list/cs.CR/recent) | [cs.CY](https://arxiv.org/list/cs.CY/recent)

In this document, we analyse the potential harms a large-scale deployment of the Luca system might cause to individuals, venues, and communities. The Luca system is a digital presence tracing system designed to provide health departments with the contact information necessary to alert individuals who have visited a location at the same time as a SARS-CoV-2-positive person. Multiple regional health departments in Germany have announced their plans to deploy the Luca system for the purpose of presence tracing. The system's developers suggest its use across various types of venues: from bars and restaurants to public and private events, such religious or political gatherings, weddings, and birthday parties. Recently, an extension to include schools and other educational facilities was discussed in public. Our analysis of the potential harms of the system is based on the publicly available Luca Security Concept which describes the system's security architecture and its planned protection mechanisms. The Security Concept furthermore provides a set of claims about the system's security and privacy properties. Besides an analysis of harms, our analysis includes a validation of these claims.

<blockquote class="twitter-tweet"><p lang="de" dir="ltr">√úber die letzten Tage haben wir eine Analyse der m√∂glichen Nachteile, die ein breiter Einsatz der <a href="https://twitter.com/hashtag/LucaApp?src=hash&amp;ref_src=twsrc%5Etfw">#LucaApp</a> f√ºr Individuen, Gruppen und Veranstalter mit sich bringen k√∂nnte, verfasst <a href="https://t.co/Hlkg5Fn2S2">https://t.co/Hlkg5Fn2S2</a> mit  <a href="https://twitter.com/WouterLueks?ref_src=twsrc%5Etfw">@WouterLueks</a> <a href="https://twitter.com/blister_green?ref_src=twsrc%5Etfw">@blister_green</a> <a href="https://twitter.com/carmelatroncoso?ref_src=twsrc%5Etfw">@carmelatroncoso</a><br>Eine Zusammenfassung üëá</p>&mdash; T (@thsStadler) <a href="https://twitter.com/thsStadler/status/1374319093449375747?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets

Isaac Caswell, Julia Kreutzer, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Allahsera Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, Monang Setyawan, Supheakmungkol Sarin, Sokhar Samb, Beno√Æt Sagot, Clara Rivera, Annette Rios, Isabel Papadimitriou, Salomey Osei, Pedro Javier Ortiz Su√°rez, Iroro Orife, Kelechi Ogueji, Rubungo Andre Niyongabo, Toan Q. Nguyen, Mathias M√ºller, Andr√© M√ºller, Shamsuddeen Hassan Muhammad, Nanda Muhammad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov, Tapiwanashe Matangira, Colin Leong, Nze Lawson, Sneha Kudugunta, Yacine Jernite, Mathias Jenny, Orhan Firat, Bonaventure F. P. Dossou, Sakhile Dlamini, Nisansa de Silva, Sakine √áabuk Ballƒ±, Stella Biderman, Alessia Battisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar

- retweets: 4613, favorites: 125 (03/24/2021 09:20:13)

- links: [abs](https://arxiv.org/abs/2103.12028) | [pdf](https://arxiv.org/pdf/2103.12028)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

With the success of large-scale pre-training and multilingual modeling in Natural Language Processing (NLP), recent years have seen a proliferation of large, web-mined text datasets covering hundreds of languages. However, to date there has been no systematic analysis of the quality of these publicly available datasets, or whether the datasets actually contain content in the languages they claim to represent. In this work, we manually audit the quality of 205 language-specific corpora released with five major public datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4), and audit the correctness of language codes in a sixth (JW300). We find that lower-resource corpora have systematic issues: at least 15 corpora are completely erroneous, and a significant fraction contains less than 50% sentences of acceptable quality. Similarly, we find 82 corpora that are mislabeled or use nonstandard/ambiguous language codes. We demonstrate that these issues are easy to detect even for non-speakers of the languages in question, and supplement the human judgements with automatic analyses. Inspired by our analysis, we recommend techniques to evaluate and improve multilingual corpora and discuss the risks that come with low-quality data releases.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Does the data used for multilingual modeling really contain content in the languages it says it does? Short answer: sometimes üôÅ <a href="https://t.co/05NWjobkwO">https://t.co/05NWjobkwO</a> 1/n</p>&mdash; Isaac R Caswell (@iseeaswell) <a href="https://twitter.com/iseeaswell/status/1374379967060996104?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets<br><br>By manually auditing the quality of 205 language-specific corpora, they find that lower-resource corpora have systematic issues in quality.<a href="https://t.co/xUiwTCoZFd">https://t.co/xUiwTCoZFd</a> <a href="https://t.co/ag4kPsxwwP">pic.twitter.com/ag4kPsxwwP</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1374164252336951296?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Interpretable Machine Learning: Fundamental Principles and 10 Grand  Challenges

Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, Chudi Zhong

- retweets: 4164, favorites: 362 (03/24/2021 09:20:14)

- links: [abs](https://arxiv.org/abs/2103.11251) | [pdf](https://arxiv.org/pdf/2103.11251)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Interpretability in machine learning (ML) is crucial for high stakes decisions and troubleshooting. In this work, we provide fundamental principles for interpretable ML, and dispel common misunderstandings that dilute the importance of this crucial topic. We also identify 10 technical challenge areas in interpretable machine learning and provide history and background on each problem. Some of these problems are classically important, and some are recent problems that have arisen in the last few years. These problems are: (1) Optimizing sparse logical models such as decision trees; (2) Optimization of scoring systems; (3) Placing constraints into generalized additive models to encourage sparsity and better interpretability; (4) Modern case-based reasoning, including neural networks and matching for causal inference; (5) Complete supervised disentanglement of neural networks; (6) Complete or even partial unsupervised disentanglement of neural networks; (7) Dimensionality reduction for data visualization; (8) Machine learning models that can incorporate physics and other generative or causal constraints; (9) Characterization of the "Rashomon set" of good models; and (10) Interpretable reinforcement learning. This survey is suitable as a starting point for statisticians and computer scientists interested in working in interpretable machine learning.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">New review paper: &quot;Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges&quot;  <a href="https://t.co/dIUwjxcISx">https://t.co/dIUwjxcISx</a></p>&mdash; Cynthia Rudin (@CynthiaRudin) <a href="https://twitter.com/CynthiaRudin/status/1374167785337716737?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Measuring and modeling the motor system with machine learning

S√©bastien B. Hausmann, Alessandro Marin Vargas, Alexander Mathis, Mackenzie W. Mathis

- retweets: 3376, favorites: 220 (03/24/2021 09:20:14)

- links: [abs](https://arxiv.org/abs/2103.11775) | [pdf](https://arxiv.org/pdf/2103.11775)
- [q-bio.QM](https://arxiv.org/list/q-bio.QM/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

The utility of machine learning in understanding the motor system is promising a revolution in how to collect, measure, and analyze data. The field of movement science already elegantly incorporates theory and engineering principles to guide experimental work, and in this review we discuss the growing use of machine learning: from pose estimation, kinematic analyses, dimensionality reduction, and closed-loop feedback, to its use in understanding neural correlates and untangling sensorimotor systems. We also give our perspective on new avenues where markerless motion capture combined with biomechanical modeling and neural networks could be a new platform for hypothesis-driven research.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">ü¶æWant to tackle the motor system with machine learning? <br><br>üéì New *review* on machine learning approaches for behavior &amp;  sensorimotor modeling! <br><br>üîñWritten with fabulous co-1st <a href="https://twitter.com/EPFL_en?ref_src=twsrc%5Etfw">@EPFL_en</a>  PhD students <a href="https://twitter.com/SebHausmann?ref_src=twsrc%5Etfw">@SebHausmann</a> &amp; <a href="https://twitter.com/a_marinvargas?ref_src=twsrc%5Etfw">@a_marinvargas</a> + <a href="https://twitter.com/TrackingPlumes?ref_src=twsrc%5Etfw">@TrackingPlumes</a> &amp; me! <a href="https://t.co/pgVMov8xZa">https://t.co/pgVMov8xZa</a> <a href="https://t.co/ZftEvNDyMz">pic.twitter.com/ZftEvNDyMz</a></p>&mdash; Dr. Mackenzie Mathis (@TrackingActions) <a href="https://twitter.com/TrackingActions/status/1374415274779156484?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. Multimodal Motion Prediction with Stacked Transformers

Yicheng Liu, Jinghuai Zhang, Liangji Fang, Qinhong Jiang, Bolei Zhou

- retweets: 1190, favorites: 231 (03/24/2021 09:20:14)

- links: [abs](https://arxiv.org/abs/2103.11624) | [pdf](https://arxiv.org/pdf/2103.11624)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

Predicting multiple plausible future trajectories of the nearby vehicles is crucial for the safety of autonomous driving. Recent motion prediction approaches attempt to achieve such multimodal motion prediction by implicitly regularizing the feature or explicitly generating multiple candidate proposals. However, it remains challenging since the latent features may concentrate on the most frequent mode of the data while the proposal-based methods depend largely on the prior knowledge to generate and select the proposals. In this work, we propose a novel transformer framework for multimodal motion prediction, termed as mmTransformer. A novel network architecture based on stacked transformers is designed to model the multimodality at feature level with a set of fixed independent proposals. A region-based training strategy is then developed to induce the multimodality of the generated proposals. Experiments on Argoverse dataset show that the proposed model achieves the state-of-the-art performance on motion prediction, substantially improving the diversity and the accuracy of the predicted trajectories. Demo video and code are available at https://decisionforce.github.io/mmTransformer.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Multimodal Motion Prediction with Stacked Transformers<br>pdf: <a href="https://t.co/OPjoTObGBt">https://t.co/OPjoTObGBt</a><br>abs: <a href="https://t.co/G32niBfY6h">https://t.co/G32niBfY6h</a><br>project page: <a href="https://t.co/xmF1AyniZj">https://t.co/xmF1AyniZj</a> <a href="https://t.co/c4EJbz0aDV">pic.twitter.com/c4EJbz0aDV</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1374165941525827595?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. DeepViT: Towards Deeper Vision Transformer

Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Qibin Hou, Jiashi Feng

- retweets: 978, favorites: 213 (03/24/2021 09:20:14)

- links: [abs](https://arxiv.org/abs/2103.11886) | [pdf](https://arxiv.org/pdf/2103.11886)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Vision transformers (ViTs) have been successfully applied in image classification tasks recently. In this paper, we show that, unlike convolution neural networks (CNNs)that can be improved by stacking more convolutional layers, the performance of ViTs saturate fast when scaled to be deeper. More specifically, we empirically observe that such scaling difficulty is caused by the attention collapse issue: as the transformer goes deeper, the attention maps gradually become similar and even much the same after certain layers. In other words, the feature maps tend to be identical in the top layers of deep ViT models. This fact demonstrates that in deeper layers of ViTs, the self-attention mechanism fails to learn effective concepts for representation learning and hinders the model from getting expected performance gain. Based on above observation, we propose a simple yet effective method, named Re-attention, to re-generate the attention maps to increase their diversity at different layers with negligible computation and memory cost. The pro-posed method makes it feasible to train deeper ViT models with consistent performance improvements via minor modification to existing ViT models. Notably, when training a deep ViT model with 32 transformer blocks, the Top-1 classification accuracy can be improved by 1.6% on ImageNet. Code will be made publicly available

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">DeepViT: Towards Deeper Vision Transformer<br><br>Achieves up to 1.6%+ in top1 acc. on Imagenet by regenerating the attention maps to increase their diversity at different layers.<a href="https://t.co/NsvfB1VJu4">https://t.co/NsvfB1VJu4</a> <a href="https://t.co/xZiEHkeGTy">pic.twitter.com/xZiEHkeGTy</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1374169592768000001?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">DeepViT: Towards Deeper Vision Transformer<br>pdf: <a href="https://t.co/V84QXEelAV">https://t.co/V84QXEelAV</a><br>abs: <a href="https://t.co/PwIza18U7N">https://t.co/PwIza18U7N</a> <a href="https://t.co/mS9WNcyZzU">pic.twitter.com/mS9WNcyZzU</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1374168048857444360?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. Efficient Visual Pretraining with Contrastive Detection

Olivier J. H√©naff, Skanda Koppula, Jean-Baptiste Alayrac, Aaron van den Oord, Oriol Vinyals, Jo√£o Carreira

- retweets: 715, favorites: 140 (03/24/2021 09:20:15)

- links: [abs](https://arxiv.org/abs/2103.10957) | [pdf](https://arxiv.org/pdf/2103.10957)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Self-supervised pretraining has been shown to yield powerful representations for transfer learning. These performance gains come at a large computational cost however, with state-of-the-art methods requiring an order of magnitude more computation than supervised pretraining. We tackle this computational bottleneck by introducing a new self-supervised objective, contrastive detection, which tasks representations with identifying object-level features across augmentations. This objective extracts a rich learning signal per image, leading to state-of-the-art transfer performance from ImageNet to COCO, while requiring up to 5x less pretraining. In particular, our strongest ImageNet-pretrained model performs on par with SEER, one of the largest self-supervised systems to date, which uses 1000x more pretraining data. Finally, our objective seamlessly handles pretraining on more complex images such as those in COCO, closing the gap with supervised transfer learning from COCO to PASCAL.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Efficient Visual Pretraining with Contrastive Detection<br><br>With a new self-supervised objective, contrastive detection, DetCon performs on par with the SotA model (SEER) w/ 1000x less pretraining data on Imagenet. <a href="https://t.co/jnfkKEkMNd">https://t.co/jnfkKEkMNd</a> <a href="https://t.co/GqhQpySe6I">pic.twitter.com/GqhQpySe6I</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1374172411982090240?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Efficient Visual Pretraining with Contrastive Detection<br>pdf: <a href="https://t.co/8M9Go75XkU">https://t.co/8M9Go75XkU</a><br>abs: <a href="https://t.co/Z9r23oDPqZ">https://t.co/Z9r23oDPqZ</a> <a href="https://t.co/zvY2nZHaDh">pic.twitter.com/zvY2nZHaDh</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1374171112297349121?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. Improving and Simplifying Pattern Exploiting Training

Derek Tam, Rakesh R Menon, Mohit Bansal, Shashank Srivastava, Colin Raffel

- retweets: 615, favorites: 181 (03/24/2021 09:20:15)

- links: [abs](https://arxiv.org/abs/2103.11955) | [pdf](https://arxiv.org/pdf/2103.11955)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Recently, pre-trained language models (LMs) have achieved strong performance when fine-tuned on difficult benchmarks like SuperGLUE. However, performance can suffer when there are very few labeled examples available for fine-tuning. Pattern Exploiting Training (PET) is a recent approach that leverages patterns for few-shot learning. However, PET uses task-specific unlabeled data. In this paper, we focus on few shot learning without any unlabeled data and introduce ADAPET, which modifies PET's objective to provide denser supervision during fine-tuning. As a result, ADAPET outperforms PET on SuperGLUE without any task-specific unlabeled data. Our code can be found at https://github.com/rrmenon10/ADAPET.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">New preprint! We introduce a simplified version of pattern-exploiting training called ADAPET. ADAPET outperforms PET and iPET on SuperGLUE without using task-specific unlabeled data or ensembling and beats few-shot GPT-3 with a much smaller model.<a href="https://t.co/ukPvsI340g">https://t.co/ukPvsI340g</a> <a href="https://t.co/E3uTj2S9yx">pic.twitter.com/E3uTj2S9yx</a></p>&mdash; Colin Raffel (@colinraffel) <a href="https://twitter.com/colinraffel/status/1374390715094986765?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Improving and Simplifying Pattern Exploiting Training<br><br>ADAPET outperforms PET (Pattern Exploiting<br>Training) on SuperGLUE without any task-specific unlabeled data. <br><br>abs: <a href="https://t.co/6iLFZEonEK">https://t.co/6iLFZEonEK</a><br>code: <a href="https://t.co/NVIUNbNlyt">https://t.co/NVIUNbNlyt</a> <a href="https://t.co/X1BQdmLvje">pic.twitter.com/X1BQdmLvje</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1374165223515426816?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 10. Retrieve Fast, Rerank Smart: Cooperative and Joint Approaches for  Improved Cross-Modal Retrieval

Gregor Geigle, Jonas Pfeiffer, Nils Reimers, Ivan Vuliƒá, Iryna Gurevych

- retweets: 276, favorites: 111 (03/24/2021 09:20:15)

- links: [abs](https://arxiv.org/abs/2103.11920) | [pdf](https://arxiv.org/pdf/2103.11920)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.CL](https://arxiv.org/list/cs.CL/recent)

Current state-of-the-art approaches to cross-modal retrieval process text and visual input jointly, relying on Transformer-based architectures with cross-attention mechanisms that attend over all words and objects in an image. While offering unmatched retrieval performance, such models: 1) are typically pretrained from scratch and thus less scalable, 2) suffer from huge retrieval latency and inefficiency issues, which makes them impractical in realistic applications. To address these crucial gaps towards both improved and efficient cross-modal retrieval, we propose a novel fine-tuning framework which turns any pretrained text-image multi-modal model into an efficient retrieval model. The framework is based on a cooperative retrieve-and-rerank approach which combines: 1) twin networks to separately encode all items of a corpus, enabling efficient initial retrieval, and 2) a cross-encoder component for a more nuanced (i.e., smarter) ranking of the retrieved small set of items. We also propose to jointly fine-tune the two components with shared weights, yielding a more parameter-efficient model. Our experiments on a series of standard cross-modal retrieval benchmarks in monolingual, multilingual, and zero-shot setups, demonstrate improved accuracy and huge efficiency benefits over the state-of-the-art cross-encoders.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Check out our paper<br>‚ÄúRetrieve Fast, Rerank Smart: Cooperative and Joint Approaches for Improved Cross-Modal Retrieval‚Äú<br><br>An efficient retrieve and rerank approach for image search!<br><br>w/ <a href="https://twitter.com/GregorGeigle?ref_src=twsrc%5Etfw">@GregorGeigle</a> <a href="https://twitter.com/Nils_Reimers?ref_src=twsrc%5Etfw">@Nils_Reimers</a> <a href="https://twitter.com/licwu?ref_src=twsrc%5Etfw">@licwu</a> <br>Paper: <a href="https://t.co/HQhp8y750K">https://t.co/HQhp8y750K</a><br>Code: <a href="https://t.co/B43M1fkoFP">https://t.co/B43M1fkoFP</a> <a href="https://t.co/r9R9fdGgMT">pic.twitter.com/r9R9fdGgMT</a></p>&mdash; Jonas Pfeiffer (@PfeiffJo) <a href="https://twitter.com/PfeiffJo/status/1374253456400265216?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Retrieve Fast, Rerank Smart: Cooperative and Joint Approaches for Improved Cross-Modal Retrieval<br>pdf: <a href="https://t.co/QQhgFMXwU1">https://t.co/QQhgFMXwU1</a><br>abs: <a href="https://t.co/aRzf5C13fH">https://t.co/aRzf5C13fH</a><br>github: <a href="https://t.co/Bb6m5c9A3t">https://t.co/Bb6m5c9A3t</a> <a href="https://t.co/9yQoLSoloo">pic.twitter.com/9yQoLSoloo</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1374213385332609026?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 11. Meta-DETR: Few-Shot Object Detection via Unified Image-Level  Meta-Learning

Gongjie Zhang, Zhipeng Luo, Kaiwen Cui, Shijian Lu

- retweets: 289, favorites: 80 (03/24/2021 09:20:16)

- links: [abs](https://arxiv.org/abs/2103.11731) | [pdf](https://arxiv.org/pdf/2103.11731)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

Few-shot object detection aims at detecting novel objects with only a few annotated examples. Prior works have proved meta-learning a promising solution, and most of them essentially address detection by meta-learning over regions for their classification and location fine-tuning. However, these methods substantially rely on initially well-located region proposals, which are usually hard to obtain under the few-shot settings. This paper presents a novel meta-detector framework, namely Meta-DETR, which eliminates region-wise prediction and instead meta-learns object localization and classification at image level in a unified and complementary manner. Specifically, it first encodes both support and query images into category-specific features and then feeds them into a category-agnostic decoder to directly generate predictions for specific categories. To facilitate meta-learning with deep networks, we design a simple but effective Semantic Alignment Mechanism (SAM), which aligns high-level and low-level feature semantics to improve the generalization of meta-learned representations. Experiments over multiple few-shot object detection benchmarks show that Meta-DETR outperforms state-of-the-art methods by large margins.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Meta-DETR: Few-Shot Object Detection via Unified Image-Level Meta-Learning<br>pdf: <a href="https://t.co/iDoXts5AJV">https://t.co/iDoXts5AJV</a><br>abs: <a href="https://t.co/iPysNYg7wx">https://t.co/iPysNYg7wx</a> <a href="https://t.co/gQ498JTC4g">pic.twitter.com/gQ498JTC4g</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1374201156340228097?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 12. Incorporating Convolution Designs into Visual Transformers

Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, Wei Wu

- retweets: 226, favorites: 73 (03/24/2021 09:20:16)

- links: [abs](https://arxiv.org/abs/2103.11816) | [pdf](https://arxiv.org/pdf/2103.11816)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Motivated by the success of Transformers in natural language processing (NLP) tasks, there emerge some attempts (e.g., ViT and DeiT) to apply Transformers to the vision domain. However, pure Transformer architectures often require a large amount of training data or extra supervision to obtain comparable performance with convolutional neural networks (CNNs). To overcome these limitations, we analyze the potential drawbacks when directly borrowing Transformer architectures from NLP. Then we propose a new \textbf{Convolution-enhanced image Transformer (CeiT)} which combines the advantages of CNNs in extracting low-level features, strengthening locality, and the advantages of Transformers in establishing long-range dependencies. Three modifications are made to the original Transformer: \textbf{1)} instead of the straightforward tokenization from raw input images, we design an \textbf{Image-to-Tokens (I2T)} module that extracts patches from generated low-level features; \textbf{2)} the feed-froward network in each encoder block is replaced with a \textbf{Locally-enhanced Feed-Forward (LeFF)} layer that promotes the correlation among neighboring tokens in the spatial dimension; \textbf{3)} a \textbf{Layer-wise Class token Attention (LCA)} is attached at the top of the Transformer that utilizes the multi-level representations.   Experimental results on ImageNet and seven downstream tasks show the effectiveness and generalization ability of CeiT compared with previous Transformers and state-of-the-art CNNs, without requiring a large amount of training data and extra CNN teachers. Besides, CeiT models also demonstrate better convergence with $3\times$ fewer training iterations, which can reduce the training cost significantly\footnote{Code and models will be released upon acceptance.}.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Incorporating Convolution Designs into Visual Transformers<br><br>CeiT matches DeiT with 3x fewer iterations by adding three modifications to the architecture.<a href="https://t.co/sXDsuIfPKS">https://t.co/sXDsuIfPKS</a> <a href="https://t.co/t1SJOyZcm9">pic.twitter.com/t1SJOyZcm9</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1374168909541044224?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Incorporating Convolution Designs into Visual Transformers<br>pdf: <a href="https://t.co/KJUo5L9mxj">https://t.co/KJUo5L9mxj</a><br>abs: <a href="https://t.co/WNDbVcCFdV">https://t.co/WNDbVcCFdV</a> <a href="https://t.co/X4vTrYwxn2">pic.twitter.com/X4vTrYwxn2</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1374167752102002692?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 13. Conceptual similarity and communicative need shape colexification: an  experimental study

Andres Karjus, Richard A. Blythe, Simon Kirby, Tianyu Wang, Kenny Smith

- retweets: 240, favorites: 31 (03/24/2021 09:20:16)

- links: [abs](https://arxiv.org/abs/2103.11024) | [pdf](https://arxiv.org/pdf/2103.11024)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

Colexification refers to the phenomenon of multiple meanings sharing one word in a language. Cross-linguistic lexification patterns have been shown to be largely predictable, as similar concepts are often colexified. We test a recent claim that, beyond this general tendency, communicative needs play an important role in shaping colexification patterns. We approach this question by means of a series of human experiments, using an artificial language communication game paradigm. Our results across four experiments match the previous cross-linguistic findings: all other things being equal, speakers do prefer to colexify similar concepts. However, we also find evidence supporting the communicative need hypothesis: when faced with a frequent need to distinguish similar pairs of meanings, speakers adjust their colexification preferences to maintain communicative efficiency, and avoid colexifying those similar meanings which need to be distinguished in communication. This research provides further evidence to support the argument that languages are shaped by the needs and preferences of their speakers.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Preprint &quot;Conceptual similarity and communicative need shape colexification&quot; w/ <a href="https://twitter.com/DrAlgernon?ref_src=twsrc%5Etfw">@DrAlgernon</a> <a href="https://twitter.com/SimonKirby?ref_src=twsrc%5Etfw">@SimonKirby</a> Tianyu Wang <a href="https://twitter.com/kennysmithed?ref_src=twsrc%5Etfw">@kennysmithed</a>: <a href="https://t.co/ZpF4dCWMxo">https://t.co/ZpF4dCWMxo</a>. We carry out 4 artificial language experiments (incl a self-repl) to test 2 hypotheses from a crosslinguistic study  1/5 <a href="https://t.co/ejJc7y9eyL">pic.twitter.com/ejJc7y9eyL</a></p>&mdash; Andres Karjus (@AndresKarjus) <a href="https://twitter.com/AndresKarjus/status/1374332704662167557?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 14. Which contributions count? Analysis of attribution in open source

Jean-Gabriel Young, Amanda Casari, Katie McLaughlin, Milo Z. Trujillo, Laurent H√©bert-Dufresne, James P. Bagrow

- retweets: 240, favorites: 26 (03/24/2021 09:20:16)

- links: [abs](https://arxiv.org/abs/2103.11007) | [pdf](https://arxiv.org/pdf/2103.11007)
- [cs.SE](https://arxiv.org/list/cs.SE/recent) | [cs.CY](https://arxiv.org/list/cs.CY/recent)

Open source software projects usually acknowledge contributions with text files, websites, and other idiosyncratic methods. These data sources are hard to mine, which is why contributorship is most frequently measured through changes to repositories, such as commits, pushes, or patches. Recently, some open source projects have taken to recording contributor actions with standardized systems; this opens up a unique opportunity to understand how community-generated notions of contributorship map onto codebases as the measure of contribution. Here, we characterize contributor acknowledgment models in open source by analyzing thousands of projects that use a model called All Contributors to acknowledge diverse contributions like outreach, finance, infrastructure, and community management. We analyze the life cycle of projects through this model's lens and contrast its representation of contributorship with the picture given by other methods of acknowledgment, including GitHub's top committers indicator and contributions derived from actions taken on the platform. We find that community-generated systems of contribution acknowledgment make work like idea generation or bug finding more visible, which generates a more extensive picture of collaboration. Further, we find that models requiring explicit attribution lead to more clearly defined boundaries around what is and what is not a contribution.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">&quot;Which contributions count? Analysis of attribution in open source‚Äù<br><br>New preprint from faculty members <a href="https://twitter.com/_jgyou?ref_src=twsrc%5Etfw">@_jgyou</a> <a href="https://twitter.com/LHDnets?ref_src=twsrc%5Etfw">@LHDnets</a> <a href="https://twitter.com/bagrow?ref_src=twsrc%5Etfw">@bagrow</a> w/PhD student <a href="https://twitter.com/illegaldaydream?ref_src=twsrc%5Etfw">@illegaldaydream</a> &amp; Google Open sourcerers <a href="https://twitter.com/amcasari?ref_src=twsrc%5Etfw">@amcasari</a> <a href="https://twitter.com/glasnt?ref_src=twsrc%5Etfw">@glasnt</a> <a href="https://t.co/ZbPjbU2kct">https://t.co/ZbPjbU2kct</a> <a href="https://t.co/7TX804DsA4">pic.twitter.com/7TX804DsA4</a></p>&mdash; Vermont Complex Systems Center @ UVM (@uvmcomplexity) <a href="https://twitter.com/uvmcomplexity/status/1374166946216837125?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 15. MoViNets: Mobile Video Networks for Efficient Video Recognition

Dan Kondratyuk, Liangzhe Yuan, Yandong Li, Li Zhang, Mingxing Tan, Matthew Brown, Boqing Gong

- retweets: 174, favorites: 86 (03/24/2021 09:20:16)

- links: [abs](https://arxiv.org/abs/2103.11511) | [pdf](https://arxiv.org/pdf/2103.11511)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

We present Mobile Video Networks (MoViNets), a family of computation and memory efficient video networks that can operate on streaming video for online inference. 3D convolutional neural networks (CNNs) are accurate at video recognition but require large computation and memory budgets and do not support online inference, making them difficult to work on mobile devices. We propose a three-step approach to improve computational efficiency while substantially reducing the peak memory usage of 3D CNNs. First, we design a video network search space and employ neural architecture search to generate efficient and diverse 3D CNN architectures. Second, we introduce the Stream Buffer technique that decouples memory from video clip duration, allowing 3D CNNs to embed arbitrary-length streaming video sequences for both training and inference with a small constant memory footprint. Third, we propose a simple ensembling technique to improve accuracy further without sacrificing efficiency. These three progressive techniques allow MoViNets to achieve state-of-the-art accuracy and efficiency on the Kinetics, Moments in Time, and Charades video action recognition datasets. For instance, MoViNet-A5-Stream achieves the same accuracy as X3D-XL on Kinetics 600 while requiring 80% fewer FLOPs and 65% less memory. Code will be made available at https://github.com/tensorflow/models/tree/master/official/vision.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">MoViNets: Mobile Video Networks for Efficient Video Recognition<br><br>Using simple techniques, we produce efficient video classifiers for mobile devices, reducing peak memory usage by 10x, and can operate on streaming video.<br><br>Paper: <a href="https://t.co/G3oZlRtNZb">https://t.co/G3oZlRtNZb</a> <a href="https://t.co/RE7iReM0rh">pic.twitter.com/RE7iReM0rh</a></p>&mdash; Dan Kondratyuk (@hyperparticle) <a href="https://twitter.com/hyperparticle/status/1374179285896884231?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">MoViNets: Mobile Video Networks for Efficient Video Recognition<br>pdf: <a href="https://t.co/FQ6k72HaHx">https://t.co/FQ6k72HaHx</a><br>abs: <a href="https://t.co/mCe5POAZdR">https://t.co/mCe5POAZdR</a><br>github: <a href="https://t.co/1NOFncyrEn">https://t.co/1NOFncyrEn</a> <a href="https://t.co/fMPu5bEqim">pic.twitter.com/fMPu5bEqim</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1374209013710262275?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 16. Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual  Tracking

Ning Wang, Wengang Zhou, Jie Wang, Houqaing Li

- retweets: 100, favorites: 75 (03/24/2021 09:20:17)

- links: [abs](https://arxiv.org/abs/2103.11681) | [pdf](https://arxiv.org/pdf/2103.11681)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

In video object tracking, there exist rich temporal contexts among successive frames, which have been largely overlooked in existing trackers. In this work, we bridge the individual video frames and explore the temporal contexts across them via a transformer architecture for robust object tracking. Different from classic usage of the transformer in natural language processing tasks, we separate its encoder and decoder into two parallel branches and carefully design them within the Siamese-like tracking pipelines. The transformer encoder promotes the target templates via attention-based feature reinforcement, which benefits the high-quality tracking model generation. The transformer decoder propagates the tracking cues from previous templates to the current frame, which facilitates the object searching process. Our transformer-assisted tracking framework is neat and trained in an end-to-end manner. With the proposed transformer, a simple Siamese matching approach is able to outperform the current top-performing trackers. By combining our transformer with the recent discriminative tracking pipeline, our method sets several new state-of-the-art records on prevalent tracking benchmarks.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking<br>pdf: <a href="https://t.co/TJ33ds2qwf">https://t.co/TJ33ds2qwf</a><br>abs: <a href="https://t.co/rKm5DtvOJ7">https://t.co/rKm5DtvOJ7</a><br>github: <a href="https://t.co/Vczrdcxv7O">https://t.co/Vczrdcxv7O</a> <a href="https://t.co/ZmLhLkUp6c">pic.twitter.com/ZmLhLkUp6c</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1374167328603131907?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 17. Fairness Perceptions of Algorithmic Decision-Making: A Systematic Review  of the Empirical Literature

Christopher Starke, Janine Baleis, Birte Keller, Frank Marcinkowski

- retweets: 123, favorites: 44 (03/24/2021 09:20:17)

- links: [abs](https://arxiv.org/abs/2103.12016) | [pdf](https://arxiv.org/pdf/2103.12016)
- [cs.HC](https://arxiv.org/list/cs.HC/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.CY](https://arxiv.org/list/cs.CY/recent)

Algorithmic decision-making (ADM) increasingly shapes people's daily lives. Given that such autonomous systems can cause severe harm to individuals and social groups, fairness concerns have arisen. A human-centric approach demanded by scholars and policymakers requires taking people's fairness perceptions into account when designing and implementing ADM. We provide a comprehensive, systematic literature review synthesizing the existing empirical insights on perceptions of algorithmic fairness from 39 empirical studies spanning multiple domains and scientific disciplines. Through thorough coding, we systemize the current empirical literature along four dimensions: (a) algorithmic predictors, (b) human predictors, (c) comparative effects (human decision-making vs. algorithmic decision-making), and (d) consequences of ADM. While we identify much heterogeneity around the theoretical concepts and empirical measurements of algorithmic fairness, the insights come almost exclusively from Western-democratic contexts. By advocating for more interdisciplinary research adopting a society-in-the-loop framework, we hope our work will contribute to fairer and more responsible ADM.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">üö®New Pre-Printüö®<br><br>We conducted a systematic literature review of 39 empirical studies on people‚Äôs fairness perceptions of algorithmic decision-making.<a href="https://t.co/qcYtFyLSmc">https://t.co/qcYtFyLSmc</a><br><br>Thanks to my co-authors <a href="https://twitter.com/birtekeller?ref_src=twsrc%5Etfw">@birtekeller</a> <a href="https://twitter.com/JanineBls?ref_src=twsrc%5Etfw">@JanineBls</a> F. Marcinkowski<br><br>Main insights üëá <a href="https://t.co/yTaLnGC4na">pic.twitter.com/yTaLnGC4na</a></p>&mdash; Christopher Starke (@ch_starke) <a href="https://twitter.com/ch_starke/status/1374243124516634627?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 18. Higher-order Homophily is Combinatorially Impossible

Nate Veldt, Austin R. Benson, Jon Kleinberg

- retweets: 72, favorites: 57 (03/24/2021 09:20:17)

- links: [abs](https://arxiv.org/abs/2103.11818) | [pdf](https://arxiv.org/pdf/2103.11818)
- [cs.SI](https://arxiv.org/list/cs.SI/recent) | [cs.DM](https://arxiv.org/list/cs.DM/recent)

Homophily is the seemingly ubiquitous tendency for people to connect with similar others, which is fundamental to how society organizes. Even though many social interactions occur in groups, homophily has traditionally been measured from collections of pairwise interactions involving just two individuals. Here, we develop a framework using hypergraphs to quantify homophily from multiway, group interactions. This framework reveals that many homophilous group preferences are impossible; for instance, men and women cannot simultaneously exhibit preferences for groups where their gender is the majority. This is not a human behavior but rather a combinatorial impossibility of hypergraphs. At the same time, our framework reveals relaxed notions of group homophily that appear in numerous contexts. For example, in order for US members of congress to exhibit high preferences for co-sponsoring bills with their own political party, there must also exist a substantial number of individuals from each party that are willing to co-sponsor bills even when their party is in the minority. Our framework also reveals how gender distribution in group pictures varies with group size, a fact that is overlooked when applying graph-based measures.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">(1/n) New preprint with <a href="https://twitter.com/austinbenson?ref_src=twsrc%5Etfw">@austinbenson</a> and Jon Kleinberg! &quot;Higher-order Homophily is Combinatorially Impossible&quot; (<a href="https://t.co/5oFRQuGPgT">https://t.co/5oFRQuGPgT</a>)</p>&mdash; Nate Veldt (@n_veldt) <a href="https://twitter.com/n_veldt/status/1374432796559609859?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 19. AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis

Yudong Guo, Keyu Chen, Sen Liang, Yongjin Liu, Hujun Bao, Juyong Zhang

- retweets: 42, favorites: 76 (03/24/2021 09:20:17)

- links: [abs](https://arxiv.org/abs/2103.11078) | [pdf](https://arxiv.org/pdf/2103.11078)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Generating high-fidelity talking head video by fitting with the input audio sequence is a challenging problem that receives considerable attentions recently. In this paper, we address this problem with the aid of neural scene representation networks. Our method is completely different from existing methods that rely on intermediate representations like 2D landmarks or 3D face models to bridge the gap between audio input and video output. Specifically, the feature of input audio signal is directly fed into a conditional implicit function to generate a dynamic neural radiance field, from which a high-fidelity talking-head video corresponding to the audio signal is synthesized using volume rendering. Another advantage of our framework is that not only the head (with hair) region is synthesized as previous methods did, but also the upper body is generated via two individual neural radiance fields. Experimental results demonstrate that our novel framework can (1) produce high-fidelity and natural results, and (2) support free adjustment of audio signals, viewing directions, and background images.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis<br>pdf: <a href="https://t.co/Yo6ceVTFGQ">https://t.co/Yo6ceVTFGQ</a><br>abs: <a href="https://t.co/JOnW9Ywua1">https://t.co/JOnW9Ywua1</a> <a href="https://t.co/SN3TmhLFka">pic.twitter.com/SN3TmhLFka</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1374169553224278017?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 20. Open Domain Question Answering over Tables via Dense Retrieval

Jonathan Herzig, Thomas M√ºller, Syrine Krichene, Julian Martin Eisenschlos

- retweets: 64, favorites: 31 (03/24/2021 09:20:17)

- links: [abs](https://arxiv.org/abs/2103.12011) | [pdf](https://arxiv.org/pdf/2103.12011)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

Recent advances in open-domain QA have led to strong models based on dense retrieval, but only focused on retrieving textual passages. In this work, we tackle open-domain QA over tables for the first time, and show that retrieval can be improved by a retriever designed to handle tabular context. We present an effective pre-training procedure for our retriever and improve retrieval quality with mined hard negatives. As relevant datasets are missing, we extract a subset of Natural Questions (Kwiatkowski et al., 2019) into a Table QA dataset. We find that our retriever improves retrieval results from 72.0 to 81.1 recall@10 and end-to-end QA results from 33.8 to 37.7 exact match, over a BERT based retriever.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">1/4 Much focus is given to dense retrieval of textual passages, but how should we design retrievers for tables in the context of open domain QA?<br><br>New <a href="https://twitter.com/hashtag/NAACL2021?src=hash&amp;ref_src=twsrc%5Etfw">#NAACL2021</a> short paper: <a href="https://t.co/q67xNSOwxq">https://t.co/q67xNSOwxq</a><br><br>With <a href="https://twitter.com/muelletm?ref_src=twsrc%5Etfw">@muelletm</a>, Syrine Krichene and <a href="https://twitter.com/eisenjulian?ref_src=twsrc%5Etfw">@eisenjulian</a></p>&mdash; Jonathan Herzig (@jonherzig) <a href="https://twitter.com/jonherzig/status/1374366178966142980?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 21. Language Models have a Moral Dimension

Patrick Schramowski, Cigdem Turan, Nico Andersen, Constantin Rothkopf, Kristian Kersting

- retweets: 30, favorites: 60 (03/24/2021 09:20:17)

- links: [abs](https://arxiv.org/abs/2103.11790) | [pdf](https://arxiv.org/pdf/2103.11790)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.CY](https://arxiv.org/list/cs.CY/recent)

Artificial writing is permeating our lives due to recent advances in large-scale, transformer-based language models (LMs) such as BERT, its variants, GPT-2/3, and others. Using them as pretrained models and fine-tuning them for specific tasks, researchers have extended the state of the art for many NLP tasks and shown that they not only capture linguistic knowledge but also retain general knowledge implicitly present in the data. These and other successes are exciting. Unfortunately, LMs trained on unfiltered text corpora suffer from degenerate and biased behaviour. While this is well established, we show that recent improvements of LMs also store ethical and moral values of the society and actually bring a ``moral dimension'' to surface: the values are capture geometrically by a direction in the embedding space, reflecting well the agreement of phrases to social norms implicitly expressed in the training texts. This provides a path for attenuating or even preventing toxic degeneration in LMs. Since one can now rate the (non-)normativity of arbitrary phrases without explicitly training the LM for this task, the moral dimension can be used as ``moral compass'' guiding (even other) LMs towards producing normative text, as we will show.

<blockquote class="twitter-tweet"><p lang="ca" dir="ltr">Language Models have a Moral Dimension<br>pdf: <a href="https://t.co/QxnEQs4206">https://t.co/QxnEQs4206</a><br>abs: <a href="https://t.co/tCM0qvIIeC">https://t.co/tCM0qvIIeC</a> <a href="https://t.co/taPdzacN88">pic.twitter.com/taPdzacN88</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1374164201309102080?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 22. Functional Pearl: Witness Me -- Constructive Arguments Must Be Guided  with Concrete Witness

Hiromi Ishii

- retweets: 30, favorites: 39 (03/24/2021 09:20:17)

- links: [abs](https://arxiv.org/abs/2103.11751) | [pdf](https://arxiv.org/pdf/2103.11751)
- [cs.PL](https://arxiv.org/list/cs.PL/recent)

Beloved Curry--Howard correspondence tells that types are intuitionistic propositions, and in constructive math, a proof of proposition can be seen as some kind of a construction, or witness, conveying the information of the proposition. We demonstrate how useful this point of view is as the guiding principle for developing dependently-typed programs.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">[2103.11751] Functional Pearl: Witness Me -- Constructive Arguments Must Be Guided with Concrete Witness<br>ÊüêÊâÄ„Å´ÊäïÁ®ø„Åó„Åü„ÄÅÁ§æ„Åß„ÅÆ‰æùÂ≠òÂûãHaskell„ÅÆÊîª„ÇÅ„ÅüÂèñ„ÇäÁµÑ„Åø„ÅÆ„Ç®„ÉÉ„Çª„É≥„Çπ„ÇíË©∞„ÇÅËæº„Çì„Å†Ë´ñÊñá„ÇíarXiv„Å´‰∏ä„Åí„Åæ„Åó„Åü„ÄÇ„ÅîÁ¨ëË¶ß‰∏ã„Åï„ÅÑ„ÄÇ <a href="https://t.co/ATUZTLNwnn">https://t.co/ATUZTLNwnn</a></p>&mdash; „Çπ„Éû„Éº„Éà„Ç≥„É≥ (@mr_konn) <a href="https://twitter.com/mr_konn/status/1374165217274257416?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 23. Catastrophic Forgetting in Deep Graph Networks: an Introductory  Benchmark for Graph Classification

Antonio Carta, Andrea Cossu, Federico Errica, Davide Bacciu

- retweets: 25, favorites: 26 (03/24/2021 09:20:18)

- links: [abs](https://arxiv.org/abs/2103.11750) | [pdf](https://arxiv.org/pdf/2103.11750)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

In this work, we study the phenomenon of catastrophic forgetting in the graph representation learning scenario. The primary objective of the analysis is to understand whether classical continual learning techniques for flat and sequential data have a tangible impact on performances when applied to graph data. To do so, we experiment with a structure-agnostic model and a deep graph network in a robust and controlled environment on three different datasets. The benchmark is complemented by an investigation on the effect of structure-preserving regularization techniques on catastrophic forgetting. We find that replay is the most effective strategy in so far, which also benefits the most from the use of regularization. Our findings suggest interesting future research at the intersection of the continual and graph representation learning fields. Finally, we provide researchers with a flexible software framework to reproduce our results and carry out further experiments.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Really excited for this work! We perform a preliminary study on catastrophic forgetting for DGNs (deep graph nets). See you at WWW &#39;21 GLB Workshop!<br>Joint work with <a href="https://twitter.com/Cossu94?ref_src=twsrc%5Etfw">@Cossu94</a> <a href="https://twitter.com/acarta7?ref_src=twsrc%5Etfw">@acarta7</a> and Davide Bacciu.  <a href="https://t.co/sjz325bkX9">https://t.co/sjz325bkX9</a></p>&mdash; Federico Errica (@federico_errica) <a href="https://twitter.com/federico_errica/status/1374257869038698497?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



