---
title: Hot Papers 2021-02-11
date: 2021-02-12T09:39:48.Z
template: "post"
draft: false
slug: "hot-papers-2021-02-11"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2021-02-11"
socialImage: "/media/flying-marine.jpg"

---

# 1. Training Vision Transformers for Image Retrieval

Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, Hervé Jégou

- retweets: 1317, favorites: 401 (02/12/2021 09:39:48)

- links: [abs](https://arxiv.org/abs/2102.05644) | [pdf](https://arxiv.org/pdf/2102.05644)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Transformers have shown outstanding results for natural language understanding and, more recently, for image classification. We here extend this work and propose a transformer-based approach for image retrieval: we adopt vision transformers for generating image descriptors and train the resulting model with a metric learning objective, which combines a contrastive loss with a differential entropy regularizer. Our results show consistent and significant improvements of transformers over convolution-based approaches. In particular, our method outperforms the state of the art on several public benchmarks for category-level retrieval, namely Stanford Online Product, In-Shop and CUB-200. Furthermore, our experiments on ROxford and RParis also show that, in comparable settings, transformers are competitive for particular object retrieval, especially in the regime of short vector representations and low-resolution images.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Training Vision Transformers for Image Retrieval<br>pdf: <a href="https://t.co/Zp9dXHcvMT">https://t.co/Zp9dXHcvMT</a><br>abs: <a href="https://t.co/axwTP8u61v">https://t.co/axwTP8u61v</a> <a href="https://t.co/DJe11NTxhv">pic.twitter.com/DJe11NTxhv</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1359703826417262594?ref_src=twsrc%5Etfw">February 11, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">画像をクエリとし画像を検索する（もしくは画像間の類似度を評価する）タスクにおいて、Vision Transformerを使った手法がCNNを使ったSOTAsに精度面で大きく凌駕。CLSの埋め込みを符号で利用、エントロピー（最近傍距離の対数）を最大化する正則化付の対比損失で学習。<a href="https://t.co/7SgM3gRR1t">https://t.co/7SgM3gRR1t</a></p>&mdash; Daisuke Okanohara (@hillbig) <a href="https://twitter.com/hillbig/status/1359999588631932933?ref_src=twsrc%5Etfw">February 11, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Training Vision Transformers for Image Retrieval<a href="https://t.co/7Q8xcZxB6i">https://t.co/7Q8xcZxB6i</a> <a href="https://t.co/Niz6xGiusL">pic.twitter.com/Niz6xGiusL</a></p>&mdash; phalanx (@ZFPhalanx) <a href="https://twitter.com/ZFPhalanx/status/1359691247191904258?ref_src=twsrc%5Etfw">February 11, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">UPDATE: <a href="https://twitter.com/hashtag/ComputerVision?src=hash&amp;ref_src=twsrc%5Etfw">#ComputerVision</a> Transformers<br> <br>✅ Image retrieval <a href="https://t.co/5fmoI1t5BW">https://t.co/5fmoI1t5BW</a><br>✅ Video <a href="https://t.co/Pl2Isp8B9V">https://t.co/Pl2Isp8B9V</a><a href="https://twitter.com/hashtag/YearOfTheTransformer?src=hash&amp;ref_src=twsrc%5Etfw">#YearOfTheTransformer</a></p>&mdash; Kosta Derpanis (@CSProfKGD) <a href="https://twitter.com/CSProfKGD/status/1359695811001671685?ref_src=twsrc%5Etfw">February 11, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Is Space-Time Attention All You Need for Video Understanding?

Gedas Bertasius, Heng Wang, Lorenzo Torresani

- retweets: 582, favorites: 213 (02/12/2021 09:39:48)

- links: [abs](https://arxiv.org/abs/2102.05095) | [pdf](https://arxiv.org/pdf/2102.05095)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

We present a convolution-free approach to video classification built exclusively on self-attention over space and time. Our method, named "TimeSformer," adapts the standard Transformer architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches. Our experimental study compares different self-attention schemes and suggests that "divided attention," where temporal attention and spatial attention are separately applied within each block, leads to the best video classification accuracy among the design choices considered. Despite the radically different design compared to the prominent paradigm of 3D convolutional architectures for video, TimeSformer achieves state-of-the-art results on several major action recognition benchmarks, including the best reported accuracy on Kinetics-400 and Kinetics-600. Furthermore, our model is faster to train and has higher test-time efficiency compared to competing architectures. Code and pretrained models will be made publicly available.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Is Space-Time Attention All You Need for Video Understanding?<br>pdf: <a href="https://t.co/9GOGsQiCgP">https://t.co/9GOGsQiCgP</a><br>abs: <a href="https://t.co/x29qlViXnm">https://t.co/x29qlViXnm</a> <a href="https://t.co/NeYcHAgm4y">pic.twitter.com/NeYcHAgm4y</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1359696923071025153?ref_src=twsrc%5Etfw">February 11, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">UPDATE: <a href="https://twitter.com/hashtag/ComputerVision?src=hash&amp;ref_src=twsrc%5Etfw">#ComputerVision</a> Transformers<br> <br>✅ Image retrieval <a href="https://t.co/5fmoI1t5BW">https://t.co/5fmoI1t5BW</a><br>✅ Video <a href="https://t.co/Pl2Isp8B9V">https://t.co/Pl2Isp8B9V</a><a href="https://twitter.com/hashtag/YearOfTheTransformer?src=hash&amp;ref_src=twsrc%5Etfw">#YearOfTheTransformer</a></p>&mdash; Kosta Derpanis (@CSProfKGD) <a href="https://twitter.com/CSProfKGD/status/1359695811001671685?ref_src=twsrc%5Etfw">February 11, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. NAST: Non-Autoregressive Spatial-Temporal Transformer for Time Series  Forecasting

Kai Chen, Guang Chen, Dan Xu, Lijun Zhang, Yuyao Huang, Alois Knoll

- retweets: 336, favorites: 97 (02/12/2021 09:39:49)

- links: [abs](https://arxiv.org/abs/2102.05624) | [pdf](https://arxiv.org/pdf/2102.05624)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Although Transformer has made breakthrough success in widespread domains especially in Natural Language Processing (NLP), applying it to time series forecasting is still a great challenge. In time series forecasting, the autoregressive decoding of canonical Transformer models could introduce huge accumulative errors inevitably. Besides, utilizing Transformer to deal with spatial-temporal dependencies in the problem still faces tough difficulties.~To tackle these limitations, this work is the first attempt to propose a Non-Autoregressive Transformer architecture for time series forecasting, aiming at overcoming the time delay and accumulative error issues in the canonical Transformer. Moreover, we present a novel spatial-temporal attention mechanism, building a bridge by a learned temporal influence map to fill the gaps between the spatial and temporal attention, so that spatial and temporal dependencies can be processed integrally. Empirically, we evaluate our model on diversified ego-centric future localization datasets and demonstrate state-of-the-art performance on both real-time and accuracy.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">NAST: Non-Autoregressive Spatial-Temporal Transformer<br>for Time Series Forecasting<br>pdf: <a href="https://t.co/DIJS3cWlRp">https://t.co/DIJS3cWlRp</a><br>abs: <a href="https://t.co/R8jauAdLEO">https://t.co/R8jauAdLEO</a> <a href="https://t.co/jJvhu3ulAJ">pic.twitter.com/jJvhu3ulAJ</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1359728220539650048?ref_src=twsrc%5Etfw">February 11, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



