---
title: Hot Papers 2020-07-28
date: 2020-07-29T08:10:30.Z
template: "post"
draft: false
slug: "hot-papers-2020-07-28"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-07-28"
socialImage: "/media/flying-marine.jpg"

---

# 1. Storywrangler: A massive exploratorium for sociolinguistic, cultural,  socioeconomic, and political timelines using Twitter

Thayer Alshaabi, Jane L. Adams, Michael V. Arnold, Joshua R. Minot, David R. Dewhurst, Andrew J. Reagan, Christopher M. Danforth, Peter Sheridan Dodds

- retweets: 2167, favorites: 3152 (07/29/2020 08:10:30)

- links: [abs](https://arxiv.org/abs/2007.12988) | [pdf](https://arxiv.org/pdf/2007.12988)
- [cs.SI](https://arxiv.org/list/cs.SI/recent) | [cs.CL](https://arxiv.org/list/cs.CL/recent) | [physics.soc-ph](https://arxiv.org/list/physics.soc-ph/recent)

In real-time, Twitter strongly imprints world events, popular culture, and the day-to-day; Twitter records an ever growing compendium of language use and change; and Twitter has been shown to enable certain kinds of prediction. Vitally, and absent from many standard corpora such as books and news archives, Twitter also encodes popularity and spreading through retweets. Here, we describe Storywrangler, an ongoing, day-scale curation of over 100 billion tweets containing around 1 trillion 1-grams from 2008 to 2020. For each day, we break tweets into 1-, 2-, and 3-grams across 150+ languages, record usage frequencies, and generate Zipf distributions. We make the data set available through an interactive time series viewer, and as downloadable time series and daily distributions. We showcase a few examples of the many possible avenues of study we aim to enable including how social amplification can be visualized through 'contagiograms'.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">ã¡ã‚ƒã‚“ã¿ãª<a href="https://twitter.com/chanmina1014?ref_src=twsrc%5Etfw">@chanmina1014</a> ã¨ã®ã‚¿ã‚¤ã‚¢ãƒƒãƒ—ã‚½ãƒ³ã‚°ã€Angelã€ã®MVãŒã¤ã„ã«å…¬é–‹ğŸŒ´<br><br>MVã‚’ä»Šã™ããƒã‚§ãƒƒã‚¯ğŸ¥: <a href="https://t.co/uOVoxMXFig">https://t.co/uOVoxMXFig</a> <br><br>å…¬é–‹ã‚’è¨˜å¿µã—ã¦ <a href="https://twitter.com/hashtag/%E3%81%A1%E3%82%83%E3%82%93%E3%81%BF%E3%81%AA?src=hash&amp;ref_src=twsrc%5Etfw">#ã¡ã‚ƒã‚“ã¿ãª</a> ã¨ã®ã‚³ãƒ©ãƒœã‚®ã‚¢ã¨ <a href="https://twitter.com/hashtag/%E3%82%A6%E3%83%AB%E3%83%88%E3%83%A9%E3%83%91%E3%83%A9%E3%83%80%E3%82%A4%E3%82%B9?src=hash&amp;ref_src=twsrc%5Etfw">#ã‚¦ãƒ«ãƒˆãƒ©ãƒ‘ãƒ©ãƒ€ã‚¤ã‚¹</a> ãŒå½“ãŸã‚‹ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚’é–‹å‚¬ï¼<a href="https://twitter.com/MonsterEnergyJP?ref_src=twsrc%5Etfw">@MonsterEnergyJP</a> ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ï¼†ã“ã®æŠ•ç¨¿ã‚’RTã—ã¦ã€å¿œå‹Ÿå®Œäº†ã€‚ <a href="https://t.co/A5KtNt2HjW">pic.twitter.com/A5KtNt2HjW</a></p>&mdash; Monster Energy Japan (@MonsterEnergyJP) <a href="https://twitter.com/MonsterEnergyJP/status/1286451077781618690?ref_src=twsrc%5Etfw">July 24, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">â€œStorywrangler: A massive exploratorium for sociolinguistic, cultural, socioeconomic, and political timelines using Twitterâ€<br><br>New preprint describing our phrase popularity viewer &amp; API for over 100,000,000,000 tweets since 2010<a href="https://t.co/xLN0DO1e7b">https://t.co/xLN0DO1e7b</a><a href="https://t.co/GBEfeFMs6g">https://t.co/GBEfeFMs6g</a> <a href="https://t.co/uQU1ZLzo5p">pic.twitter.com/uQU1ZLzo5p</a></p>&mdash; Chris Danforth (@ChrisDanforth) <a href="https://twitter.com/ChrisDanforth/status/1288067910494310403?ref_src=twsrc%5Etfw">July 28, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Decomposed Generation Networks with Structure Prediction for Recipe  Generation from Food Images

Hao Wang, Guosheng Lin, Steven C. H. Hoi, Chunyan Miao

- retweets: 2114, favorites: 3021 (07/29/2020 08:10:30)

- links: [abs](https://arxiv.org/abs/2007.13374) | [pdf](https://arxiv.org/pdf/2007.13374)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Recipe generation from food images and ingredients is a challenging task, which requires the interpretation of the information from another modality. Different from the image captioning task, where the captions usually have one sentence, cooking instructions contain multiple sentences and have obvious structures. To help the model capture the recipe structure and avoid missing some cooking details, we propose a novel framework: Decomposed Generation Networks (DGN) with structure prediction, to get more structured and complete recipe generation outputs. To be specific, we split each cooking instruction into several phases, and assign different sub-generators to each phase. Our approach includes two novel ideas: (i) learning the recipe structures with the global structure prediction component and (ii) producing recipe phases in the sub-generator output component based on the predicted structure. Extensive experiments on the challenging large-scale Recipe1M dataset validate the effectiveness of our proposed model DGN, which improves the performance over the state-of-the-art results.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">ã¡ã‚ƒã‚“ã¿ãª<a href="https://twitter.com/chanmina1014?ref_src=twsrc%5Etfw">@chanmina1014</a> ã¨ã®ã‚¿ã‚¤ã‚¢ãƒƒãƒ—ã‚½ãƒ³ã‚°ã€Angelã€ã®MVãŒã¤ã„ã«å…¬é–‹ğŸŒ´<br><br>MVã‚’ä»Šã™ããƒã‚§ãƒƒã‚¯ğŸ¥: <a href="https://t.co/uOVoxMXFig">https://t.co/uOVoxMXFig</a> <br><br>å…¬é–‹ã‚’è¨˜å¿µã—ã¦ <a href="https://twitter.com/hashtag/%E3%81%A1%E3%82%83%E3%82%93%E3%81%BF%E3%81%AA?src=hash&amp;ref_src=twsrc%5Etfw">#ã¡ã‚ƒã‚“ã¿ãª</a> ã¨ã®ã‚³ãƒ©ãƒœã‚®ã‚¢ã¨ <a href="https://twitter.com/hashtag/%E3%82%A6%E3%83%AB%E3%83%88%E3%83%A9%E3%83%91%E3%83%A9%E3%83%80%E3%82%A4%E3%82%B9?src=hash&amp;ref_src=twsrc%5Etfw">#ã‚¦ãƒ«ãƒˆãƒ©ãƒ‘ãƒ©ãƒ€ã‚¤ã‚¹</a> ãŒå½“ãŸã‚‹ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚’é–‹å‚¬ï¼<a href="https://twitter.com/MonsterEnergyJP?ref_src=twsrc%5Etfw">@MonsterEnergyJP</a> ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ï¼†ã“ã®æŠ•ç¨¿ã‚’RTã—ã¦ã€å¿œå‹Ÿå®Œäº†ã€‚ <a href="https://t.co/A5KtNt2HjW">pic.twitter.com/A5KtNt2HjW</a></p>&mdash; Monster Energy Japan (@MonsterEnergyJP) <a href="https://twitter.com/MonsterEnergyJP/status/1286451077781618690?ref_src=twsrc%5Etfw">July 24, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Decomposed Generation Networks with Structure Prediction for Recipe Generation from Food Images<br>pdf: <a href="https://t.co/Q3K6ytutSr">https://t.co/Q3K6ytutSr</a><br>abs: <a href="https://t.co/74PP6Ptv6x">https://t.co/74PP6Ptv6x</a> <a href="https://t.co/vcHA4l9q5X">pic.twitter.com/vcHA4l9q5X</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1287938432774397954?ref_src=twsrc%5Etfw">July 28, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Representation Learning with Video Deep InfoMax

Hjelm, R Devon, Bachman, Philip

- retweets: 2109, favorites: 3023 (07/29/2020 08:10:31)

- links: [abs](https://arxiv.org/abs/2007.13278) | [pdf](https://arxiv.org/pdf/2007.13278)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Self-supervised learning has made unsupervised pretraining relevant again for difficult computer vision tasks. The most effective self-supervised methods involve prediction tasks based on features extracted from diverse views of the data. DeepInfoMax (DIM) is a self-supervised method which leverages the internal structure of deep networks to construct such views, forming prediction tasks between local features which depend on small patches in an image and global features which depend on the whole image. In this paper, we extend DIM to the video domain by leveraging similar structure in spatio-temporal networks, producing a method we call Video Deep InfoMax(VDIM). We find that drawing views from both natural-rate sequences and temporally-downsampled sequences yields results on Kinetics-pretrained action recognition tasks which match or outperform prior state-of-the-art methods that use more costly large-time-scale transformer models. We also examine the effects of data augmentation and fine-tuning methods, accomplishingSoTA by a large margin when training only on the UCF-101 dataset.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">ã¡ã‚ƒã‚“ã¿ãª<a href="https://twitter.com/chanmina1014?ref_src=twsrc%5Etfw">@chanmina1014</a> ã¨ã®ã‚¿ã‚¤ã‚¢ãƒƒãƒ—ã‚½ãƒ³ã‚°ã€Angelã€ã®MVãŒã¤ã„ã«å…¬é–‹ğŸŒ´<br><br>MVã‚’ä»Šã™ããƒã‚§ãƒƒã‚¯ğŸ¥: <a href="https://t.co/uOVoxMXFig">https://t.co/uOVoxMXFig</a> <br><br>å…¬é–‹ã‚’è¨˜å¿µã—ã¦ <a href="https://twitter.com/hashtag/%E3%81%A1%E3%82%83%E3%82%93%E3%81%BF%E3%81%AA?src=hash&amp;ref_src=twsrc%5Etfw">#ã¡ã‚ƒã‚“ã¿ãª</a> ã¨ã®ã‚³ãƒ©ãƒœã‚®ã‚¢ã¨ <a href="https://twitter.com/hashtag/%E3%82%A6%E3%83%AB%E3%83%88%E3%83%A9%E3%83%91%E3%83%A9%E3%83%80%E3%82%A4%E3%82%B9?src=hash&amp;ref_src=twsrc%5Etfw">#ã‚¦ãƒ«ãƒˆãƒ©ãƒ‘ãƒ©ãƒ€ã‚¤ã‚¹</a> ãŒå½“ãŸã‚‹ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚’é–‹å‚¬ï¼<a href="https://twitter.com/MonsterEnergyJP?ref_src=twsrc%5Etfw">@MonsterEnergyJP</a> ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ï¼†ã“ã®æŠ•ç¨¿ã‚’RTã—ã¦ã€å¿œå‹Ÿå®Œäº†ã€‚ <a href="https://t.co/A5KtNt2HjW">pic.twitter.com/A5KtNt2HjW</a></p>&mdash; Monster Energy Japan (@MonsterEnergyJP) <a href="https://twitter.com/MonsterEnergyJP/status/1286451077781618690?ref_src=twsrc%5Etfw">July 24, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Learning Disentangled Representations with Latent Variation  Predictability

Xinqi Zhu, Chang Xu, Dacheng Tao

- retweets: 2114, favorites: 3017 (07/29/2020 08:10:31)

- links: [abs](https://arxiv.org/abs/2007.12885) | [pdf](https://arxiv.org/pdf/2007.12885)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Latent traversal is a popular approach to visualize the disentangled latent representations. Given a bunch of variations in a single unit of the latent representation, it is expected that there is a change in a single factor of variation of the data while others are fixed. However, this impressive experimental observation is rarely explicitly encoded in the objective function of learning disentangled representations. This paper defines the variation predictability of latent disentangled representations. Given image pairs generated by latent codes varying in a single dimension, this varied dimension could be closely correlated with these image pairs if the representation is well disentangled. Within an adversarial generation process, we encourage variation predictability by maximizing the mutual information between latent variations and corresponding image pairs. We further develop an evaluation metric that does not rely on the ground-truth generative factors to measure the disentanglement of latent representations. The proposed variation predictability is a general constraint that is applicable to the VAE and GAN frameworks for boosting disentanglement of latent representations. Experiments show that the proposed variation predictability correlates well with existing ground-truth-required metrics and the proposed algorithm is effective for disentanglement learning.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">ã¡ã‚ƒã‚“ã¿ãª<a href="https://twitter.com/chanmina1014?ref_src=twsrc%5Etfw">@chanmina1014</a> ã¨ã®ã‚¿ã‚¤ã‚¢ãƒƒãƒ—ã‚½ãƒ³ã‚°ã€Angelã€ã®MVãŒã¤ã„ã«å…¬é–‹ğŸŒ´<br><br>MVã‚’ä»Šã™ããƒã‚§ãƒƒã‚¯ğŸ¥: <a href="https://t.co/uOVoxMXFig">https://t.co/uOVoxMXFig</a> <br><br>å…¬é–‹ã‚’è¨˜å¿µã—ã¦ <a href="https://twitter.com/hashtag/%E3%81%A1%E3%82%83%E3%82%93%E3%81%BF%E3%81%AA?src=hash&amp;ref_src=twsrc%5Etfw">#ã¡ã‚ƒã‚“ã¿ãª</a> ã¨ã®ã‚³ãƒ©ãƒœã‚®ã‚¢ã¨ <a href="https://twitter.com/hashtag/%E3%82%A6%E3%83%AB%E3%83%88%E3%83%A9%E3%83%91%E3%83%A9%E3%83%80%E3%82%A4%E3%82%B9?src=hash&amp;ref_src=twsrc%5Etfw">#ã‚¦ãƒ«ãƒˆãƒ©ãƒ‘ãƒ©ãƒ€ã‚¤ã‚¹</a> ãŒå½“ãŸã‚‹ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚’é–‹å‚¬ï¼<a href="https://twitter.com/MonsterEnergyJP?ref_src=twsrc%5Etfw">@MonsterEnergyJP</a> ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ï¼†ã“ã®æŠ•ç¨¿ã‚’RTã—ã¦ã€å¿œå‹Ÿå®Œäº†ã€‚ <a href="https://t.co/A5KtNt2HjW">pic.twitter.com/A5KtNt2HjW</a></p>&mdash; Monster Energy Japan (@MonsterEnergyJP) <a href="https://twitter.com/MonsterEnergyJP/status/1286451077781618690?ref_src=twsrc%5Etfw">July 24, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. From Boltzmann Machines to Neural Networks and Back Again

Surbhi Goel, Adam Klivans, Frederic Koehler

- retweets: 2107, favorites: 3007 (07/29/2020 08:10:31)

- links: [abs](https://arxiv.org/abs/2007.12815) | [pdf](https://arxiv.org/pdf/2007.12815)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.DS](https://arxiv.org/list/cs.DS/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Graphical models are powerful tools for modeling high-dimensional data, but learning graphical models in the presence of latent variables is well-known to be difficult. In this work we give new results for learning Restricted Boltzmann Machines, probably the most well-studied class of latent variable models. Our results are based on new connections to learning two-layer neural networks under $\ell_{\infty}$ bounded input; for both problems, we give nearly optimal results under the conjectured hardness of sparse parity with noise. Using the connection between RBMs and feedforward networks, we also initiate the theoretical study of $supervised~RBMs$ [Hinton, 2012], a version of neural-network learning that couples distributional assumptions induced from the underlying graphical model with the architecture of the unknown function class. We then give an algorithm for learning a natural class of supervised RBMs with better runtime than what is possible for its related class of networks without distributional assumptions.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">ã¡ã‚ƒã‚“ã¿ãª<a href="https://twitter.com/chanmina1014?ref_src=twsrc%5Etfw">@chanmina1014</a> ã¨ã®ã‚¿ã‚¤ã‚¢ãƒƒãƒ—ã‚½ãƒ³ã‚°ã€Angelã€ã®MVãŒã¤ã„ã«å…¬é–‹ğŸŒ´<br><br>MVã‚’ä»Šã™ããƒã‚§ãƒƒã‚¯ğŸ¥: <a href="https://t.co/uOVoxMXFig">https://t.co/uOVoxMXFig</a> <br><br>å…¬é–‹ã‚’è¨˜å¿µã—ã¦ <a href="https://twitter.com/hashtag/%E3%81%A1%E3%82%83%E3%82%93%E3%81%BF%E3%81%AA?src=hash&amp;ref_src=twsrc%5Etfw">#ã¡ã‚ƒã‚“ã¿ãª</a> ã¨ã®ã‚³ãƒ©ãƒœã‚®ã‚¢ã¨ <a href="https://twitter.com/hashtag/%E3%82%A6%E3%83%AB%E3%83%88%E3%83%A9%E3%83%91%E3%83%A9%E3%83%80%E3%82%A4%E3%82%B9?src=hash&amp;ref_src=twsrc%5Etfw">#ã‚¦ãƒ«ãƒˆãƒ©ãƒ‘ãƒ©ãƒ€ã‚¤ã‚¹</a> ãŒå½“ãŸã‚‹ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚’é–‹å‚¬ï¼<a href="https://twitter.com/MonsterEnergyJP?ref_src=twsrc%5Etfw">@MonsterEnergyJP</a> ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ï¼†ã“ã®æŠ•ç¨¿ã‚’RTã—ã¦ã€å¿œå‹Ÿå®Œäº†ã€‚ <a href="https://t.co/A5KtNt2HjW">pic.twitter.com/A5KtNt2HjW</a></p>&mdash; Monster Energy Japan (@MonsterEnergyJP) <a href="https://twitter.com/MonsterEnergyJP/status/1286451077781618690?ref_src=twsrc%5Etfw">July 24, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. Unsupervised Subword Modeling Using Autoregressive Pretraining and  Cross-Lingual Phone-Aware Modeling

Siyuan Feng, Odette Scharenborg

- retweets: 2107, favorites: 3005 (07/29/2020 08:10:31)

- links: [abs](https://arxiv.org/abs/2007.13002) | [pdf](https://arxiv.org/pdf/2007.13002)
- [eess.AS](https://arxiv.org/list/eess.AS/recent) | [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.SD](https://arxiv.org/list/cs.SD/recent)

This study addresses unsupervised subword modeling, i.e., learning feature representations that can distinguish subword units of a language. The proposed approach adopts a two-stage bottleneck feature (BNF) learning framework, consisting of autoregressive predictive coding (APC) as a front-end and a DNN-BNF model as a back-end. APC pretrained features are set as input features to a DNN-BNF model. A language-mismatched ASR system is used to provide cross-lingual phone labels for DNN-BNF model training. Finally, BNFs are extracted as the subword-discriminative feature representation. A second aim of this work is to investigate the robustness of our approach's effectiveness to different amounts of training data. The results on Libri-light and the ZeroSpeech 2017 databases show that APC is effective in front-end feature pretraining. Our whole system outperforms the state of the art on both databases. Cross-lingual phone labels for English data by a Dutch ASR outperform those by a Mandarin ASR, possibly linked to the larger similarity of Dutch compared to Mandarin with English. Our system is less sensitive to training data amount when the training data is over 50 hours. APC pretraining leads to a reduction of needed training material from over 5,000 hours to around 200 hours with little performance degradation.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">ã¡ã‚ƒã‚“ã¿ãª<a href="https://twitter.com/chanmina1014?ref_src=twsrc%5Etfw">@chanmina1014</a> ã¨ã®ã‚¿ã‚¤ã‚¢ãƒƒãƒ—ã‚½ãƒ³ã‚°ã€Angelã€ã®MVãŒã¤ã„ã«å…¬é–‹ğŸŒ´<br><br>MVã‚’ä»Šã™ããƒã‚§ãƒƒã‚¯ğŸ¥: <a href="https://t.co/uOVoxMXFig">https://t.co/uOVoxMXFig</a> <br><br>å…¬é–‹ã‚’è¨˜å¿µã—ã¦ <a href="https://twitter.com/hashtag/%E3%81%A1%E3%82%83%E3%82%93%E3%81%BF%E3%81%AA?src=hash&amp;ref_src=twsrc%5Etfw">#ã¡ã‚ƒã‚“ã¿ãª</a> ã¨ã®ã‚³ãƒ©ãƒœã‚®ã‚¢ã¨ <a href="https://twitter.com/hashtag/%E3%82%A6%E3%83%AB%E3%83%88%E3%83%A9%E3%83%91%E3%83%A9%E3%83%80%E3%82%A4%E3%82%B9?src=hash&amp;ref_src=twsrc%5Etfw">#ã‚¦ãƒ«ãƒˆãƒ©ãƒ‘ãƒ©ãƒ€ã‚¤ã‚¹</a> ãŒå½“ãŸã‚‹ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚’é–‹å‚¬ï¼<a href="https://twitter.com/MonsterEnergyJP?ref_src=twsrc%5Etfw">@MonsterEnergyJP</a> ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ï¼†ã“ã®æŠ•ç¨¿ã‚’RTã—ã¦ã€å¿œå‹Ÿå®Œäº†ã€‚ <a href="https://t.co/A5KtNt2HjW">pic.twitter.com/A5KtNt2HjW</a></p>&mdash; Monster Energy Japan (@MonsterEnergyJP) <a href="https://twitter.com/MonsterEnergyJP/status/1286451077781618690?ref_src=twsrc%5Etfw">July 24, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. XCAT-GAN for Synthesizing 3D Consistent Labeled Cardiac MR Images on  Anatomically Variable XCAT Phantoms

Sina Amirrajab, Samaneh Abbasi-Sureshjani, Yasmina Al Khalil, Cristian Lorenz, Juergen Weese, Josien Pluim, Marcel Breeuwer

- retweets: 2108, favorites: 3004 (07/29/2020 08:10:31)

- links: [abs](https://arxiv.org/abs/2007.13408) | [pdf](https://arxiv.org/pdf/2007.13408)
- [eess.IV](https://arxiv.org/list/eess.IV/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Generative adversarial networks (GANs) have provided promising data enrichment solutions by synthesizing high-fidelity images. However, generating large sets of labeled images with new anatomical variations remains unexplored. We propose a novel method for synthesizing cardiac magnetic resonance (CMR) images on a population of virtual subjects with a large anatomical variation, introduced using the 4D eXtended Cardiac and Torso (XCAT) computerized human phantom. We investigate two conditional image synthesis approaches grounded on a semantically-consistent mask-guided image generation technique: 4-class and 8-class XCAT-GANs. The 4-class technique relies on only the annotations of the heart; while the 8-class technique employs a predicted multi-tissue label map of the heart-surrounding organs and provides better guidance for our conditional image synthesis. For both techniques, we train our conditional XCAT-GAN with real images paired with corresponding labels and subsequently at the inference time, we substitute the labels with the XCAT derived ones. Therefore, the trained network accurately transfers the tissue-specific textures to the new label maps. By creating 33 virtual subjects of synthetic CMR images at the end-diastolic and end-systolic phases, we evaluate the usefulness of such data in the downstream cardiac cavity segmentation task under different augmentation strategies. Results demonstrate that even with only 20% of real images (40 volumes) seen during training, segmentation performance is retained with the addition of synthetic CMR images. Moreover, the improvement in utilizing synthetic images for augmenting the real data is evident through the reduction of Hausdorff distance up to 28% and an increase in the Dice score up to 5%, indicating a higher similarity to the ground truth in all dimensions.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">ã¡ã‚ƒã‚“ã¿ãª<a href="https://twitter.com/chanmina1014?ref_src=twsrc%5Etfw">@chanmina1014</a> ã¨ã®ã‚¿ã‚¤ã‚¢ãƒƒãƒ—ã‚½ãƒ³ã‚°ã€Angelã€ã®MVãŒã¤ã„ã«å…¬é–‹ğŸŒ´<br><br>MVã‚’ä»Šã™ããƒã‚§ãƒƒã‚¯ğŸ¥: <a href="https://t.co/uOVoxMXFig">https://t.co/uOVoxMXFig</a> <br><br>å…¬é–‹ã‚’è¨˜å¿µã—ã¦ <a href="https://twitter.com/hashtag/%E3%81%A1%E3%82%83%E3%82%93%E3%81%BF%E3%81%AA?src=hash&amp;ref_src=twsrc%5Etfw">#ã¡ã‚ƒã‚“ã¿ãª</a> ã¨ã®ã‚³ãƒ©ãƒœã‚®ã‚¢ã¨ <a href="https://twitter.com/hashtag/%E3%82%A6%E3%83%AB%E3%83%88%E3%83%A9%E3%83%91%E3%83%A9%E3%83%80%E3%82%A4%E3%82%B9?src=hash&amp;ref_src=twsrc%5Etfw">#ã‚¦ãƒ«ãƒˆãƒ©ãƒ‘ãƒ©ãƒ€ã‚¤ã‚¹</a> ãŒå½“ãŸã‚‹ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚’é–‹å‚¬ï¼<a href="https://twitter.com/MonsterEnergyJP?ref_src=twsrc%5Etfw">@MonsterEnergyJP</a> ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ï¼†ã“ã®æŠ•ç¨¿ã‚’RTã—ã¦ã€å¿œå‹Ÿå®Œäº†ã€‚ <a href="https://t.co/A5KtNt2HjW">pic.twitter.com/A5KtNt2HjW</a></p>&mdash; Monster Energy Japan (@MonsterEnergyJP) <a href="https://twitter.com/MonsterEnergyJP/status/1286451077781618690?ref_src=twsrc%5Etfw">July 24, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. Dialog without Dialog Data: Learning Visual Dialog Agents from VQA Data

Michael Cogswell, Jiasen Lu, Rishabh Jain, Stefan Lee, Devi Parikh, Dhruv Batra

- retweets: 2107, favorites: 3003 (07/29/2020 08:10:31)

- links: [abs](https://arxiv.org/abs/2007.12750) | [pdf](https://arxiv.org/pdf/2007.12750)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.CL](https://arxiv.org/list/cs.CL/recent)

Can we develop visually grounded dialog agents that can efficiently adapt to new tasks without forgetting how to talk to people? Such agents could leverage a larger variety of existing data to generalize to new tasks, minimizing expensive data collection and annotation. In this work, we study a setting we call "Dialog without Dialog", which requires agents to develop visually grounded dialog models that can adapt to new tasks without language level supervision. By factorizing intention and language, our model minimizes linguistic drift after fine-tuning for new tasks. We present qualitative results, automated metrics, and human studies that all show our model can adapt to new tasks and maintain language quality. Baselines either fail to perform well at new tasks or experience language drift, becoming unintelligible to humans. Code has been made available at https://github.com/mcogswell/dialog_without_dialog

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">ã¡ã‚ƒã‚“ã¿ãª<a href="https://twitter.com/chanmina1014?ref_src=twsrc%5Etfw">@chanmina1014</a> ã¨ã®ã‚¿ã‚¤ã‚¢ãƒƒãƒ—ã‚½ãƒ³ã‚°ã€Angelã€ã®MVãŒã¤ã„ã«å…¬é–‹ğŸŒ´<br><br>MVã‚’ä»Šã™ããƒã‚§ãƒƒã‚¯ğŸ¥: <a href="https://t.co/uOVoxMXFig">https://t.co/uOVoxMXFig</a> <br><br>å…¬é–‹ã‚’è¨˜å¿µã—ã¦ <a href="https://twitter.com/hashtag/%E3%81%A1%E3%82%83%E3%82%93%E3%81%BF%E3%81%AA?src=hash&amp;ref_src=twsrc%5Etfw">#ã¡ã‚ƒã‚“ã¿ãª</a> ã¨ã®ã‚³ãƒ©ãƒœã‚®ã‚¢ã¨ <a href="https://twitter.com/hashtag/%E3%82%A6%E3%83%AB%E3%83%88%E3%83%A9%E3%83%91%E3%83%A9%E3%83%80%E3%82%A4%E3%82%B9?src=hash&amp;ref_src=twsrc%5Etfw">#ã‚¦ãƒ«ãƒˆãƒ©ãƒ‘ãƒ©ãƒ€ã‚¤ã‚¹</a> ãŒå½“ãŸã‚‹ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚’é–‹å‚¬ï¼<a href="https://twitter.com/MonsterEnergyJP?ref_src=twsrc%5Etfw">@MonsterEnergyJP</a> ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ï¼†ã“ã®æŠ•ç¨¿ã‚’RTã—ã¦ã€å¿œå‹Ÿå®Œäº†ã€‚ <a href="https://t.co/A5KtNt2HjW">pic.twitter.com/A5KtNt2HjW</a></p>&mdash; Monster Energy Japan (@MonsterEnergyJP) <a href="https://twitter.com/MonsterEnergyJP/status/1286451077781618690?ref_src=twsrc%5Etfw">July 24, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. Modal Uncertainty Estimation via Discrete Latent Representation

Di Qiu, Lok Ming Lui

- retweets: 2107, favorites: 3002 (07/29/2020 08:10:31)

- links: [abs](https://arxiv.org/abs/2007.12858) | [pdf](https://arxiv.org/pdf/2007.12858)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Many important problems in the real world don't have unique solutions. It is thus important for machine learning models to be capable of proposing different plausible solutions with meaningful probability measures. In this work we introduce such a deep learning framework that learns the one-to-many mappings between the inputs and outputs, together with faithful uncertainty measures. We call our framework {\it modal uncertainty estimation} since we model the one-to-many mappings to be generated through a set of discrete latent variables, each representing a latent mode hypothesis that explains the corresponding type of input-output relationship. The discrete nature of the latent representations thus allows us to estimate for any input the conditional probability distribution of the outputs very effectively. Both the discrete latent space and its uncertainty estimation are jointly learned during training. We motivate our use of discrete latent space through the multi-modal posterior collapse problem in current conditional generative models, then develop the theoretical background, and extensively validate our method on both synthetic and realistic tasks. Our framework demonstrates significantly more accurate uncertainty estimation than the current state-of-the-art methods, and is informative and convenient for practical use.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">ã¡ã‚ƒã‚“ã¿ãª<a href="https://twitter.com/chanmina1014?ref_src=twsrc%5Etfw">@chanmina1014</a> ã¨ã®ã‚¿ã‚¤ã‚¢ãƒƒãƒ—ã‚½ãƒ³ã‚°ã€Angelã€ã®MVãŒã¤ã„ã«å…¬é–‹ğŸŒ´<br><br>MVã‚’ä»Šã™ããƒã‚§ãƒƒã‚¯ğŸ¥: <a href="https://t.co/uOVoxMXFig">https://t.co/uOVoxMXFig</a> <br><br>å…¬é–‹ã‚’è¨˜å¿µã—ã¦ <a href="https://twitter.com/hashtag/%E3%81%A1%E3%82%83%E3%82%93%E3%81%BF%E3%81%AA?src=hash&amp;ref_src=twsrc%5Etfw">#ã¡ã‚ƒã‚“ã¿ãª</a> ã¨ã®ã‚³ãƒ©ãƒœã‚®ã‚¢ã¨ <a href="https://twitter.com/hashtag/%E3%82%A6%E3%83%AB%E3%83%88%E3%83%A9%E3%83%91%E3%83%A9%E3%83%80%E3%82%A4%E3%82%B9?src=hash&amp;ref_src=twsrc%5Etfw">#ã‚¦ãƒ«ãƒˆãƒ©ãƒ‘ãƒ©ãƒ€ã‚¤ã‚¹</a> ãŒå½“ãŸã‚‹ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚’é–‹å‚¬ï¼<a href="https://twitter.com/MonsterEnergyJP?ref_src=twsrc%5Etfw">@MonsterEnergyJP</a> ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ï¼†ã“ã®æŠ•ç¨¿ã‚’RTã—ã¦ã€å¿œå‹Ÿå®Œäº†ã€‚ <a href="https://t.co/A5KtNt2HjW">pic.twitter.com/A5KtNt2HjW</a></p>&mdash; Monster Energy Japan (@MonsterEnergyJP) <a href="https://twitter.com/MonsterEnergyJP/status/1286451077781618690?ref_src=twsrc%5Etfw">July 24, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 10. A Survey on Complex Question Answering over Knowledge Base: Recent  Advances and Challenges

Bin Fu, Yunqi Qiu, Chengguang Tang, Yang Li, Haiyang Yu, Jian Sun

- retweets: 2107, favorites: 3002 (07/29/2020 08:10:32)

- links: [abs](https://arxiv.org/abs/2007.13069) | [pdf](https://arxiv.org/pdf/2007.13069)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.HC](https://arxiv.org/list/cs.HC/recent)

Question Answering (QA) over Knowledge Base (KB) aims to automatically answer natural language questions via well-structured relation information between entities stored in knowledge bases. In order to make KBQA more applicable in actual scenarios, researchers have shifted their attention from simple questions to complex questions, which require more KB triples and constraint inference. In this paper, we introduce the recent advances in complex QA. Besides traditional methods relying on templates and rules, the research is categorized into a taxonomy that contains two main branches, namely Information Retrieval-based and Neural Semantic Parsing-based. After describing the methods of these branches, we analyze directions for future research and introduce the models proposed by the Alime team.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">ã¡ã‚ƒã‚“ã¿ãª<a href="https://twitter.com/chanmina1014?ref_src=twsrc%5Etfw">@chanmina1014</a> ã¨ã®ã‚¿ã‚¤ã‚¢ãƒƒãƒ—ã‚½ãƒ³ã‚°ã€Angelã€ã®MVãŒã¤ã„ã«å…¬é–‹ğŸŒ´<br><br>MVã‚’ä»Šã™ããƒã‚§ãƒƒã‚¯ğŸ¥: <a href="https://t.co/uOVoxMXFig">https://t.co/uOVoxMXFig</a> <br><br>å…¬é–‹ã‚’è¨˜å¿µã—ã¦ <a href="https://twitter.com/hashtag/%E3%81%A1%E3%82%83%E3%82%93%E3%81%BF%E3%81%AA?src=hash&amp;ref_src=twsrc%5Etfw">#ã¡ã‚ƒã‚“ã¿ãª</a> ã¨ã®ã‚³ãƒ©ãƒœã‚®ã‚¢ã¨ <a href="https://twitter.com/hashtag/%E3%82%A6%E3%83%AB%E3%83%88%E3%83%A9%E3%83%91%E3%83%A9%E3%83%80%E3%82%A4%E3%82%B9?src=hash&amp;ref_src=twsrc%5Etfw">#ã‚¦ãƒ«ãƒˆãƒ©ãƒ‘ãƒ©ãƒ€ã‚¤ã‚¹</a> ãŒå½“ãŸã‚‹ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚’é–‹å‚¬ï¼<a href="https://twitter.com/MonsterEnergyJP?ref_src=twsrc%5Etfw">@MonsterEnergyJP</a> ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ï¼†ã“ã®æŠ•ç¨¿ã‚’RTã—ã¦ã€å¿œå‹Ÿå®Œäº†ã€‚ <a href="https://t.co/A5KtNt2HjW">pic.twitter.com/A5KtNt2HjW</a></p>&mdash; Monster Energy Japan (@MonsterEnergyJP) <a href="https://twitter.com/MonsterEnergyJP/status/1286451077781618690?ref_src=twsrc%5Etfw">July 24, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 11. Applying Semantic Segmentation to Autonomous Cars in the Snowy  Environment

Zhaoyu Pan, Takanori Emaru, Ankit Ravankar, Yukinori Kobayashi

- retweets: 2107, favorites: 3001 (07/29/2020 08:10:32)

- links: [abs](https://arxiv.org/abs/2007.12869) | [pdf](https://arxiv.org/pdf/2007.12869)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

This paper mainly focuses on environment perception in snowy situations which forms the backbone of the autonomous driving technology. For the purpose, semantic segmentation is employed to classify the objects while the vehicle is driven autonomously. We train the Fully Convolutional Networks (FCN) on our own dataset and present the experimental results. Finally, the outcomes are analyzed to give a conclusion. It can be concluded that the database still needs to be optimized and a favorable algorithm should be proposed to get better results.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">ã¡ã‚ƒã‚“ã¿ãª<a href="https://twitter.com/chanmina1014?ref_src=twsrc%5Etfw">@chanmina1014</a> ã¨ã®ã‚¿ã‚¤ã‚¢ãƒƒãƒ—ã‚½ãƒ³ã‚°ã€Angelã€ã®MVãŒã¤ã„ã«å…¬é–‹ğŸŒ´<br><br>MVã‚’ä»Šã™ããƒã‚§ãƒƒã‚¯ğŸ¥: <a href="https://t.co/uOVoxMXFig">https://t.co/uOVoxMXFig</a> <br><br>å…¬é–‹ã‚’è¨˜å¿µã—ã¦ <a href="https://twitter.com/hashtag/%E3%81%A1%E3%82%83%E3%82%93%E3%81%BF%E3%81%AA?src=hash&amp;ref_src=twsrc%5Etfw">#ã¡ã‚ƒã‚“ã¿ãª</a> ã¨ã®ã‚³ãƒ©ãƒœã‚®ã‚¢ã¨ <a href="https://twitter.com/hashtag/%E3%82%A6%E3%83%AB%E3%83%88%E3%83%A9%E3%83%91%E3%83%A9%E3%83%80%E3%82%A4%E3%82%B9?src=hash&amp;ref_src=twsrc%5Etfw">#ã‚¦ãƒ«ãƒˆãƒ©ãƒ‘ãƒ©ãƒ€ã‚¤ã‚¹</a> ãŒå½“ãŸã‚‹ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚’é–‹å‚¬ï¼<a href="https://twitter.com/MonsterEnergyJP?ref_src=twsrc%5Etfw">@MonsterEnergyJP</a> ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ï¼†ã“ã®æŠ•ç¨¿ã‚’RTã—ã¦ã€å¿œå‹Ÿå®Œäº†ã€‚ <a href="https://t.co/A5KtNt2HjW">pic.twitter.com/A5KtNt2HjW</a></p>&mdash; Monster Energy Japan (@MonsterEnergyJP) <a href="https://twitter.com/MonsterEnergyJP/status/1286451077781618690?ref_src=twsrc%5Etfw">July 24, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 12. Gradient Regularized Contrastive Learning for Continual Domain  Adaptation

Peng Su, Shixiang Tang, Peng Gao, Di Qiu, Ni Zhao, Xiaogang Wang

- retweets: 2107, favorites: 3000 (07/29/2020 08:10:32)

- links: [abs](https://arxiv.org/abs/2007.12942) | [pdf](https://arxiv.org/pdf/2007.12942)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Human beings can quickly adapt to environmental changes by leveraging learning experience. However, the poor ability of adapting to dynamic environments remains a major challenge for AI models. To better understand this issue, we study the problem of continual domain adaptation, where the model is presented with a labeled source domain and a sequence of unlabeled target domains. There are two major obstacles in this problem: domain shifts and catastrophic forgetting. In this work, we propose Gradient Regularized Contrastive Learning to solve the above obstacles. At the core of our method, gradient regularization plays two key roles: (1) enforces the gradient of contrastive loss not to increase the supervised training loss on the source domain, which maintains the discriminative power of learned features; (2) regularizes the gradient update on the new domain not to increase the classification loss on the old target domains, which enables the model to adapt to an in-coming target domain while preserving the performance of previously observed domains. Hence our method can jointly learn both semantically discriminative and domain-invariant features with labeled source domain and unlabeled target domains. The experiments on Digits, DomainNet and Office-Caltech benchmarks demonstrate the strong performance of our approach when compared to the state-of-the-art.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">ã¡ã‚ƒã‚“ã¿ãª<a href="https://twitter.com/chanmina1014?ref_src=twsrc%5Etfw">@chanmina1014</a> ã¨ã®ã‚¿ã‚¤ã‚¢ãƒƒãƒ—ã‚½ãƒ³ã‚°ã€Angelã€ã®MVãŒã¤ã„ã«å…¬é–‹ğŸŒ´<br><br>MVã‚’ä»Šã™ããƒã‚§ãƒƒã‚¯ğŸ¥: <a href="https://t.co/uOVoxMXFig">https://t.co/uOVoxMXFig</a> <br><br>å…¬é–‹ã‚’è¨˜å¿µã—ã¦ <a href="https://twitter.com/hashtag/%E3%81%A1%E3%82%83%E3%82%93%E3%81%BF%E3%81%AA?src=hash&amp;ref_src=twsrc%5Etfw">#ã¡ã‚ƒã‚“ã¿ãª</a> ã¨ã®ã‚³ãƒ©ãƒœã‚®ã‚¢ã¨ <a href="https://twitter.com/hashtag/%E3%82%A6%E3%83%AB%E3%83%88%E3%83%A9%E3%83%91%E3%83%A9%E3%83%80%E3%82%A4%E3%82%B9?src=hash&amp;ref_src=twsrc%5Etfw">#ã‚¦ãƒ«ãƒˆãƒ©ãƒ‘ãƒ©ãƒ€ã‚¤ã‚¹</a> ãŒå½“ãŸã‚‹ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚’é–‹å‚¬ï¼<a href="https://twitter.com/MonsterEnergyJP?ref_src=twsrc%5Etfw">@MonsterEnergyJP</a> ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ï¼†ã“ã®æŠ•ç¨¿ã‚’RTã—ã¦ã€å¿œå‹Ÿå®Œäº†ã€‚ <a href="https://t.co/A5KtNt2HjW">pic.twitter.com/A5KtNt2HjW</a></p>&mdash; Monster Energy Japan (@MonsterEnergyJP) <a href="https://twitter.com/MonsterEnergyJP/status/1286451077781618690?ref_src=twsrc%5Etfw">July 24, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 13. Robust and Generalizable Visual Representation Learning via Random  Convolutions

Zhenlin Xu, Deyi Liu, Junlin Yang, Marc Niethammer

- retweets: 2107, favorites: 3000 (07/29/2020 08:10:32)

- links: [abs](https://arxiv.org/abs/2007.13003) | [pdf](https://arxiv.org/pdf/2007.13003)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

While successful for various computer vision tasks, deep neural networks have shown to be vulnerable to texture style shifts and small perturbations to which humans are robust. Hence, our goal is to train models in such a way that improves their robustness to these perturbations. We are motivated by the approximately shape-preserving property of randomized convolutions, which is due to distance preservation under random linear transforms. Intuitively, randomized convolutions create an infinite number of new domains with similar object shapes but random local texture. Therefore, we explore using outputs of multi-scale random convolutions as new images or mixing them with the original images during training. When applying a network trained with our approach to unseen domains, our method consistently improves the performance on domain generalization benchmarks and is scalable to ImageNet. Especially for the challenging scenario of generalizing to the sketch domain in PACS and to ImageNet-Sketch, our method outperforms state-of-art methods by a large margin. More interestingly, our method can benefit downstream tasks by providing a more robust pretrained visual representation.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">ã¡ã‚ƒã‚“ã¿ãª<a href="https://twitter.com/chanmina1014?ref_src=twsrc%5Etfw">@chanmina1014</a> ã¨ã®ã‚¿ã‚¤ã‚¢ãƒƒãƒ—ã‚½ãƒ³ã‚°ã€Angelã€ã®MVãŒã¤ã„ã«å…¬é–‹ğŸŒ´<br><br>MVã‚’ä»Šã™ããƒã‚§ãƒƒã‚¯ğŸ¥: <a href="https://t.co/uOVoxMXFig">https://t.co/uOVoxMXFig</a> <br><br>å…¬é–‹ã‚’è¨˜å¿µã—ã¦ <a href="https://twitter.com/hashtag/%E3%81%A1%E3%82%83%E3%82%93%E3%81%BF%E3%81%AA?src=hash&amp;ref_src=twsrc%5Etfw">#ã¡ã‚ƒã‚“ã¿ãª</a> ã¨ã®ã‚³ãƒ©ãƒœã‚®ã‚¢ã¨ <a href="https://twitter.com/hashtag/%E3%82%A6%E3%83%AB%E3%83%88%E3%83%A9%E3%83%91%E3%83%A9%E3%83%80%E3%82%A4%E3%82%B9?src=hash&amp;ref_src=twsrc%5Etfw">#ã‚¦ãƒ«ãƒˆãƒ©ãƒ‘ãƒ©ãƒ€ã‚¤ã‚¹</a> ãŒå½“ãŸã‚‹ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚’é–‹å‚¬ï¼<a href="https://twitter.com/MonsterEnergyJP?ref_src=twsrc%5Etfw">@MonsterEnergyJP</a> ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ï¼†ã“ã®æŠ•ç¨¿ã‚’RTã—ã¦ã€å¿œå‹Ÿå®Œäº†ã€‚ <a href="https://t.co/A5KtNt2HjW">pic.twitter.com/A5KtNt2HjW</a></p>&mdash; Monster Energy Japan (@MonsterEnergyJP) <a href="https://twitter.com/MonsterEnergyJP/status/1286451077781618690?ref_src=twsrc%5Etfw">July 24, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 14. Self-Prediction for Joint Instance and Semantic Segmentation of Point  Clouds

Jinxian Liu, Minghui Yu, Bingbing Ni, Ye Chen

- retweets: 2107, favorites: 2999 (07/29/2020 08:10:32)

- links: [abs](https://arxiv.org/abs/2007.13344) | [pdf](https://arxiv.org/pdf/2007.13344)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.GR](https://arxiv.org/list/cs.GR/recent)

We develop a novel learning scheme named Self-Prediction for 3D instance and semantic segmentation of point clouds. Distinct from most existing methods that focus on designing convolutional operators, our method designs a new learning scheme to enhance point relation exploring for better segmentation. More specifically, we divide a point cloud sample into two subsets and construct a complete graph based on their representations. Then we use label propagation algorithm to predict labels of one subset when given labels of the other subset. By training with this Self-Prediction task, the backbone network is constrained to fully explore relational context/geometric/shape information and learn more discriminative features for segmentation. Moreover, a general associated framework equipped with our Self-Prediction scheme is designed for enhancing instance and semantic segmentation simultaneously, where instance and semantic representations are combined to perform Self-Prediction. Through this way, instance and semantic segmentation are collaborated and mutually reinforced. Significant performance improvements on instance and semantic segmentation compared with baseline are achieved on S3DIS and ShapeNet. Our method achieves state-of-the-art instance segmentation results on S3DIS and comparable semantic segmentation results compared with state-of-the-arts on S3DIS and ShapeNet when we only take PointNet++ as the backbone network.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">ã¡ã‚ƒã‚“ã¿ãª<a href="https://twitter.com/chanmina1014?ref_src=twsrc%5Etfw">@chanmina1014</a> ã¨ã®ã‚¿ã‚¤ã‚¢ãƒƒãƒ—ã‚½ãƒ³ã‚°ã€Angelã€ã®MVãŒã¤ã„ã«å…¬é–‹ğŸŒ´<br><br>MVã‚’ä»Šã™ããƒã‚§ãƒƒã‚¯ğŸ¥: <a href="https://t.co/uOVoxMXFig">https://t.co/uOVoxMXFig</a> <br><br>å…¬é–‹ã‚’è¨˜å¿µã—ã¦ <a href="https://twitter.com/hashtag/%E3%81%A1%E3%82%83%E3%82%93%E3%81%BF%E3%81%AA?src=hash&amp;ref_src=twsrc%5Etfw">#ã¡ã‚ƒã‚“ã¿ãª</a> ã¨ã®ã‚³ãƒ©ãƒœã‚®ã‚¢ã¨ <a href="https://twitter.com/hashtag/%E3%82%A6%E3%83%AB%E3%83%88%E3%83%A9%E3%83%91%E3%83%A9%E3%83%80%E3%82%A4%E3%82%B9?src=hash&amp;ref_src=twsrc%5Etfw">#ã‚¦ãƒ«ãƒˆãƒ©ãƒ‘ãƒ©ãƒ€ã‚¤ã‚¹</a> ãŒå½“ãŸã‚‹ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚’é–‹å‚¬ï¼<a href="https://twitter.com/MonsterEnergyJP?ref_src=twsrc%5Etfw">@MonsterEnergyJP</a> ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ï¼†ã“ã®æŠ•ç¨¿ã‚’RTã—ã¦ã€å¿œå‹Ÿå®Œäº†ã€‚ <a href="https://t.co/A5KtNt2HjW">pic.twitter.com/A5KtNt2HjW</a></p>&mdash; Monster Energy Japan (@MonsterEnergyJP) <a href="https://twitter.com/MonsterEnergyJP/status/1286451077781618690?ref_src=twsrc%5Etfw">July 24, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 15. Building Trust in Autonomous Vehicles: Role of Virtual Reality Driving  Simulators in HMI Design

Lia Morra, Fabrizio Lamberti, F. Gabriele PratticÃ³, Salvatore La Rosa, Paolo Montuschi

- retweets: 2107, favorites: 2999 (07/29/2020 08:10:32)

- links: [abs](https://arxiv.org/abs/2007.13371) | [pdf](https://arxiv.org/pdf/2007.13371)
- [cs.HC](https://arxiv.org/list/cs.HC/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.GR](https://arxiv.org/list/cs.GR/recent)

The investigation of factors contributing at making humans trust Autonomous Vehicles (AVs) will play a fundamental role in the adoption of such technology. The user's ability to form a mental model of the AV, which is crucial to establish trust, depends on effective user-vehicle communication; thus, the importance of Human-Machine Interaction (HMI) is poised to increase. In this work, we propose a methodology to validate the user experience in AVs based on continuous, objective information gathered from physiological signals, while the user is immersed in a Virtual Reality-based driving simulation. We applied this methodology to the design of a head-up display interface delivering visual cues about the vehicle' sensory and planning systems. Through this approach, we obtained qualitative and quantitative evidence that a complete picture of the vehicle's surrounding, despite the higher cognitive load, is conducive to a less stressful experience. Moreover, after having been exposed to a more informative interface, users involved in the study were also more willing to test a real AV. The proposed methodology could be extended by adjusting the simulation environment, the HMI and/or the vehicle's Artificial Intelligence modules to dig into other aspects of the user experience.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">ã¡ã‚ƒã‚“ã¿ãª<a href="https://twitter.com/chanmina1014?ref_src=twsrc%5Etfw">@chanmina1014</a> ã¨ã®ã‚¿ã‚¤ã‚¢ãƒƒãƒ—ã‚½ãƒ³ã‚°ã€Angelã€ã®MVãŒã¤ã„ã«å…¬é–‹ğŸŒ´<br><br>MVã‚’ä»Šã™ããƒã‚§ãƒƒã‚¯ğŸ¥: <a href="https://t.co/uOVoxMXFig">https://t.co/uOVoxMXFig</a> <br><br>å…¬é–‹ã‚’è¨˜å¿µã—ã¦ <a href="https://twitter.com/hashtag/%E3%81%A1%E3%82%83%E3%82%93%E3%81%BF%E3%81%AA?src=hash&amp;ref_src=twsrc%5Etfw">#ã¡ã‚ƒã‚“ã¿ãª</a> ã¨ã®ã‚³ãƒ©ãƒœã‚®ã‚¢ã¨ <a href="https://twitter.com/hashtag/%E3%82%A6%E3%83%AB%E3%83%88%E3%83%A9%E3%83%91%E3%83%A9%E3%83%80%E3%82%A4%E3%82%B9?src=hash&amp;ref_src=twsrc%5Etfw">#ã‚¦ãƒ«ãƒˆãƒ©ãƒ‘ãƒ©ãƒ€ã‚¤ã‚¹</a> ãŒå½“ãŸã‚‹ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚’é–‹å‚¬ï¼<a href="https://twitter.com/MonsterEnergyJP?ref_src=twsrc%5Etfw">@MonsterEnergyJP</a> ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ï¼†ã“ã®æŠ•ç¨¿ã‚’RTã—ã¦ã€å¿œå‹Ÿå®Œäº†ã€‚ <a href="https://t.co/A5KtNt2HjW">pic.twitter.com/A5KtNt2HjW</a></p>&mdash; Monster Energy Japan (@MonsterEnergyJP) <a href="https://twitter.com/MonsterEnergyJP/status/1286451077781618690?ref_src=twsrc%5Etfw">July 24, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 16. Towards Learning Convolutions from Scratch

Behnam Neyshabur

- retweets: 170, favorites: 842 (07/29/2020 08:10:32)

- links: [abs](https://arxiv.org/abs/2007.13657) | [pdf](https://arxiv.org/pdf/2007.13657)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Convolution is one of the most essential components of architectures used in computer vision. As machine learning moves towards reducing the expert bias and learning it from data, a natural next step seems to be learning convolution-like structures from scratch. This, however, has proven elusive. For example, current state-of-the-art architecture search algorithms use convolution as one of the existing modules rather than learning it from data. In an attempt to understand the inductive bias that gives rise to convolutions, we investigate minimum description length as a guiding principle and show that in some settings, it can indeed be indicative of the performance of architectures. To find architectures with small description length, we propose $\beta$-LASSO, a simple variant of LASSO algorithm that, when applied on fully-connected networks for image classification tasks, learns architectures with local connections and achieves state-of-the-art accuracies for training fully-connected nets on CIFAR-10 (85.19%), CIFAR-100 (59.56%) and SVHN (94.07%) bridging the gap between fully-connected and convolutional nets.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">ğŸ’¡ğŸ’¡What is the best acc an MLP can get on CIFAR10â“<br><br>65%â“ No, 85%â€¼ï¸<br><br>Trying to understand convolutions, we look at MDL and come up with a variant of LASSO that when applied to MLPs, it learns local connections and achieves amazing accuracy!<br><br>Paper: <a href="https://t.co/PUb2Q4tIBT">https://t.co/PUb2Q4tIBT</a><br><br>1/n <a href="https://t.co/ijrX9CFJ41">pic.twitter.com/ijrX9CFJ41</a></p>&mdash; Behnam Neyshabur (@bneyshabur) <a href="https://twitter.com/bneyshabur/status/1287936315829313536?ref_src=twsrc%5Etfw">July 28, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Towards Learning Convolutions from Scratch<br><br>â€œAs ML moves towards reducing the expert bias and learning it from data, a natural next step seems to be learning convolution-like structures from scratch.â€<br><br>Would be great to find the &quot;ConvNet&quot; for new domains.<a href="https://t.co/AoZh4LJCDK">https://t.co/AoZh4LJCDK</a> <a href="https://t.co/lJPGLJDWO6">https://t.co/lJPGLJDWO6</a> <a href="https://t.co/vYM3DjOXDC">pic.twitter.com/vYM3DjOXDC</a></p>&mdash; hardmaru (@hardmaru) <a href="https://twitter.com/hardmaru/status/1288016998929063936?ref_src=twsrc%5Etfw">July 28, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 17. 3D Human Shape and Pose from a Single Low-Resolution Image with  Self-Supervised Learning

Xiangyu Xu, Hao Chen, Francesc Moreno-Noguer, Laszlo A. Jeni, Fernando De la Torre

- retweets: 14, favorites: 82 (07/29/2020 08:10:33)

- links: [abs](https://arxiv.org/abs/2007.13666) | [pdf](https://arxiv.org/pdf/2007.13666)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [eess.IV](https://arxiv.org/list/eess.IV/recent)

3D human shape and pose estimation from monocular images has been an active area of research in computer vision, having a substantial impact on the development of new applications, from activity recognition to creating virtual avatars. Existing deep learning methods for 3D human shape and pose estimation rely on relatively high-resolution input images; however, high-resolution visual content is not always available in several practical scenarios such as video surveillance and sports broadcasting. Low-resolution images in real scenarios can vary in a wide range of sizes, and a model trained in one resolution does not typically degrade gracefully across resolutions. Two common approaches to solve the problem of low-resolution input are applying super-resolution techniques to the input images which may result in visual artifacts, or simply training one model for each resolution, which is impractical in many realistic applications. To address the above issues, this paper proposes a novel algorithm called RSC-Net, which consists of a Resolution-aware network, a Self-supervision loss, and a Contrastive learning scheme. The proposed network is able to learn the 3D body shape and pose across different resolutions with a single model. The self-supervision loss encourages scale-consistency of the output, and the contrastive learning scheme enforces scale-consistency of the deep features. We show that both these new training losses provide robustness when learning 3D shape and pose in a weakly-supervised manner. Extensive experiments demonstrate that the RSC-Net can achieve consistently better results than the state-of-the-art methods for challenging low-resolution images.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">3D Human Shape and Pose from a Single Low-Resolution Image with Self-Supervised Learning<br>pdf: <a href="https://t.co/WkRAuyXQkb">https://t.co/WkRAuyXQkb</a><br>abs: <a href="https://t.co/n5ALp76mDp">https://t.co/n5ALp76mDp</a><br>project page: <a href="https://t.co/x8Bjrlu4lO">https://t.co/x8Bjrlu4lO</a> <a href="https://t.co/no0vDmfETe">pic.twitter.com/no0vDmfETe</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1287919826422435840?ref_src=twsrc%5Etfw">July 28, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 18. How Epidemic Psychology Works on Social Media: Evolution of responses to  the COVID-19 pandemic

Luca Maria Aiello, Daniele Quercia, Ke Zhou, Marios Constantinides, Sanja Å Ä‡epanoviÄ‡, Sagar Joglekar

- retweets: 26, favorites: 69 (07/29/2020 08:10:33)

- links: [abs](https://arxiv.org/abs/2007.13169) | [pdf](https://arxiv.org/pdf/2007.13169)
- [cs.CY](https://arxiv.org/list/cs.CY/recent) | [cs.SI](https://arxiv.org/list/cs.SI/recent)

Disruptions resulting from an epidemic might often appear to amount to chaos but, in reality, can be understood in a systematic way through the lens of "epidemic psychology". According to the father of this research field, Philip Strong, not only is the epidemic biological; there is also the potential for three social epidemics: of fear, moralization, and action. This work is the first study to empirically test Strong's model at scale. It does so by studying the use of language on 39M social media posts in US about the COVID-19 pandemic, which is the first pandemic to spread this quickly not only on a global scale but also online. We identified three distinct phases, which parallel Kuebler-Ross's stages of grief. Each of them is characterized by different regimes of the three social epidemics: in the refusal phase, people refused to accept reality despite the increasing numbers of deaths in other countries; in the suspended reality phase (started after the announcement of the first death in the country), people's fear translated into anger about the looming feeling that things were about to change; finally, in the acceptance phase (started after the authorities imposed physical-distancing measures), people found a "new normal" for their daily activities. Our real-time operationalization of Strong's model makes it possible to embed epidemic psychology in any real-time model (e.g., epidemiological and mobility models).

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">The Epidemic Psychology of <a href="https://twitter.com/hashtag/COVID19?src=hash&amp;ref_src=twsrc%5Etfw">#COVID19</a>: we monitor on Twitter the epidemics of fear, moralization, and action theorized by sociologist Philip Strong. A nice display of two brand-new NLP tools to extract interaction types and medical entities from text <a href="https://t.co/ZnKztLP0ci">https://t.co/ZnKztLP0ci</a> <a href="https://t.co/0X1NQBCE7e">pic.twitter.com/0X1NQBCE7e</a></p>&mdash; Luca Maria Aiello (@lajello) <a href="https://twitter.com/lajello/status/1288077282041626628?ref_src=twsrc%5Etfw">July 28, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 19. From climate change to pandemics: decision science can help scientists  have impact

Christopher M. Baker, Patricia T. Campbell, Iadine Chades, Angela J. Dean, Susan M. Hester, Matthew H. Holden, James M. McCaw, Jodie McVernon, Robert Moss, Freya M. Shearer, Hugh P. Possingham

- retweets: 22, favorites: 59 (07/29/2020 08:10:33)

- links: [abs](https://arxiv.org/abs/2007.13261) | [pdf](https://arxiv.org/pdf/2007.13261)
- [cs.CY](https://arxiv.org/list/cs.CY/recent) | [physics.soc-ph](https://arxiv.org/list/physics.soc-ph/recent)

Scientific knowledge and advances are a cornerstone of modern society. They improve our understanding of the world we live in and help us navigate global challenges including emerging infectious diseases, climate change and the biodiversity crisis. For any scientist, whether they work primarily in fundamental knowledge generation or in the applied sciences, it is important to understand how science fits into a decision-making framework. Decision science is a field that aims to pinpoint evidence-based management strategies. It provides a framework for scientists to directly impact decisions or to understand how their work will fit into a decision process. Decision science is more than undertaking targeted and relevant scientific research or providing tools to assist policy makers; it is an approach to problem formulation, bringing together mathematical modelling, stakeholder values and logistical constraints to support decision making. In this paper we describe decision science, its use in different contexts, and highlight current gaps in methodology and application. The COVID-19 pandemic has thrust mathematical models into the public spotlight, but it is one of innumerable examples in which modelling informs decision making. Other examples include models of storm systems (eg. cyclones, hurricanes) and climate change. Although the decision timescale in these examples differs enormously (from hours to decades), the underlying decision science approach is common across all problems. Bridging communication gaps between different groups is one of the greatest challenges for scientists. However, by better understanding and engaging with the decision-making processes, scientists will have greater impact and make stronger contributions to important societal problems.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Iâ€™m excited to finally share our work about scientific research in the context of decision-making and public policy. We aimed to characterise how decision science can work in different contexts, depending on the urgency of decisions. <a href="https://t.co/MoUe6I5Vbo">https://t.co/MoUe6I5Vbo</a> <a href="https://t.co/WKvlNkJp00">pic.twitter.com/WKvlNkJp00</a></p>&mdash; Christopher Baker (@cbaker_research) <a href="https://twitter.com/cbaker_research/status/1287970985321230336?ref_src=twsrc%5Etfw">July 28, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 20. Trick the Body Trick the Mind: Avatar representation affects the  perception of available action possibilities in Virtual Reality

Tugce Akkoc, Emre Ugur, Inci Ayhan

- retweets: 5, favorites: 75 (07/29/2020 08:10:33)

- links: [abs](https://arxiv.org/abs/2007.13048) | [pdf](https://arxiv.org/pdf/2007.13048)
- [cs.HC](https://arxiv.org/list/cs.HC/recent)

In immersive Virtual Reality (VR), your brain can trick you into believing that your virtual hands are your real hands. Manipulating the representation of the body, namely the avatar, is a potentially powerful tool for the design of innovative interactive systems in VR. In this study, we investigated interactive behavior in VR by using the methods of experimental psychology. Objects with handles are known to potentiate the afforded action. Participants tend to respond faster when the handle is on the same side as the responding hand in bi-manual speed response tasks. In the first experiment, we successfully replicated this affordance effect in a Virtual Reality (VR) setting. In the second experiment, we showed that the affordance effect was influenced by the avatar, which was manipulated by two different hand types: 1) hand models with full finger tracking that are able to grasp objects, and 2) capsule-shaped -- fingerless -- hand models that are not able to grasp objects. We found that less than 5 minutes of adaptation to an avatar, significantly altered the affordance perception. Counter intuitively, action planning was significantly shorter with the hand model that is not able to grasp. Possibly, fewer action possibilities provided an advantage in processing time. The presence of a handle speeded up the initiation of the hand movement but slowed down the action completion because of ongoing action planning. The results were examined from a multidisciplinary perspective and the design implications for VR applications were discussed.

<blockquote class="twitter-tweet"><p lang="tr" dir="ltr">Bir gÃ¼zel haber daha ğŸ“¢: Emre Hoca (<a href="https://twitter.com/emreugur__?ref_src=twsrc%5Etfw">@emreugur__</a>)  ile beraber sÃ¼pervize ettiÄŸimiz Ã¶ÄŸrencimiz TuÄŸÃ§e AkkoÃ§&#39;un â€œTrick the Body Trick the Mind: Avatar representation affects the perception of available action possibilities in <a href="https://twitter.com/hashtag/VirtualReality?src=hash&amp;ref_src=twsrc%5Etfw">#VirtualReality</a>â€ makalesi online: <a href="https://t.co/t2zTSwDLD8">https://t.co/t2zTSwDLD8</a> <a href="https://t.co/ESdbqKjlCX">pic.twitter.com/ESdbqKjlCX</a></p>&mdash; Ä°nci Ayhan (@nciAyhan3) <a href="https://twitter.com/nciAyhan3/status/1288033761230311429?ref_src=twsrc%5Etfw">July 28, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 21. Few-shot Knowledge Transfer for Fine-grained Cartoon Face Generation

Nan Zhuang, Cheng Yang

- retweets: 15, favorites: 61 (07/29/2020 08:10:33)

- links: [abs](https://arxiv.org/abs/2007.13332) | [pdf](https://arxiv.org/pdf/2007.13332)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

In this paper, we are interested in generating fine-grained cartoon faces for various groups. We assume that one of these groups consists of sufficient training data while the others only contain few samples. Although the cartoon faces of these groups share similar style, the appearances in various groups could still have some specific characteristics, which makes them differ from each other. A major challenge of this task is how to transfer knowledge among groups and learn group-specific characteristics with only few samples. In order to solve this problem, we propose a two-stage training process. First, a basic translation model for the basic group (which consists of sufficient data) is trained. Then, given new samples of other groups, we extend the basic model by creating group-specific branches for each new group. Group-specific branches are updated directly to capture specific appearances for each group while the remaining group-shared parameters are updated indirectly to maintain the distribution of intermediate feature space. In this manner, our approach is capable to generate high-quality cartoon faces for various groups.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Few-shot Knowledge Transfer for Fine-grained Cartoon Face Generation<br>pdf: <a href="https://t.co/1JTZ0V3KrU">https://t.co/1JTZ0V3KrU</a><br>abs: <a href="https://t.co/x2vDNj2h2d">https://t.co/x2vDNj2h2d</a> <a href="https://t.co/6fUAVUyWf8">pic.twitter.com/6fUAVUyWf8</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1287931048903487490?ref_src=twsrc%5Etfw">July 28, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 22. Statistical Bootstrapping for Uncertainty Estimation in Off-Policy  Evaluation

Ilya Kostrikov, Ofir Nachum

- retweets: 5, favorites: 59 (07/29/2020 08:10:33)

- links: [abs](https://arxiv.org/abs/2007.13609) | [pdf](https://arxiv.org/pdf/2007.13609)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

In reinforcement learning, it is typical to use the empirically observed transitions and rewards to estimate the value of a policy via either model-based or Q-fitting approaches. Although straightforward, these techniques in general yield biased estimates of the true value of the policy. In this work, we investigate the potential for statistical bootstrapping to be used as a way to take these biased estimates and produce calibrated confidence intervals for the true value of the policy. We identify conditions - specifically, sufficient data size and sufficient coverage - under which statistical bootstrapping in this setting is guaranteed to yield correct confidence intervals. In practical situations, these conditions often do not hold, and so we discuss and propose mechanisms that can be employed to mitigate their effects. We evaluate our proposed method and show that it can yield accurate confidence intervals in a variety of conditions, including challenging continuous control environments and small data regimes.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Many times in RL, people appeal to notions of statistical bootstrapping to motivate use of ensembles for uncertainty estimation. <a href="https://twitter.com/ikostrikov?ref_src=twsrc%5Etfw">@ikostrikov</a> &amp; I were surprised to find that conditions typically needed for bootstrap are actually almost never present in RL.. <a href="https://t.co/Iz2kT1LsmE">https://t.co/Iz2kT1LsmE</a> <a href="https://t.co/eXQnpb2ayR">pic.twitter.com/eXQnpb2ayR</a></p>&mdash; Ofir Nachum (@ofirnachum) <a href="https://twitter.com/ofirnachum/status/1288121459647209472?ref_src=twsrc%5Etfw">July 28, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



