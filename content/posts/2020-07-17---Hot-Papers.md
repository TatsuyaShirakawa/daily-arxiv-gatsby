---
title: Hot Papers 2020-07-17
date: 2020-07-18T05:56:48.Z
template: "post"
draft: false
slug: "hot-papers-2020-07-17"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-07-17"
socialImage: "/media/42-line-bible.jpg"

---

# 1. Implicit Mesh Reconstruction from Unannotated Image Collections

Shubham Tulsiani, Nilesh Kulkarni, Abhinav Gupta

- retweets: 50, favorites: 224 (07/18/2020 05:56:48)

- links: [abs](https://arxiv.org/abs/2007.08504) | [pdf](https://arxiv.org/pdf/2007.08504)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

We present an approach to infer the 3D shape, texture, and camera pose for an object from a single RGB image, using only category-level image collections with foreground masks as supervision. We represent the shape as an image-conditioned implicit function that transforms the surface of a sphere to that of the predicted mesh, while additionally predicting the corresponding texture. To derive supervisory signal for learning, we enforce that: a) our predictions when rendered should explain the available image evidence, and b) the inferred 3D structure should be geometrically consistent with learned pixel to surface mappings. We empirically show that our approach improves over prior work that leverages similar supervision, and in fact performs competitively to methods that use stronger supervision. Finally, as our method enables learning with limited supervision, we qualitatively demonstrate its applicability over a set of about 30 object categories.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Implicit Mesh Reconstruction from Unannotated Image Collections<br>pdf: <a href="https://t.co/VhxqTW2wa8">https://t.co/VhxqTW2wa8</a><br>abs: <a href="https://t.co/iTZ12nuAdl">https://t.co/iTZ12nuAdl</a><br>project page: <a href="https://t.co/cqx7EPYYrO">https://t.co/cqx7EPYYrO</a><br>video: <a href="https://t.co/nbQwE6kIpq">https://t.co/nbQwE6kIpq</a> <a href="https://t.co/bB1bgcAwG9">pic.twitter.com/bB1bgcAwG9</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1283938642818469889?ref_src=twsrc%5Etfw">July 17, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Implicit Mesh Reconstruction from Unannotated Image Collections. Nice 3D recovery with a clever parameterization of shape. <a href="https://t.co/DO162A0aow">https://t.co/DO162A0aow</a> <a href="https://twitter.com/hashtag/robotics?src=hash&amp;ref_src=twsrc%5Etfw">#robotics</a> <a href="https://twitter.com/hashtag/computervision?src=hash&amp;ref_src=twsrc%5Etfw">#computervision</a> <a href="https://t.co/rTQOmrZISF">pic.twitter.com/rTQOmrZISF</a></p>&mdash; Tomasz Malisiewicz (@quantombone) <a href="https://twitter.com/quantombone/status/1283938772141268993?ref_src=twsrc%5Etfw">July 17, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. OrbNet: Deep Learning for Quantum Chemistry Using Symmetry-Adapted  Atomic-Orbital Features

Zhuoran Qiao, Matthew Welborn, Animashree Anandkumar, Frederick R. Manby, Thomas F. Miller III

- retweets: 51, favorites: 171 (07/18/2020 05:56:49)

- links: [abs](https://arxiv.org/abs/2007.08026) | [pdf](https://arxiv.org/pdf/2007.08026)
- [physics.chem-ph](https://arxiv.org/list/physics.chem-ph/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

We introduce a machine learning method in which energy solutions from the Schrodinger equation are predicted using symmetry adapted atomic orbitals features and a graph neural-network architecture. \textsc{OrbNet} is shown to outperform existing methods in terms of learning efficiency and transferability for the prediction of density functional theory results while employing low-cost features that are obtained from semi-empirical electronic structure calculations. For applications to datasets of drug-like molecules, including QM7b-T, QM9, GDB-13-T, DrugBank, and the conformer benchmark dataset of Folmsbee and Hutchison, \textsc{OrbNet} predicts energies within chemical accuracy of DFT at a computational cost that is thousand-fold or more reduced.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Introducing OrbNet!  <a href="https://twitter.com/hashtag/MachineLearning?src=hash&amp;ref_src=twsrc%5Etfw">#MachineLearning</a> GNN for first-principles <a href="https://twitter.com/hashtag/compchem?src=hash&amp;ref_src=twsrc%5Etfw">#compchem</a> at semi-empirical cost with excellent chemical transferability.<br><br>Awesome collaboration with <a href="https://twitter.com/AnimaAnandkumar?ref_src=twsrc%5Etfw">@AnimaAnandkumar</a>, <a href="https://twitter.com/EntosAI?ref_src=twsrc%5Etfw">@EntosAI</a>, and <a href="https://twitter.com/Caltech?ref_src=twsrc%5Etfw">@Caltech</a>. Congrats <a href="https://twitter.com/ZhuoranQ?ref_src=twsrc%5Etfw">@ZhuoranQ</a> and <a href="https://twitter.com/mattgwelborn?ref_src=twsrc%5Etfw">@mattgwelborn</a>!<a href="https://t.co/7y26tG51Xe">https://t.co/7y26tG51Xe</a> <a href="https://t.co/TcH6uneHPj">pic.twitter.com/TcH6uneHPj</a></p>&mdash; Thomas Miller (@tfmiller3) <a href="https://twitter.com/tfmiller3/status/1283928561166905344?ref_src=twsrc%5Etfw">July 17, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Distributed Reinforcement Learning of Targeted Grasping with Active  Vision for Mobile Manipulators

Yasuhiro Fujita, Kota Uenishi, Avinash Ummadisingu, Prabhat Nagarajan, Shimpei Masuda, Mario Ynocente Castro

- retweets: 46, favorites: 142 (07/18/2020 05:56:49)

- links: [abs](https://arxiv.org/abs/2007.08082) | [pdf](https://arxiv.org/pdf/2007.08082)
- [cs.RO](https://arxiv.org/list/cs.RO/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.DC](https://arxiv.org/list/cs.DC/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Developing personal robots that can perform a diverse range of manipulation tasks in unstructured environments necessitates solving several challenges for robotic grasping systems. We take a step towards this broader goal by presenting the first RL-based system, to our knowledge, for a mobile manipulator that can (a) achieve targeted grasping generalizing to unseen target objects, (b) learn complex grasping strategies for cluttered scenes with occluded objects, and (c) perform active vision through its movable wrist camera to better locate objects. The system is informed of the desired target object in the form of a single, arbitrary-pose RGB image of that object, enabling the system to generalize to unseen objects without retraining. To achieve such a system, we combine several advances in deep reinforcement learning and present a large-scale distributed training system using synchronous SGD that seamlessly scales to multi-node, multi-GPU infrastructure to make rapid prototyping easier. We train and evaluate our system in a simulated environment, identify key components for improving performance, analyze its behaviors, and transfer to a real-world setup.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">PFN RL チームの成果。モバイルマニピュレーターロボット（HSR）を使い、環境固定でなく手首につけたカメラを使って画像で指定された物体の把持をシミュレーター上の分散強化学習システムで学習。把持前動作や見失った物体を探す能動視覚を自動で獲得 <a href="https://t.co/xhMXGjbY6H">https://t.co/xhMXGjbY6H</a> <a href="https://t.co/H3tBtMI4wc">https://t.co/H3tBtMI4wc</a></p>&mdash; Daisuke Okanohara (@hillbig) <a href="https://twitter.com/hillbig/status/1283994497341288452?ref_src=twsrc%5Etfw">July 17, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Our new work is accepted at IROS 2020!<br><br>Distributed Reinforcement Learning of Targeted Grasping with Active Vision for Mobile Manipulators.<br><br>Paper: <a href="https://t.co/FLndJD9Du1">https://t.co/FLndJD9Du1</a><a href="https://t.co/PPU6DSMovj">https://t.co/PPU6DSMovj</a></p>&mdash; mooopan (@mooopan) <a href="https://twitter.com/mooopan/status/1283949972254126081?ref_src=twsrc%5Etfw">July 17, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Accelerating 3D Deep Learning with PyTorch3D

Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, Georgia Gkioxari

- retweets: 48, favorites: 112 (07/18/2020 05:56:49)

- links: [abs](https://arxiv.org/abs/2007.08501) | [pdf](https://arxiv.org/pdf/2007.08501)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.GR](https://arxiv.org/list/cs.GR/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Deep learning has significantly improved 2D image recognition. Extending into 3D may advance many new applications including autonomous vehicles, virtual and augmented reality, authoring 3D content, and even improving 2D recognition. However despite growing interest, 3D deep learning remains relatively underexplored. We believe that some of this disparity is due to the engineering challenges involved in 3D deep learning, such as efficiently processing heterogeneous data and reframing graphics operations to be differentiable. We address these challenges by introducing PyTorch3D, a library of modular, efficient, and differentiable operators for 3D deep learning. It includes a fast, modular differentiable renderer for meshes and point clouds, enabling analysis-by-synthesis approaches. Compared with other differentiable renderers, PyTorch3D is more modular and efficient, allowing users to more easily extend it while also gracefully scaling to large meshes and images. We compare the PyTorch3D operators and renderer with other implementations and demonstrate significant speed and memory improvements. We also use PyTorch3D to improve the state-of-the-art for unsupervised 3D mesh and point cloud prediction from 2D images on ShapeNet. PyTorch3D is open-source and we hope it will help accelerate research in 3D deep learning.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Accelerating 3D Deep Learning with PyTorch3D<br>pdf: <a href="https://t.co/g2RaekzuBM">https://t.co/g2RaekzuBM</a><br>abs: <a href="https://t.co/bDPpOYAM1k">https://t.co/bDPpOYAM1k</a> <a href="https://t.co/avec7uiFKY">pic.twitter.com/avec7uiFKY</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1283957410638508033?ref_src=twsrc%5Etfw">July 17, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, Georgia Gkioxari, Accelerating 3D Deep Learning with PyTorch3D<a href="https://t.co/JHQP5tmCbL">https://t.co/JHQP5tmCbL</a> <a href="https://t.co/l4L1K6Fdm2">pic.twitter.com/l4L1K6Fdm2</a></p>&mdash; Kosta Derpanis (@CSProfKGD) <a href="https://twitter.com/CSProfKGD/status/1284141584934752256?ref_src=twsrc%5Etfw">July 17, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Quantum algorithms for graph problems with cut queries

Troy Lee, Miklos Santha, Shengyu Zhang

- retweets: 14, favorites: 58 (07/18/2020 05:56:49)

- links: [abs](https://arxiv.org/abs/2007.08285) | [pdf](https://arxiv.org/pdf/2007.08285)
- [cs.DS](https://arxiv.org/list/cs.DS/recent) | [quant-ph](https://arxiv.org/list/quant-ph/recent)

Let $G$ be an $n$-vertex graph with $m$ edges. When asked a subset $S$ of vertices, a cut query on $G$ returns the number of edges of $G$ that have exactly one endpoint in $S$. We show that there is a bounded-error quantum algorithm that determines all connected components of $G$ after making $O(\log(n)^5)$ many cut queries. In contrast, it follows from results in communication complexity that any randomized algorithm even just to decide whether the graph is connected or not must make at least $\Omega(n/\log(n))$ many cut queries. We further show that with $O(\log(n)^7)$ many cut queries a quantum algorithm can with high probability output a spanning forest for $G$.   En route to proving these results, we design quantum algorithms for learning a graph using cut queries. We show that a quantum algorithm can learn a graph with maximum degree $d$ after $O(d \log(n)^2)$ many cut queries, and can learn a general graph with $O(\sqrt{m} \log(n)^{3/2})$ many cut queries. These two upper bounds are tight up to the poly-logarithmic factors, and compare to $\Omega(dn)$ and $\Omega(m/\log(n))$ lower bounds on the number of cut queries needed by a randomized algorithm for the same problems, respectively.   The key ingredients in our results are the Bernstein-Vazirani algorithm, approximate counting with "OR queries", and learning sparse vectors from inner products as in compressed sensing.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Awesome new result from Troy Lee, Miklos Santha, and Shengyu Zhang gives an exponential quantum speedup for graph connectivity problems using cut queries. <a href="https://t.co/XXOcxhqBYt">https://t.co/XXOcxhqBYt</a></p>&mdash; Andrew Childs (@andrewmchilds) <a href="https://twitter.com/andrewmchilds/status/1284115006531960832?ref_src=twsrc%5Etfw">July 17, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. Learning from Noisy Labels with Deep Neural Networks: A Survey

Hwanjun Song, Minseok Kim, Dongmin Park, Jae-Gil Lee

- retweets: 12, favorites: 53 (07/18/2020 05:56:50)

- links: [abs](https://arxiv.org/abs/2007.08199) | [pdf](https://arxiv.org/pdf/2007.08199)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Deep learning has achieved remarkable success in numerous domains with help from large amounts of big data. However, the quality of data labels is a concern because of the lack of high-quality labels in many real-world scenarios. As noisy labels severely degrade the generalization performance of deep neural networks, learning from noisy labels (robust training) is becoming an important task in modern deep learning applications. In this survey, we first describe the problem of learning with label noise from a supervised learning perspective. Next, we provide a comprehensive review of 46 state-of-the-art robust training methods, all of which are categorized into seven groups according to their methodological difference, followed by a systematic comparison of six properties used to evaluate their superiority. Subsequently, we summarize the typically used evaluation methodology, including public noisy datasets and evaluation metrics. Finally, we present several promising research directions that can serve as a guideline for future studies.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Learning from Noisy Labels with Deep Neural<br>Networks: A Survey<a href="https://t.co/jHCS80o1rG">https://t.co/jHCS80o1rG</a> <a href="https://t.co/e9XcAqXpFZ">pic.twitter.com/e9XcAqXpFZ</a></p>&mdash; phalanx (@ZFPhalanx) <a href="https://twitter.com/ZFPhalanx/status/1284001754007212032?ref_src=twsrc%5Etfw">July 17, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. D2D: Learning to find good correspondences for image matching and  manipulation

Olivia Wiles, Sebastien Ehrhardt, Andrew Zisserman

- retweets: 18, favorites: 47 (07/18/2020 05:56:50)

- links: [abs](https://arxiv.org/abs/2007.08480) | [pdf](https://arxiv.org/pdf/2007.08480)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

We propose a new approach to determining correspondences between image pairs under large changes in illumination, viewpoint, context, and material. While most approaches seek to extract a set of reliably detectable regions in each image which are then compared (sparse-to-sparse) using increasingly complicated or specialized pipelines, we propose a simple approach for matching all points between the images (dense-to-dense) and subsequently selecting the best matches. The two key parts of our approach are: (i) to condition the learned features on both images, and (ii) to learn a distinctiveness score which is used to choose the best matches at test time. We demonstrate that our model can be used to achieve state of the art or competitive results on a wide range of tasks: local matching, camera localization, 3D reconstruction, and image stylization.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">D2D: Learning to find good correspondences for image matching and manipulation<a href="https://twitter.com/oliviawiles1?ref_src=twsrc%5Etfw">@oliviawiles1</a>, Sebastien Ehrhardt, Andrew Zisserman, <a href="https://twitter.com/Oxford_VGG?ref_src=twsrc%5Etfw">@Oxford_VGG</a> <br>Idea: extract features conditionally on 2nd image.<br>1/<a href="https://t.co/oMKHaszHMG">https://t.co/oMKHaszHMG</a> <a href="https://t.co/TrPVOj5SUr">pic.twitter.com/TrPVOj5SUr</a></p>&mdash; Dmytro Mishkin (@ducha_aiki) <a href="https://twitter.com/ducha_aiki/status/1284064126050930694?ref_src=twsrc%5Etfw">July 17, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. Xiaomingbot: A Multilingual Robot News Reporter

Runxin Xu, Jun Cao, Mingxuan Wang, Jiaze Chen, Hao Zhou, Ying Zeng, Yuping Wang, Li Chen, Xiang Yin, Xijin Zhang, Songcheng Jiang, Yuxuan Wang, Lei Li

- retweets: 14, favorites: 47 (07/18/2020 05:56:50)

- links: [abs](https://arxiv.org/abs/2007.08005) | [pdf](https://arxiv.org/pdf/2007.08005)
- [eess.AS](https://arxiv.org/list/eess.AS/recent) | [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.SD](https://arxiv.org/list/cs.SD/recent)

This paper proposes the building of Xiaomingbot, an intelligent, multilingual and multimodal software robot equipped with four integral capabilities: news generation, news translation, news reading and avatar animation. Its system summarizes Chinese news that it automatically generates from data tables. Next, it translates the summary or the full article into multiple languages, and reads the multilingual rendition through synthesized speech. Notably, Xiaomingbot utilizes a voice cloning technology to synthesize the speech trained from a real person's voice data in one input language. The proposed system enjoys several merits: it has an animated avatar, and is able to generate and read multilingual news. Since it was put into practice, Xiaomingbot has written over 600,000 articles, and gained over 150,000 followers on social media platforms.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Xiaomingbot: A Multilingual Robot News Reporter<br>pdf: <a href="https://t.co/RO4lJPCXer">https://t.co/RO4lJPCXer</a><br>abs: <a href="https://t.co/RhMG9uo8qV">https://t.co/RhMG9uo8qV</a><br>project page: <a href="https://t.co/NwDLvVT3JS">https://t.co/NwDLvVT3JS</a> <a href="https://t.co/VjNISCZewx">pic.twitter.com/VjNISCZewx</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1283953990842163202?ref_src=twsrc%5Etfw">July 17, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. Controllable Image Synthesis via SegVAE

Yen-Chi Cheng, Hsin-Ying Lee, Min Sun, Ming-Hsuan Yang

- retweets: 9, favorites: 41 (07/18/2020 05:56:50)

- links: [abs](https://arxiv.org/abs/2007.08397) | [pdf](https://arxiv.org/pdf/2007.08397)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Flexible user controls are desirable for content creation and image editing. A semantic map is commonly used intermediate representation for conditional image generation. Compared to the operation on raw RGB pixels, the semantic map enables simpler user modification. In this work, we specifically target at generating semantic maps given a label-set consisting of desired categories. The proposed framework, SegVAE, synthesizes semantic maps in an iterative manner using conditional variational autoencoder. Quantitative and qualitative experiments demonstrate that the proposed model can generate realistic and diverse semantic maps. We also apply an off-the-shelf image-to-image translation model to generate realistic RGB images to better understand the quality of the synthesized semantic maps. Furthermore, we showcase several real-world image-editing applications including object removal, object insertion, and object replacement.

<blockquote class="twitter-tweet"><p lang="pt" dir="ltr">Controllable Image Synthesis via SegVAE<br>pdf: <a href="https://t.co/j0k7tQn0dZ">https://t.co/j0k7tQn0dZ</a><br>abs: <a href="https://t.co/8xZKTeOnmz">https://t.co/8xZKTeOnmz</a> <a href="https://t.co/L4WOLbUfHH">pic.twitter.com/L4WOLbUfHH</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1283936521234636802?ref_src=twsrc%5Etfw">July 17, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



