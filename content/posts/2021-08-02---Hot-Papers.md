---
title: Hot Papers 2021-08-02
date: 2021-08-03T08:14:19.Z
template: "post"
draft: false
slug: "hot-papers-2021-08-02"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2021-08-02"
socialImage: "/media/flying-marine.jpg"

---

# 1. Perceiver IO: A General Architecture for Structured Inputs & Outputs

Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Andrew Brock, Evan Shelhamer, Olivier H√©naff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, Jo√£o Carreira

- retweets: 1056, favorites: 186 (08/03/2021 08:14:19)

- links: [abs](https://arxiv.org/abs/2107.14795) | [pdf](https://arxiv.org/pdf/2107.14795)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.SD](https://arxiv.org/list/cs.SD/recent) | [eess.AS](https://arxiv.org/list/eess.AS/recent)

The recently-proposed Perceiver model obtains good results on several domains (images, audio, multimodal, point clouds) while scaling linearly in compute and memory with the input size. While the Perceiver supports many kinds of inputs, it can only produce very simple outputs such as class scores. Perceiver IO overcomes this limitation without sacrificing the original's appealing properties by learning to flexibly query the model's latent space to produce outputs of arbitrary size and semantics. Perceiver IO still decouples model depth from data size and still scales linearly with data size, but now with respect to both input and output sizes. The full Perceiver IO model achieves strong results on tasks with highly structured output spaces, such as natural language and visual understanding, StarCraft II, and multi-task and multi-modal domains. As highlights, Perceiver IO matches a Transformer-based BERT baseline on the GLUE language benchmark without the need for input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Perceiver IO: A General Architecture for Structured Inputs &amp; Outputs<br>paper: <a href="https://t.co/iEeOKVI1OJ">https://t.co/iEeOKVI1OJ</a><br><br>matches a Transformer-based BERT baseline on the GLUE language benchmark without the need for input tokenization and achieves sota performance on Sintel<br>optical flow estimation <a href="https://t.co/2VF2AEAFfT">pic.twitter.com/2VF2AEAFfT</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1421993124482764802?ref_src=twsrc%5Etfw">August 2, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Measuring Disagreement in Science

Wout S. Lamers, Kevin Boyack, Vincent Larivi√®re, Cassidy R. Sugimoto, Nees Jan van Eck, Ludo Waltman, Dakota Murray

- retweets: 372, favorites: 54 (08/03/2021 08:14:19)

- links: [abs](https://arxiv.org/abs/2107.14641) | [pdf](https://arxiv.org/pdf/2107.14641)
- [cs.DL](https://arxiv.org/list/cs.DL/recent) | [cs.CL](https://arxiv.org/list/cs.CL/recent)

Disagreement is essential to scientific progress. However, the extent of disagreement in science, its evolution over time, and the fields in which it happens, remains largely unknown. Leveraging a massive collection of scientific texts, we develop a cue-phrase based approach to identify instances of disagreement citations across more than four million scientific articles. Using this method, we construct an indicator of disagreement across scientific fields over the 2000-2015 period. In contrast with black-box text classification methods, our framework is transparent and easily interpretable. We reveal a disciplinary spectrum of disagreement, with higher disagreement in the social sciences and lower disagreement in physics and mathematics. However, detailed disciplinary analysis demonstrates heterogeneity across sub-fields, revealing the importance of local disciplinary cultures and epistemic characteristics of disagreement. Paper-level analysis reveals notable episodes of disagreement in science, and illustrates how methodological artefacts can confound analyses of scientific texts. These findings contribute to a broader understanding of disagreement and establish a foundation for future research to understanding key processes underlying scientific progress.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">At long last, our manuscript Measuring Disagreement in Science is up on arXiv: <a href="https://t.co/Ao4CqEhJ42">https://t.co/Ao4CqEhJ42</a><br>Thanks to my coauthors <a href="https://twitter.com/dakotasmurray?ref_src=twsrc%5Etfw">@dakotasmurray</a> <a href="https://twitter.com/KevinBoyack?ref_src=twsrc%5Etfw">@KevinBoyack</a> <a href="https://twitter.com/lariviev?ref_src=twsrc%5Etfw">@lariviev</a> <a href="https://twitter.com/csugimoto?ref_src=twsrc%5Etfw">@csugimoto</a> <a href="https://twitter.com/neesjanvaneck?ref_src=twsrc%5Etfw">@neesjanvaneck</a> <a href="https://twitter.com/LudoWaltman?ref_src=twsrc%5Etfw">@LudoWaltman</a> <br>Next stop, journal submission!</p>&mdash; Wout Lamers (@WoutLamers) <a href="https://twitter.com/WoutLamers/status/1422105234638348290?ref_src=twsrc%5Etfw">August 2, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Decentralized Basic Income: Creating Wealth with On-Chain Staking and  Fixed-Rate Protocols

Hakwan Lau, Stephen Tse

- retweets: 143, favorites: 69 (08/03/2021 08:14:20)

- links: [abs](https://arxiv.org/abs/2107.14312) | [pdf](https://arxiv.org/pdf/2107.14312)
- [cs.CY](https://arxiv.org/list/cs.CY/recent)

In this review, we evaluate the mechanisms behind the decentralized finance protocols for generating stable, passive income. Currently, such savings interest rates can be as high as 20% annually, payable in traditional currency values such as US dollars. Therefore, one can benefit from the growth of the cryptocurrency markets, with minimal exposure to their volatility risks. We aim to explain the rationale behind these savings products in simple terms. The key to this puzzle is that asset deposits in cryptocurrency ecosystems are of intrinsic economic value, as they facilitate network consensus mechanisms and automated marketplaces (e.g. for lending). These functions create wealth for the participants, and they provide unique advantages unavailable in traditional financial systems. Our review speaks to the notion of decentralized basic income - analogous to universal basic income but guaranteed by financial products on blockchains instead of public policies. We will go through their implementations of how savings can be channeled into the staking deposits in Proof-of-Stake (PoS) protocols, through fixed-rate lending protocols and staking derivative tokens, thereby exposing savers with minimal risks. We will discuss potential pitfalls, assess how these protocols may behave in market cycles, as well as suggest areas for further research and development.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I‚Äôm incredibly honored to be involved with this radical project. Generating decentralized, stable, and passive basic income payable in stable currencies. Using novel Proof of Stake protocols to generate up to 20% APY! Creating wealth together in <a href="https://twitter.com/hashtag/harmony?src=hash&amp;ref_src=twsrc%5Etfw">#harmony</a> - <a href="https://t.co/kHPZbQofKI">https://t.co/kHPZbQofKI</a></p>&mdash; Giv Parvaneh | giv.one (@givp) <a href="https://twitter.com/givp/status/1422055682325372931?ref_src=twsrc%5Etfw">August 2, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. EmailSum: Abstractive Email Thread Summarization

Shiyue Zhang, Asli Celikyilmaz, Jianfeng Gao, Mohit Bansal

- retweets: 84, favorites: 87 (08/03/2021 08:14:20)

- links: [abs](https://arxiv.org/abs/2107.14691) | [pdf](https://arxiv.org/pdf/2107.14691)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

Recent years have brought about an interest in the challenging task of summarizing conversation threads (meetings, online discussions, etc.). Such summaries help analysis of the long text to quickly catch up with the decisions made and thus improve our work or communication efficiency. To spur research in thread summarization, we have developed an abstractive Email Thread Summarization (EmailSum) dataset, which contains human-annotated short (<30 words) and long (<100 words) summaries of 2549 email threads (each containing 3 to 10 emails) over a wide variety of topics. We perform a comprehensive empirical study to explore different summarization techniques (including extractive and abstractive methods, single-document and hierarchical models, as well as transfer and semisupervised learning) and conduct human evaluations on both short and long summary generation tasks. Our results reveal the key challenges of current abstractive summarization models in this task, such as understanding the sender's intent and identifying the roles of sender and receiver. Furthermore, we find that widely used automatic evaluation metrics (ROUGE, BERTScore) are weakly correlated with human judgments on this email thread summarization task. Hence, we emphasize the importance of human evaluation and the development of better metrics by the community. Our code and summary data have been made available at: https://github.com/ZhangShiyue/EmailSum

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Hi <a href="https://twitter.com/hashtag/ACL2021nlp?src=hash&amp;ref_src=twsrc%5Etfw">#ACL2021nlp</a>, check out our new work ‚ÄúEmailSum: Abstractive Email Thread Summarization‚Äù (11-12p ET Aug4 Wed 15D). üòÄ<br><br>w <a href="https://twitter.com/real_asli?ref_src=twsrc%5Etfw">@real_Asli</a> <a href="https://twitter.com/JianfengGao0217?ref_src=twsrc%5Etfw">@JianfengGao0217</a> <a href="https://twitter.com/mohitban47?ref_src=twsrc%5Etfw">@MohitBan47</a> (<a href="https://twitter.com/uncnlp?ref_src=twsrc%5Etfw">@uncnlp</a>+<a href="https://twitter.com/MSFTResearch?ref_src=twsrc%5Etfw">@MSFTResearch</a>)<a href="https://t.co/estXVP6b7N">https://t.co/estXVP6b7N</a><br>Github: <a href="https://t.co/tBQJ2Nyru7">https://t.co/tBQJ2Nyru7</a> <br>Video: <a href="https://t.co/enSOSBYpXV">https://t.co/enSOSBYpXV</a><br><br>üßµ‚¨áÔ∏è <a href="https://t.co/tPKTRoRa2p">pic.twitter.com/tPKTRoRa2p</a></p>&mdash; Shiyue Zhang (@byryuer) <a href="https://twitter.com/byryuer/status/1421999972212297730?ref_src=twsrc%5Etfw">August 2, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">EmailSum: Abstractive Email Thread Summarization<br>pdf: <a href="https://t.co/R6BvJCV84B">https://t.co/R6BvJCV84B</a><br>abs: <a href="https://t.co/fXpX3mR9iD">https://t.co/fXpX3mR9iD</a><br><br>propose an abstractive email thread summarization dataset, EMAILSUM, that contains 2,549 email threads with human-written short and long summaries <a href="https://t.co/JcY74l6ixd">pic.twitter.com/JcY74l6ixd</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1422002003941773316?ref_src=twsrc%5Etfw">August 2, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Relightable Neural Video Portrait

Youjia Wang, Taotao Zhou, Minzhang Li, Teng Xu, Minye Wu, Lan Xu, Jingyi Yu

- retweets: 64, favorites: 50 (08/03/2021 08:14:20)

- links: [abs](https://arxiv.org/abs/2107.14735) | [pdf](https://arxiv.org/pdf/2107.14735)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.GR](https://arxiv.org/list/cs.GR/recent)

Photo-realistic facial video portrait reenactment benefits virtual production and numerous VR/AR experiences. The task remains challenging as the portrait should maintain high realism and consistency with the target environment. In this paper, we present a relightable neural video portrait, a simultaneous relighting and reenactment scheme that transfers the head pose and facial expressions from a source actor to a portrait video of a target actor with arbitrary new backgrounds and lighting conditions. Our approach combines 4D reflectance field learning, model-based facial performance capture and target-aware neural rendering. Specifically, we adopt a rendering-to-video translation network to first synthesize high-quality OLAT imagesets and alpha mattes from hybrid facial performance capture results. We then design a semantic-aware facial normalization scheme to enable reliable explicit control as well as a multi-frame multi-task learning strategy to encode content, segmentation and temporal information simultaneously for high-quality reflectance field inference. After training, our approach further enables photo-realistic and controllable video portrait editing of the target performer. Reliable face poses and expression editing is obtained by applying the same hybrid facial capture and normalization scheme to the source video input, while our explicit alpha and OLAT output enable high-quality relit and background editing. With the ability to achieve simultaneous relighting and reenactment, we are able to improve the realism in a variety of virtual production and video rewrite applications.

<blockquote class="twitter-tweet"><p lang="fr" dir="ltr">Relightable Neural Video Portrait<br>pdf: <a href="https://t.co/YoMeLnjECJ">https://t.co/YoMeLnjECJ</a><br>abs: <a href="https://t.co/rtrFGoDn6I">https://t.co/rtrFGoDn6I</a> <a href="https://t.co/3gKvEkmrt6">pic.twitter.com/3gKvEkmrt6</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1422023305926791168?ref_src=twsrc%5Etfw">August 2, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. DadaGP: A Dataset of Tokenized GuitarPro Songs for Sequence Models

Pedro Sarmento, Adarsh Kumar, CJ Carr, Zack Zukowski, Mathieu Barthet, Yi-Hsuan Yang

- retweets: 56, favorites: 50 (08/03/2021 08:14:20)

- links: [abs](https://arxiv.org/abs/2107.14653) | [pdf](https://arxiv.org/pdf/2107.14653)
- [cs.SD](https://arxiv.org/list/cs.SD/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [eess.AS](https://arxiv.org/list/eess.AS/recent)

Originating in the Renaissance and burgeoning in the digital era, tablatures are a commonly used music notation system which provides explicit representations of instrument fingerings rather than pitches. GuitarPro has established itself as a widely used tablature format and software enabling musicians to edit and share songs for musical practice, learning, and composition. In this work, we present DadaGP, a new symbolic music dataset comprising 26,181 song scores in the GuitarPro format covering 739 musical genres, along with an accompanying tokenized format well-suited for generative sequence models such as the Transformer. The tokenized format is inspired by event-based MIDI encodings, often used in symbolic music generation models. The dataset is released with an encoder/decoder which converts GuitarPro files to tokens and back. We present results of a use case in which DadaGP is used to train a Transformer-based model to generate new songs in GuitarPro format. We discuss other relevant use cases for the dataset (guitar-bass transcription, music style transfer and artist/genre classification) as well as ethical implications. DadaGP opens up the possibility to train GuitarPro score generators, fine-tune models on custom data, create new styles of music, AI-powered songwriting apps, and human-AI improvisation.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">DadaGP: A Dataset of Tokenized GuitarPro Songs for Sequence Models<br>paper: <a href="https://t.co/YTIJQDrSIo">https://t.co/YTIJQDrSIo</a><br><br>symbolic music dataset comprising 26,181 song scores in the GuitarPro format covering 739 musical genres, along with an accompanying tokenized format <a href="https://t.co/pvlh0ySCHt">pic.twitter.com/pvlh0ySCHt</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1422000094681907202?ref_src=twsrc%5Etfw">August 2, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. Distributed Representations of Atoms and Materials for Machine Learning

Luis M. Antunes, Ricardo Grau-Crespo, Keith T. Butler

- retweets: 46, favorites: 49 (08/03/2021 08:14:20)

- links: [abs](https://arxiv.org/abs/2107.14664) | [pdf](https://arxiv.org/pdf/2107.14664)
- [cond-mat.mtrl-sci](https://arxiv.org/list/cond-mat.mtrl-sci/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

The use of machine learning is becoming increasingly common in computational materials science. To build effective models of the chemistry of materials, useful machine-based representations of atoms and their compounds are required. We derive distributed representations of compounds from their chemical formulas only, via pooling operations of distributed representations of atoms. These compound representations are evaluated on ten different tasks, such as the prediction of formation energy and band gap, and are found to be competitive with existing benchmarks that make use of structure, and even superior in cases where only composition is available. Finally, we introduce a new approach for learning distributed representations of atoms, named SkipAtom, which makes use of the growing information in materials structure databases.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">You know an atom by the company it keeps! <a href="https://twitter.com/_lantunes?ref_src=twsrc%5Etfw">@_lantunes</a> has developed a cool new materials&#39; representation for <a href="https://twitter.com/hashtag/machinelearning?src=hash&amp;ref_src=twsrc%5Etfw">#machinelearning</a> insipred by natural language processing.  With <a href="https://twitter.com/rgraucrespo?ref_src=twsrc%5Etfw">@rgraucrespo</a> <a href="https://t.co/OCGg4efTY8">https://t.co/OCGg4efTY8</a> Also a nice repo available if you&#39;d like to try it out. <a href="https://t.co/IIVnLhBpdj">pic.twitter.com/IIVnLhBpdj</a></p>&mdash; Keith Butler (@keeeto2000) <a href="https://twitter.com/keeeto2000/status/1422093882439962624?ref_src=twsrc%5Etfw">August 2, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. Batch Active Learning at Scale

Gui Citovsky, Giulia DeSalvo, Claudio Gentile, Lazaros Karydas, Anand Rajagopalan, Afshin Rostamizadeh, Sanjiv Kumar

- retweets: 36, favorites: 45 (08/03/2021 08:14:20)

- links: [abs](https://arxiv.org/abs/2107.14263) | [pdf](https://arxiv.org/pdf/2107.14263)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

The ability to train complex and highly effective models often requires an abundance of training data, which can easily become a bottleneck in cost, time, and computational resources. Batch active learning, which adaptively issues batched queries to a labeling oracle, is a common approach for addressing this problem. The practical benefits of batch sampling come with the downside of less adaptivity and the risk of sampling redundant examples within a batch -- a risk that grows with the batch size. In this work, we analyze an efficient active learning algorithm, which focuses on the large batch setting. In particular, we show that our sampling method, which combines notions of uncertainty and diversity, easily scales to batch sizes (100K-1M) several orders of magnitude larger than used in previous studies and provides significant improvements in model training efficiency compared to recent baselines. Finally, we provide an initial theoretical analysis, proving label complexity guarantees for a related sampling method, which we show is approximately equivalent to our sampling method in specific settings.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Batch Active Learning at Scale<br>pdf: <a href="https://t.co/07wCo0zp3S">https://t.co/07wCo0zp3S</a><br>abs: <a href="https://t.co/Oac21CLlCf">https://t.co/Oac21CLlCf</a><br><br>show that sampling method, which combines notions of uncertainty and diversity, easily scales to batch sizes (100K-1M) several orders of magnitude larger than used in previous studies <a href="https://t.co/bslt0HZFMf">pic.twitter.com/bslt0HZFMf</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1422043498765750274?ref_src=twsrc%5Etfw">August 2, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. Strategically Efficient Exploration in Competitive Multi-agent  Reinforcement Learning

Robert Loftin, Aadirupa Saha, Sam Devlin, Katja Hofmann

- retweets: 49, favorites: 32 (08/03/2021 08:14:20)

- links: [abs](https://arxiv.org/abs/2107.14698) | [pdf](https://arxiv.org/pdf/2107.14698)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.MA](https://arxiv.org/list/cs.MA/recent)

High sample complexity remains a barrier to the application of reinforcement learning (RL), particularly in multi-agent systems. A large body of work has demonstrated that exploration mechanisms based on the principle of optimism under uncertainty can significantly improve the sample efficiency of RL in single agent tasks. This work seeks to understand the role of optimistic exploration in non-cooperative multi-agent settings. We will show that, in zero-sum games, optimistic exploration can cause the learner to waste time sampling parts of the state space that are irrelevant to strategic play, as they can only be reached through cooperation between both players. To address this issue, we introduce a formal notion of strategically efficient exploration in Markov games, and use this to develop two strategically efficient learning algorithms for finite Markov games. We demonstrate that these methods can be significantly more sample efficient than their optimistic counterparts.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Strategically Efficient Exploration in Competitive Multi-agent Reinforcement Learning<br>pdf: <a href="https://t.co/xSvF7XF8wo">https://t.co/xSvF7XF8wo</a><br>abs: <a href="https://t.co/eBQjTnkGzq">https://t.co/eBQjTnkGzq</a><br>github: <a href="https://t.co/zf7i4vQI6W">https://t.co/zf7i4vQI6W</a> <a href="https://t.co/jXIeReCcDg">pic.twitter.com/jXIeReCcDg</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1422018886237573120?ref_src=twsrc%5Etfw">August 2, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



