---
title: Hot Papers 2021-05-17
date: 2021-05-18T08:47:47.Z
template: "post"
draft: false
slug: "hot-papers-2021-05-17"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2021-05-17"
socialImage: "/media/flying-marine.jpg"

---

# 1. Omnimatte: Associating Objects and Their Effects in Video

Erika Lu, Forrester Cole, Tali Dekel, Andrew Zisserman, William T. Freeman, Michael Rubinstein

- retweets: 11040, favorites: 0 (05/18/2021 08:47:47)

- links: [abs](https://arxiv.org/abs/2105.06993) | [pdf](https://arxiv.org/pdf/2105.06993)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Computer vision is increasingly effective at segmenting objects in images and videos; however, scene effects related to the objects---shadows, reflections, generated smoke, etc---are typically overlooked. Identifying such scene effects and associating them with the objects producing them is important for improving our fundamental understanding of visual scenes, and can also assist a variety of applications such as removing, duplicating, or enhancing objects in video. In this work, we take a step towards solving this novel problem of automatically associating objects with their effects in video. Given an ordinary video and a rough segmentation mask over time of one or more subjects of interest, we estimate an omnimatte for each subject---an alpha matte and color image that includes the subject along with all its related time-varying scene elements. Our model is trained only on the input video in a self-supervised manner, without any manual labels, and is generic---it produces omnimattes automatically for arbitrary objects and a variety of effects. We show results on real-world videos containing interactions between different types of subjects (cars, animals, people) and complex effects, ranging from semi-transparent elements such as smoke and reflections, to fully opaque effects such as objects attached to the subject.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Omnimatte: Associating Objects and Their Effects in Video<br>pdf: <a href="https://t.co/z1LLzborDK">https://t.co/z1LLzborDK</a><br>abs: <a href="https://t.co/ylCWjre2oh">https://t.co/ylCWjre2oh</a><br>project page: <a href="https://t.co/5m7IsONaEj">https://t.co/5m7IsONaEj</a> <a href="https://t.co/6QMF7r3J3w">pic.twitter.com/6QMF7r3J3w</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1394093891079622658?ref_src=twsrc%5Etfw">May 17, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Priors in Bayesian Deep Learning: A Review

Vincent Fortuin

- retweets: 2134, favorites: 277 (05/18/2021 08:47:47)

- links: [abs](https://arxiv.org/abs/2105.06868) | [pdf](https://arxiv.org/pdf/2105.06868)
- [stat.ML](https://arxiv.org/list/stat.ML/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

While the choice of prior is one of the most critical parts of the Bayesian inference workflow, recent Bayesian deep learning models have often fallen back on uninformative priors, such as standard Gaussians. In this review, we highlight the importance of prior choices for Bayesian deep learning and present an overview of different priors that have been proposed for (deep) Gaussian processes, variational autoencoders, and Bayesian neural networks. We also outline different methods of learning priors for these models from data. We hope to motivate practitioners in Bayesian deep learning to think more carefully about the prior specification for their models and to provide them with some inspiration in this regard.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I&#39;ve been thinking a lot about priors in Bayesian deep learning recently, so I decided to write a little review about the topic: <a href="https://t.co/m1BOj4Tbbk">https://t.co/m1BOj4Tbbk</a><br>Hopefully it can serve as a primer to the field. Please RT and let me know if there are any references you think I&#39;ve missed.</p>&mdash; Vincent Fortuin (@vincefort) <a href="https://twitter.com/vincefort/status/1394214008320888832?ref_src=twsrc%5Etfw">May 17, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Joint Retrieval and Generation Training for Grounded Text Generation

Yizhe Zhang, Siqi Sun, Xiang Gao, Yuwei Fang, Chris Brockett, Michel Galley, Jianfeng Gao, Bill Dolan

- retweets: 1142, favorites: 195 (05/18/2021 08:47:47)

- links: [abs](https://arxiv.org/abs/2105.06597) | [pdf](https://arxiv.org/pdf/2105.06597)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

Recent advances in large-scale pre-training such as GPT-3 allow seemingly high quality text to be generated from a given prompt. However, such generation systems often suffer from problems of hallucinated facts, and are not inherently designed to incorporate useful external information. Grounded generation models appear to offer remedies, but their training typically relies on rarely-available parallel data where corresponding documents are provided for context. We propose a framework that alleviates this data constraint by jointly training a grounded generator and document retriever on the language model signal. The model learns to retrieve the documents with the highest utility in generation and attentively combines them in the output. We demonstrate that by taking advantage of external references our approach can produce more informative and interesting text in both prose and dialogue generation.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Joint Retrieval and Generation Training for Grounded Text Generation<br><br>Proposes a model that retrieves the documents with the highest utility in generation and combines them in the output. <br><br>The model receives higher human rating than human outputs lol<a href="https://t.co/zhnXphtwFp">https://t.co/zhnXphtwFp</a> <a href="https://t.co/PE2bPR11Kk">pic.twitter.com/PE2bPR11Kk</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1394091705377300480?ref_src=twsrc%5Etfw">May 17, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Not All Memories are Created Equal: Learning to Forget by Expiring

Sainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur Szlam, Jason Weston, Angela Fan

- retweets: 156, favorites: 88 (05/18/2021 08:47:48)

- links: [abs](https://arxiv.org/abs/2105.06548) | [pdf](https://arxiv.org/pdf/2105.06548)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

Attention mechanisms have shown promising results in sequence modeling tasks that require long-term memory. Recent work investigated mechanisms to reduce the computational cost of preserving and storing memories. However, not all content in the past is equally important to remember. We propose Expire-Span, a method that learns to retain the most important information and expire the irrelevant information. This forgetting of memories enables Transformers to scale to attend over tens of thousands of previous timesteps efficiently, as not all states from previous timesteps are preserved. We demonstrate that Expire-Span can help models identify and retain critical information and show it can achieve strong performance on reinforcement learning tasks specifically designed to challenge this functionality. Next, we show that Expire-Span can scale to memories that are tens of thousands in size, setting a new state of the art on incredibly long context tasks such as character-level language modeling and a frame-by-frame moving objects task. Finally, we analyze the efficiency of Expire-Span compared to existing approaches and demonstrate that it trains faster and uses less memory.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Not All Memories are Created Equal: Learning to Forget by Expiring<br>pdf: <a href="https://t.co/t3hsYHyQar">https://t.co/t3hsYHyQar</a><br>abs: <a href="https://t.co/lJ0y6Hxa6M">https://t.co/lJ0y6Hxa6M</a><br><br>an operation that can be added to any attention mechanism to enable models to learn what to forget <a href="https://t.co/flFRGpKsdW">pic.twitter.com/flFRGpKsdW</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1394090527080001541?ref_src=twsrc%5Etfw">May 17, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Sketch2Model: View-Aware 3D Modeling from Single Free-Hand Sketches

Song-Hai Zhang, Yuan-Chen Guo, Qing-Wen Gu

- retweets: 42, favorites: 51 (05/18/2021 08:47:48)

- links: [abs](https://arxiv.org/abs/2105.06663) | [pdf](https://arxiv.org/pdf/2105.06663)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

We investigate the problem of generating 3D meshes from single free-hand sketches, aiming at fast 3D modeling for novice users. It can be regarded as a single-view reconstruction problem, but with unique challenges, brought by the variation and conciseness of sketches. Ambiguities in poorly-drawn sketches could make it hard to determine how the sketched object is posed. In this paper, we address the importance of viewpoint specification for overcoming such ambiguities, and propose a novel view-aware generation approach. By explicitly conditioning the generation process on a given viewpoint, our method can generate plausible shapes automatically with predicted viewpoints, or with specified viewpoints to help users better express their intentions. Extensive evaluations on various datasets demonstrate the effectiveness of our view-aware design in solving sketch ambiguities and improving reconstruction quality.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Sketch2Model: View-Aware 3D Modeling from Single Free-Hand Sketches<br>pdf: <a href="https://t.co/xiz20ValzE">https://t.co/xiz20ValzE</a><br>abs: <a href="https://t.co/iAWyP3eSFK">https://t.co/iAWyP3eSFK</a><br><br>method can generate promising shapes on several free-hand sketch datasets <a href="https://t.co/aOWTjNhRPf">pic.twitter.com/aOWTjNhRPf</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1394103787837661189?ref_src=twsrc%5Etfw">May 17, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. QAConv: Question Answering on Informative Conversations

Chien-Sheng Wu, Andrea Madotto, Wenhao Liu, Pascale Fung, Caiming Xiong

- retweets: 14, favorites: 75 (05/18/2021 08:47:48)

- links: [abs](https://arxiv.org/abs/2105.06912) | [pdf](https://arxiv.org/pdf/2105.06912)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.IR](https://arxiv.org/list/cs.IR/recent)

This paper introduces QAConv, a new question answering (QA) dataset that uses conversations as a knowledge source. We focus on informative conversations including business emails, panel discussions, and work channels. Unlike open-domain and task-oriented dialogues, these conversations are usually long, complex, asynchronous, and involve strong domain knowledge. In total, we collect 34,204 QA pairs, including span-based, free-form, and unanswerable questions, from 10,259 selected conversations with both human-written and machine-generated questions. We segment long conversations into chunks, and use a question generator and dialogue summarizer as auxiliary tools to collect multi-hop questions. The dataset has two testing scenarios, chunk mode and full mode, depending on whether the grounded chunk is provided or retrieved from a large conversational pool. Experimental results show that state-of-the-art QA systems trained on existing QA datasets have limited zero-shot ability and tend to predict our questions as unanswerable. Fine-tuning such systems on our corpus can achieve significant improvement up to 23.6% and 13.6% in both chunk mode and full mode, respectively.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Excited to share our new dataset paper &quot;QAConv: Question Answering on Informative Conversations&quot;, where we collect QA pairs for complex conversations including business emails, panel discussions, and work channels! <br><br>Paper: <a href="https://t.co/3hy6c9Kp4x">https://t.co/3hy6c9Kp4x</a><br>Code: <a href="https://t.co/wxkEezcGsu">https://t.co/wxkEezcGsu</a> <a href="https://t.co/XBpBJsNEFy">pic.twitter.com/XBpBJsNEFy</a></p>&mdash; Jason C.S. Wu (@jasonwu0731) <a href="https://twitter.com/jasonwu0731/status/1394104098597654532?ref_src=twsrc%5Etfw">May 17, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. Decision Diagrams for Quantum Measurements with Shallow Circuits

Stefan Hillmich, Charles Hadfield, Rudy Raymond, Antonio Mezzacapo, Robert Wille

- retweets: 42, favorites: 24 (05/18/2021 08:47:48)

- links: [abs](https://arxiv.org/abs/2105.06932) | [pdf](https://arxiv.org/pdf/2105.06932)
- [quant-ph](https://arxiv.org/list/quant-ph/recent) | [cs.DS](https://arxiv.org/list/cs.DS/recent)

We consider the problem of estimating quantum observables on a collection of qubits, given as a linear combination of Pauli operators, with shallow quantum circuits consisting of single-qubit rotations. We introduce estimators based on randomised measurements, which use decision diagrams to sample from probability distributions on measurement bases. This approach generalises previously known uniform and locally-biased randomised estimators. The decision diagrams are constructed given target quantum operators and can be optimised considering different strategies. We show numerically that the estimators introduced here can produce more precise estimates on some quantum chemistry Hamiltonians, compared to previously known randomised protocols and Pauli grouping methods.




# 8. Translating Extensive Form Games to Open Games with Agency

Matteo Capucci, Neil Ghani, Jérémy Ledent, Fredrik Nordvall Forsberg

- retweets: 20, favorites: 41 (05/18/2021 08:47:48)

- links: [abs](https://arxiv.org/abs/2105.06763) | [pdf](https://arxiv.org/pdf/2105.06763)
- [cs.GT](https://arxiv.org/list/cs.GT/recent) | [cs.MA](https://arxiv.org/list/cs.MA/recent) | [math.CT](https://arxiv.org/list/math.CT/recent)

We show open games cover extensive form games with both perfect and imperfect information. Doing so forces us to address two current weaknesses in open games: the lack of a notion of player and their agency within open games, and the lack of choice operators. Using the former we construct the latter, and these choice operators subsume previous proposed operators for open games, thereby making progress towards a core, canonical and ergonomic calculus of game operators. Collectively these innovations increase the level of compositionality of open games, and demonstrate their expressiveness.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">My second first paper is out!<br>It gets around agency problems in open games and exemplifies this by translating classical tree-like denotations of games into open games, inductively<br>JWW <a href="https://twitter.com/Anarchia45?ref_src=twsrc%5Etfw">@Anarchia45</a> <a href="https://twitter.com/f_nf_?ref_src=twsrc%5Etfw">@f_nf_</a> and Jérémy Ledent<a href="https://t.co/BcrT1mIiSl">https://t.co/BcrT1mIiSl</a> <a href="https://t.co/wCe1tjk0HC">pic.twitter.com/wCe1tjk0HC</a></p>&mdash; hom(—, matteo) (@mattecapu) <a href="https://twitter.com/mattecapu/status/1394244062761336832?ref_src=twsrc%5Etfw">May 17, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. Automatic Non-Linear Video Editing Transfer

Nathan Frey, Peggy Chi, Weilong Yang, Irfan Essa

- retweets: 20, favorites: 40 (05/18/2021 08:47:48)

- links: [abs](https://arxiv.org/abs/2105.06988) | [pdf](https://arxiv.org/pdf/2105.06988)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

We propose an automatic approach that extracts editing styles in a source video and applies the edits to matched footage for video creation. Our Computer Vision based techniques considers framing, content type, playback speed, and lighting of each input video segment. By applying a combination of these features, we demonstrate an effective method that automatically transfers the visual and temporal styles from professionally edited videos to unseen raw footage. We evaluated our approach with real-world videos that contained a total of 3872 video shots of a variety of editing styles, including different subjects, camera motions, and lighting. We reported feedback from survey participants who reviewed a set of our results.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Automatic Non-Linear Video Editing Transfer<br>pdf: <a href="https://t.co/8EltwPb74s">https://t.co/8EltwPb74s</a><br>abs: <a href="https://t.co/jSsH5WCXON">https://t.co/jSsH5WCXON</a><br><br>an automatic approach that extracts editing styles in a source video and applies the edits to matched footage for video creation <a href="https://t.co/FzGQS6jLvu">pic.twitter.com/FzGQS6jLvu</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1394107300609150985?ref_src=twsrc%5Etfw">May 17, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



