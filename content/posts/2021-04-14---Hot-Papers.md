---
title: Hot Papers 2021-04-14
date: 2021-04-15T07:28:16.Z
template: "post"
draft: false
slug: "hot-papers-2021-04-14"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2021-04-14"
socialImage: "/media/flying-marine.jpg"

---

# 1. Semantic Segmentation with Generative Models: Semi-Supervised Learning  and Strong Out-of-Domain Generalization

Daiqing Li, Junlin Yang, Karsten Kreis, Antonio Torralba, Sanja Fidler

- retweets: 4485, favorites: 269 (04/15/2021 07:28:16)

- links: [abs](https://arxiv.org/abs/2104.05833) | [pdf](https://arxiv.org/pdf/2104.05833)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Training deep networks with limited labeled data while achieving a strong generalization ability is key in the quest to reduce human annotation efforts. This is the goal of semi-supervised learning, which exploits more widely available unlabeled data to complement small labeled data sets. In this paper, we propose a novel framework for discriminative pixel-level tasks using a generative model of both images and labels. Concretely, we learn a generative adversarial network that captures the joint image-label distribution and is trained efficiently using a large set of unlabeled images supplemented with only few labeled ones. We build our architecture on top of StyleGAN2, augmented with a label synthesis branch. Image labeling at test time is achieved by first embedding the target image into the joint latent space via an encoder network and test-time optimization, and then generating the label from the inferred embedding. We evaluate our approach in two important domains: medical image segmentation and part-based face segmentation. We demonstrate strong in-domain performance compared to several baselines, and are the first to showcase extreme out-of-domain generalization, such as transferring from CT to MRI in medical imaging, and photographs of real faces to paintings, sculptures, and even cartoons and animal faces. Project Page: \url{https://nv-tlabs.github.io/semanticGAN/}

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization<br>pdf: <a href="https://t.co/99DM3uwwyu">https://t.co/99DM3uwwyu</a><br>abs: <a href="https://t.co/riNxSUfFBJ">https://t.co/riNxSUfFBJ</a><br>project page: <a href="https://t.co/v8No5RHzJE">https://t.co/v8No5RHzJE</a> <a href="https://t.co/X8V9i4B1dO">pic.twitter.com/X8V9i4B1dO</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1382143181643472900?ref_src=twsrc%5Etfw">April 14, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. BARF: Bundle-Adjusting Neural Radiance Fields

Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, Simon Lucey

- retweets: 3043, favorites: 428 (04/15/2021 07:28:16)

- links: [abs](https://arxiv.org/abs/2104.06405) | [pdf](https://arxiv.org/pdf/2104.06405)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.GR](https://arxiv.org/list/cs.GR/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.RO](https://arxiv.org/list/cs.RO/recent)

Neural Radiance Fields (NeRF) have recently gained a surge of interest within the computer vision community for its power to synthesize photorealistic novel views of real-world scenes. One limitation of NeRF, however, is its requirement of accurate camera poses to learn the scene representations. In this paper, we propose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from imperfect (or even unknown) camera poses -- the joint problem of learning neural 3D representations and registering camera frames. We establish a theoretical connection to classical image alignment and show that coarse-to-fine registration is also applicable to NeRF. Furthermore, we show that na\"ively applying positional encoding in NeRF has a negative impact on registration with a synthesis-based objective. Experiments on synthetic and real-world data show that BARF can effectively optimize the neural scene representations and resolve large camera pose misalignment at the same time. This enables view synthesis and localization of video sequences from unknown camera poses, opening up new avenues for visual localization systems (e.g. SLAM) and potential applications for dense 3D mapping and reconstruction.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">BARF: Bundle-Adjusting Neural Radiance Fields <a href="https://t.co/jW3TZWJkgn">https://t.co/jW3TZWJkgn</a> <br><br>Cool new work showcasing how to blend classical bundle adjustment with new representations of 3D space like NeRF. <a href="https://twitter.com/hashtag/computervision?src=hash&amp;ref_src=twsrc%5Etfw">#computervision</a> <a href="https://twitter.com/hashtag/3d?src=hash&amp;ref_src=twsrc%5Etfw">#3d</a> <a href="https://t.co/H0xA14Tqsf">pic.twitter.com/H0xA14Tqsf</a></p>&mdash; Tomasz Malisiewicz (@quantombone) <a href="https://twitter.com/quantombone/status/1382139584268668930?ref_src=twsrc%5Etfw">April 14, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">BARF : Bundle-Adjusting Neural Radiance Fields<br>pdf: <a href="https://t.co/35zgrdjnFE">https://t.co/35zgrdjnFE</a><br>abs: <a href="https://t.co/gpixZhhDfD">https://t.co/gpixZhhDfD</a><br>project page: <a href="https://t.co/39Y56Luua9">https://t.co/39Y56Luua9</a> <a href="https://t.co/i02bzZ31fH">pic.twitter.com/i02bzZ31fH</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1382138531649351683?ref_src=twsrc%5Etfw">April 14, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. What's in your Head? Emergent Behaviour in Multi-Task Transformer Models

Mor Geva, Uri Katz, Aviv Ben-Arie, Jonathan Berant

- retweets: 749, favorites: 103 (04/15/2021 07:28:16)

- links: [abs](https://arxiv.org/abs/2104.06129) | [pdf](https://arxiv.org/pdf/2104.06129)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

The primary paradigm for multi-task training in natural language processing is to represent the input with a shared pre-trained language model, and add a small, thin network (head) per task. Given an input, a target head is the head that is selected for outputting the final prediction. In this work, we examine the behaviour of non-target heads, that is, the output of heads when given input that belongs to a different task than the one they were trained for. We find that non-target heads exhibit emergent behaviour, which may either explain the target task, or generalize beyond their original task. For example, in a numerical reasoning task, a span extraction head extracts from the input the arguments to a computation that results in a number generated by a target generative head. In addition, a summarization head that is trained with a target question answering head, outputs query-based summaries when given a question and a context from which the answer is to be extracted. This emergent behaviour suggests that multi-task training leads to non-trivial extrapolation of skills, which can be harnessed for interpretability and generalization.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">New preprint! We show that training transformer models with multiple output heads leads to non-trivial interactions between the heads and emergent head behaviour that generalizes beyond the task the head was trained for.<a href="https://t.co/zftnny93B3">https://t.co/zftnny93B3</a> <a href="https://twitter.com/UrikaUri?ref_src=twsrc%5Etfw">@UrikaUri</a> Aviv BA <a href="https://twitter.com/JonathanBerant?ref_src=twsrc%5Etfw">@JonathanBerant</a> <a href="https://t.co/PX0pS9aM8L">pic.twitter.com/PX0pS9aM8L</a></p>&mdash; Mor Geva (@megamor2) <a href="https://twitter.com/megamor2/status/1382299065375936512?ref_src=twsrc%5Etfw">April 14, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Self-supervised object detection from audio-visual correspondence

Triantafyllos Afouras, Yuki M. Asano, Francois Fagan, Andrea Vedaldi, Florian Metze

- retweets: 702, favorites: 112 (04/15/2021 07:28:17)

- links: [abs](https://arxiv.org/abs/2104.06401) | [pdf](https://arxiv.org/pdf/2104.06401)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

We tackle the problem of learning object detectors without supervision. Differently from weakly-supervised object detection, we do not assume image-level class labels. Instead, we extract a supervisory signal from audio-visual data, using the audio component to "teach" the object detector. While this problem is related to sound source localisation, it is considerably harder because the detector must classify the objects by type, enumerate each instance of the object, and do so even when the object is silent. We tackle this problem by first designing a self-supervised framework with a contrastive objective that jointly learns to classify and localise objects. Then, without using any supervision, we simply use these self-supervised labels and boxes to train an image-based object detector. With this, we outperform previous unsupervised and weakly-supervised detectors for the task of object detection and sound source localization. We also show that we can align this detector to ground-truth classes with as little as one label per pseudo-class, and show how our method can learn to detect generic objects that go beyond instruments, such as airplanes and cats.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Self-supervised object detection from audio-visual correspondence<br>pdf: <a href="https://t.co/wTRsrF15bJ">https://t.co/wTRsrF15bJ</a><br>abs: <a href="https://t.co/KcH3VuqDKO">https://t.co/KcH3VuqDKO</a> <a href="https://t.co/tfmTsdXzkS">pic.twitter.com/tfmTsdXzkS</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1382141744851079170?ref_src=twsrc%5Etfw">April 14, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Online and Offline Reinforcement Learning by Planning with a Learned  Model

Julian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis Antonoglou, David Silver

- retweets: 317, favorites: 113 (04/15/2021 07:28:17)

- links: [abs](https://arxiv.org/abs/2104.06294) | [pdf](https://arxiv.org/pdf/2104.06294)
- [cs.LG](https://arxiv.org/list/cs.LG/recent)

Learning efficiently from small amounts of data has long been the focus of model-based reinforcement learning, both for the online case when interacting with the environment and the offline case when learning from a fixed dataset. However, to date no single unified algorithm could demonstrate state-of-the-art results in both settings. In this work, we describe the Reanalyse algorithm which uses model-based policy and value improvement operators to compute new improved training targets on existing data points, allowing efficient learning for data budgets varying by several orders of magnitude. We further show that Reanalyse can also be used to learn entirely from demonstrations without any environment interactions, as in the case of offline Reinforcement Learning (offline RL). Combining Reanalyse with the MuZero algorithm, we introduce MuZero Unplugged, a single unified algorithm for any data budget, including offline RL. In contrast to previous work, our algorithm does not require any special adaptations for the off-policy or offline RL settings. MuZero Unplugged sets new state-of-the-art results in the RL Unplugged offline RL benchmark as well as in the online RL benchmark of Atari in the standard 200 million frame setting.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Online and Offline Reinforcement Learning by Planning with a Learned Model<br><br>Proposes MuZero Unplugged by combining Reanalyse with MuZero, which sets a new SotA on various online/offline tasks.<a href="https://t.co/YCbr9hVeEC">https://t.co/YCbr9hVeEC</a> <a href="https://t.co/g10CQj3Vmf">pic.twitter.com/g10CQj3Vmf</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1382150039544336384?ref_src=twsrc%5Etfw">April 14, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">MuZeroの拡張「Sampled MuZero」<a href="https://t.co/NN3RG4tzx3">https://t.co/NN3RG4tzx3</a><br><br>オンラインとオフラインの両方で新しい最高性能を達成する、統一されたモデルベース強化学習アルゴリズム「MuZero Unplugged」<a href="https://t.co/sSJ6UkNmEG">https://t.co/sSJ6UkNmEG</a><br><br>Muesli（ディープサーチを使わずにアタリでMuZeroと同等）<a href="https://t.co/443Zxx9mJ8">https://t.co/443Zxx9mJ8</a></p>&mdash; 小猫遊りょう（たかにゃし・りょう） (@jaguring1) <a href="https://twitter.com/jaguring1/status/1382339687591804929?ref_src=twsrc%5Etfw">April 14, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. Learning and Planning in Complex Action Spaces

Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Mohammadamin Barekatain, Simon Schmitt, David Silver

- retweets: 226, favorites: 101 (04/15/2021 07:28:17)

- links: [abs](https://arxiv.org/abs/2104.06303) | [pdf](https://arxiv.org/pdf/2104.06303)
- [cs.LG](https://arxiv.org/list/cs.LG/recent)

Many important real-world problems have action spaces that are high-dimensional, continuous or both, making full enumeration of all possible actions infeasible. Instead, only small subsets of actions can be sampled for the purpose of policy evaluation and improvement. In this paper, we propose a general framework to reason in a principled way about policy evaluation and improvement over such sampled action subsets. This sample-based policy iteration framework can in principle be applied to any reinforcement learning algorithm based upon policy iteration. Concretely, we propose Sampled MuZero, an extension of the MuZero algorithm that is able to learn in domains with arbitrarily complex action spaces by planning over sampled actions. We demonstrate this approach on the classical board game of Go and on two continuous control benchmark domains: DeepMind Control Suite and Real-World RL Suite.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Learning and Planning in Complex Action Spaces<br><br>Proposes Sampled MuZero, an extension of the MuZero that is able to learn in domains with arbitrarily complex action spaces by planning over sampled actions. <a href="https://t.co/eIzkitwpuv">https://t.co/eIzkitwpuv</a> <a href="https://t.co/ekLHn9zHBi">pic.twitter.com/ekLHn9zHBi</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1382135776725520386?ref_src=twsrc%5Etfw">April 14, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">MuZeroの拡張「Sampled MuZero」<a href="https://t.co/NN3RG4tzx3">https://t.co/NN3RG4tzx3</a><br><br>オンラインとオフラインの両方で新しい最高性能を達成する、統一されたモデルベース強化学習アルゴリズム「MuZero Unplugged」<a href="https://t.co/sSJ6UkNmEG">https://t.co/sSJ6UkNmEG</a><br><br>Muesli（ディープサーチを使わずにアタリでMuZeroと同等）<a href="https://t.co/443Zxx9mJ8">https://t.co/443Zxx9mJ8</a></p>&mdash; 小猫遊りょう（たかにゃし・りょう） (@jaguring1) <a href="https://twitter.com/jaguring1/status/1382339687591804929?ref_src=twsrc%5Etfw">April 14, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. Podracer architectures for scalable Reinforcement Learning

Matteo Hessel, Manuel Kroiss, Aidan Clark, Iurii Kemaev, John Quan, Thomas Keck, Fabio Viola, Hado van Hasselt

- retweets: 174, favorites: 147 (04/15/2021 07:28:18)

- links: [abs](https://arxiv.org/abs/2104.06272) | [pdf](https://arxiv.org/pdf/2104.06272)
- [cs.LG](https://arxiv.org/list/cs.LG/recent)

Supporting state-of-the-art AI research requires balancing rapid prototyping, ease of use, and quick iteration, with the ability to deploy experiments at a scale traditionally associated with production systems.Deep learning frameworks such as TensorFlow, PyTorch and JAX allow users to transparently make use of accelerators, such as TPUs and GPUs, to offload the more computationally intensive parts of training and inference in modern deep learning systems. Popular training pipelines that use these frameworks for deep learning typically focus on (un-)supervised learning. How to best train reinforcement learning (RL) agents at scale is still an active research area. In this report we argue that TPUs are particularly well suited for training RL agents in a scalable, efficient and reproducible way. Specifically we describe two architectures designed to make the best use of the resources available on a TPU Pod (a special configuration in a Google data center that features multiple TPU devices connected to each other by extremely low latency communication channels).

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Podracer architectures for scalable Reinforcement Learning<br>pdf: <a href="https://t.co/2cKpTVT8cZ">https://t.co/2cKpTVT8cZ</a><br>abs: <a href="https://t.co/Fh697A7zs4">https://t.co/Fh697A7zs4</a><br>&quot;we argue that TPUs are particularly well suited for training RL agents in a scalable, efficient and reproducible way&quot; <a href="https://t.co/Z6kE75U4SR">pic.twitter.com/Z6kE75U4SR</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1382147577320255493?ref_src=twsrc%5Etfw">April 14, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Checkout our latest writeup on TPU-based computing architectures for scalable Reinforcement Learning! <a href="https://t.co/IvDnbIJOD2">https://t.co/IvDnbIJOD2</a></p>&mdash; matteo hessel (@matteohessel) <a href="https://twitter.com/matteohessel/status/1382240700444606464?ref_src=twsrc%5Etfw">April 14, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. Muesli: Combining Improvements in Policy Optimization

Matteo Hessel, Ivo Danihelka, Fabio Viola, Arthur Guez, Simon Schmitt, Laurent Sifre, Theophane Weber, David Silver, Hado van Hasselt

- retweets: 175, favorites: 86 (04/15/2021 07:28:18)

- links: [abs](https://arxiv.org/abs/2104.06159) | [pdf](https://arxiv.org/pdf/2104.06159)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

We propose a novel policy update that combines regularized policy optimization with model learning as an auxiliary loss. The update (henceforth Muesli) matches MuZero's state-of-the-art performance on Atari. Notably, Muesli does so without using deep search: it acts directly with a policy network and has computation speed comparable to model-free baselines. The Atari results are complemented by extensive ablations, and by additional results on continuous control and 9x9 Go.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">MuZeroの拡張「Sampled MuZero」<a href="https://t.co/NN3RG4tzx3">https://t.co/NN3RG4tzx3</a><br><br>オンラインとオフラインの両方で新しい最高性能を達成する、統一されたモデルベース強化学習アルゴリズム「MuZero Unplugged」<a href="https://t.co/sSJ6UkNmEG">https://t.co/sSJ6UkNmEG</a><br><br>Muesli（ディープサーチを使わずにアタリでMuZeroと同等）<a href="https://t.co/443Zxx9mJ8">https://t.co/443Zxx9mJ8</a></p>&mdash; 小猫遊りょう（たかにゃし・りょう） (@jaguring1) <a href="https://twitter.com/jaguring1/status/1382339687591804929?ref_src=twsrc%5Etfw">April 14, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Interesting piece of work by: <br><br>Matteo Hessel, Ivo Danihelka, Fabio Viola, Arthur Guez, Simon Schmitt, Laurent Sifre, Theophane Weber, David Silver, Hado van Hasselt<br><br>Muesli: Combining Improvements in Policy Optimization<a href="https://t.co/zHbJ9NEJCK">https://t.co/zHbJ9NEJCK</a></p>&mdash; Anirudh Goyal (@anirudhg9119) <a href="https://twitter.com/anirudhg9119/status/1382149900348092417?ref_src=twsrc%5Etfw">April 14, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Muesli: Combining Improvements in Policy Optimization<br><br>Proposes a novel policy update that combines regularized policy optimization with model learning as an aux. loss. <br><br>Muesli matches MuZero’s SotA perf on Atari with great perf on Atari and Go.<a href="https://t.co/mm9GQvRN8k">https://t.co/mm9GQvRN8k</a> <a href="https://t.co/wQfVFR7tXg">pic.twitter.com/wQfVFR7tXg</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1382154988244242432?ref_src=twsrc%5Etfw">April 14, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. DropLoss for Long-Tail Instance Segmentation

Ting-I Hsieh, Esther Robb, Hwann-Tzong Chen, Jia-Bin Huang

- retweets: 72, favorites: 77 (04/15/2021 07:28:18)

- links: [abs](https://arxiv.org/abs/2104.06402) | [pdf](https://arxiv.org/pdf/2104.06402)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Long-tailed class distributions are prevalent among the practical applications of object detection and instance segmentation. Prior work in long-tail instance segmentation addresses the imbalance of losses between rare and frequent categories by reducing the penalty for a model incorrectly predicting a rare class label. We demonstrate that the rare categories are heavily suppressed by correct background predictions, which reduces the probability for all foreground categories with equal weight. Due to the relative infrequency of rare categories, this leads to an imbalance that biases towards predicting more frequent categories. Based on this insight, we develop DropLoss -- a novel adaptive loss to compensate for this imbalance without a trade-off between rare and frequent categories. With this loss, we show state-of-the-art mAP across rare, common, and frequent categories on the LVIS dataset.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">DropLoss for Long-Tail Instance Segmentation<br>pdf: <a href="https://t.co/pBgzdZTKpX">https://t.co/pBgzdZTKpX</a><br>abs: <a href="https://t.co/FW5XOhZVLm">https://t.co/FW5XOhZVLm</a><br>github: <a href="https://t.co/pJhaWF1gBT">https://t.co/pJhaWF1gBT</a> <a href="https://t.co/cNqAyINCRd">pic.twitter.com/cNqAyINCRd</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1382160307980795913?ref_src=twsrc%5Etfw">April 14, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 10. Pointly-Supervised Instance Segmentation

Bowen Cheng, Omkar Parkhi, Alexander Kirillov

- retweets: 43, favorites: 50 (04/15/2021 07:28:18)

- links: [abs](https://arxiv.org/abs/2104.06404) | [pdf](https://arxiv.org/pdf/2104.06404)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

We propose point-based instance-level annotation, a new form of weak supervision for instance segmentation. It combines the standard bounding box annotation with labeled points that are uniformly sampled inside each bounding box. We show that the existing instance segmentation models developed for full mask supervision, like Mask R-CNN, can be seamlessly trained with the point-based annotation without any major modifications. In our experiments, Mask R-CNN models trained on COCO, PASCAL VOC, Cityscapes, and LVIS with only 10 annotated points per object achieve 94%--98% of their fully-supervised performance. The new point-based annotation is approximately 5 times faster to collect than object masks, making high-quality instance segmentation more accessible for new data.   Inspired by the new annotation form, we propose a modification to PointRend instance segmentation module. For each object, the new architecture, called Implicit PointRend, generates parameters for a function that makes the final point-level mask prediction. Implicit PointRend is more straightforward and uses a single point-level mask loss. Our experiments show that the new module is more suitable for the proposed point-based supervision.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Pointly-Supervised Instance Segmentation<br>pdf: <a href="https://t.co/O9z072y4VR">https://t.co/O9z072y4VR</a><br>abs: <a href="https://t.co/S2TxPapZkU">https://t.co/S2TxPapZkU</a><br>project page: <a href="https://t.co/OclZbxAaFt">https://t.co/OclZbxAaFt</a> <a href="https://t.co/CXvNvNPVzF">pic.twitter.com/CXvNvNPVzF</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1382161377540907014?ref_src=twsrc%5Etfw">April 14, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 11. A Replication Study of Dense Passage Retriever

Xueguang Ma, Kai Sun, Ronak Pradeep, Jimmy Lin

- retweets: 30, favorites: 58 (04/15/2021 07:28:19)

- links: [abs](https://arxiv.org/abs/2104.05740) | [pdf](https://arxiv.org/pdf/2104.05740)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.IR](https://arxiv.org/list/cs.IR/recent)

Text retrieval using learned dense representations has recently emerged as a promising alternative to "traditional" text retrieval using sparse bag-of-words representations. One recent work that has garnered much attention is the dense passage retriever (DPR) technique proposed by Karpukhin et al. (2020) for end-to-end open-domain question answering. We present a replication study of this work, starting with model checkpoints provided by the authors, but otherwise from an independent implementation in our group's Pyserini IR toolkit and PyGaggle neural text ranking library. Although our experimental results largely verify the claims of the original paper, we arrived at two important additional findings that contribute to a better understanding of DPR: First, it appears that the original authors under-report the effectiveness of the BM25 baseline and hence also dense--sparse hybrid retrieval results. Second, by incorporating evidence from the retriever and an improved answer span scoring technique, we are able to improve end-to-end question answering effectiveness using exactly the same models as in the original work.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">If you&#39;re interested in dense retrieval, you&#39;ll want to check out this DPR replication effort led by <a href="https://twitter.com/xueguang_ma?ref_src=twsrc%5Etfw">@xueguang_ma</a> <a href="https://t.co/kxCvJhgWEv">https://t.co/kxCvJhgWEv</a> tl;dr - BM25 is better than the original authors made it out to be, and free QA boost with better evidence fusion!</p>&mdash; Jimmy Lin (@lintool) <a href="https://twitter.com/lintool/status/1382320581354344450?ref_src=twsrc%5Etfw">April 14, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 12. RECON: Rapid Exploration for Open-World Navigation with Latent Goal  Models

Dhruv Shah, Benjamin Eysenbach, Gregory Kahn, Nicholas Rhinehart, Sergey Levine

- retweets: 38, favorites: 30 (04/15/2021 07:28:19)

- links: [abs](https://arxiv.org/abs/2104.05859) | [pdf](https://arxiv.org/pdf/2104.05859)
- [cs.RO](https://arxiv.org/list/cs.RO/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

We describe a robotic learning system for autonomous navigation in diverse environments. At the core of our method are two components: (i) a non-parametric map that reflects the connectivity of the environment but does not require geometric reconstruction or localization, and (ii) a latent variable model of distances and actions that enables efficiently constructing and traversing this map. The model is trained on a large dataset of prior experience to predict the expected amount of time and next action needed to transit between the current image and a goal image. Training the model in this way enables it to develop a representation of goals robust to distracting information in the input images, which aids in deploying the system to quickly explore new environments. We demonstrate our method on a mobile ground robot in a range of outdoor navigation scenarios. Our method can learn to reach new goals, specified as images, in a radius of up to 80 meters in just 20 minutes, and reliably revisit these goals in changing environments. We also demonstrate our method's robustness to previously-unseen obstacles and variable weather conditions. We encourage the reader to visit the project website for videos of our experiments and demonstrations https://sites.google.com/view/recon-robot




# 13. Paragraph-level Simplification of Medical Texts

Ashwin Devaraj, Iain J. Marshall, Byron C. Wallace, Junyi Jessy Li

- retweets: 32, favorites: 22 (04/15/2021 07:28:19)

- links: [abs](https://arxiv.org/abs/2104.05767) | [pdf](https://arxiv.org/pdf/2104.05767)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

We consider the problem of learning to simplify medical texts. This is important because most reliable, up-to-date information in biomedicine is dense with jargon and thus practically inaccessible to the lay audience. Furthermore, manual simplification does not scale to the rapidly growing body of biomedical literature, motivating the need for automated approaches. Unfortunately, there are no large-scale resources available for this task. In this work we introduce a new corpus of parallel texts in English comprising technical and lay summaries of all published evidence pertaining to different clinical topics. We then propose a new metric based on likelihood scores from a masked language model pretrained on scientific texts. We show that this automated measure better differentiates between technical and lay summaries than existing heuristics. We introduce and evaluate baseline encoder-decoder Transformer models for simplification and propose a novel augmentation to these in which we explicitly penalize the decoder for producing "jargon" terms; we find that this yields improvements over baselines in terms of readability.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">How would we improve the accessibility of highly technical information? Sharing our <a href="https://twitter.com/hashtag/NAACL2021?src=hash&amp;ref_src=twsrc%5Etfw">#NAACL2021</a> paper &quot;Paragraph-level Simplification of Medical Texts&quot; w/ Ashwin Devaraj, <a href="https://twitter.com/ijmarshall?ref_src=twsrc%5Etfw">@ijmarshall</a>, and <a href="https://twitter.com/byron_c_wallace?ref_src=twsrc%5Etfw">@byron_c_wallace</a> <a href="https://t.co/sUucFy8lBH">https://t.co/sUucFy8lBH</a><br>(1/2) <a href="https://t.co/mymbfqeopQ">pic.twitter.com/mymbfqeopQ</a></p>&mdash; Jessy Li (@jessyjli) <a href="https://twitter.com/jessyjli/status/1382414523135451136?ref_src=twsrc%5Etfw">April 14, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 14. Co-Scale Conv-Attentional Image Transformers

Weijian Xu, Yifan Xu, Tyler Chang, Zhuowen Tu

- retweets: 20, favorites: 30 (04/15/2021 07:28:19)

- links: [abs](https://arxiv.org/abs/2104.06399) | [pdf](https://arxiv.org/pdf/2104.06399)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.NE](https://arxiv.org/list/cs.NE/recent)

In this paper, we present Co-scale conv-attentional image Transformers (CoaT), a Transformer-based image classifier equipped with co-scale and conv-attentional mechanisms. First, the co-scale mechanism maintains the integrity of Transformers' encoder branches at individual scales, while allowing representations learned at different scales to effectively communicate with each other; we design a series of serial and parallel blocks to realize the co-scale attention mechanism. Second, we devise a conv-attentional mechanism by realizing a relative position embedding formulation in the factorized attention module with an efficient convolution-like implementation. CoaT empowers image Transformers with enriched multi-scale and contextual modeling capabilities. On ImageNet, relatively small CoaT models attain superior classification results compared with the similar-sized convolutional neural networks and image/vision Transformers. The effectiveness of CoaT's backbone is also illustrated on object detection and instance segmentation, demonstrating its applicability to the downstream computer vision tasks.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Co-Scale Conv-Attentional Image Transformers<br>pdf: <a href="https://t.co/9aTQlMINPB">https://t.co/9aTQlMINPB</a><br>abs: <a href="https://t.co/RclvZgtHn3">https://t.co/RclvZgtHn3</a><br>&quot;On ImageNet, relatively small CoaT models attain superior classification results compared with the similar-sized convolutional neural networks and image/vision Transformers&quot; <a href="https://t.co/D4YPucpPaq">pic.twitter.com/D4YPucpPaq</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1382137415578570752?ref_src=twsrc%5Etfw">April 14, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



