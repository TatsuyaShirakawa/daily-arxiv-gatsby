---
title: Hot Papers 2020-07-06
date: 2020-07-07T09:58:05.Z
template: "post"
draft: false
slug: "hot-papers-2020-07-06"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-07-06"
socialImage: "/media/42-line-bible.jpg"

---

# 1. A (Slightly) Improved Approximation Algorithm for Metric TSP

Anna R. Karlin, Nathan Klein, Shayan Oveis Gharan

- retweets: 60, favorites: 266 (07/07/2020 09:58:05)

- links: [abs](https://arxiv.org/abs/2007.01409) | [pdf](https://arxiv.org/pdf/2007.01409)
- [cs.DS](https://arxiv.org/list/cs.DS/recent) | [math.CO](https://arxiv.org/list/math.CO/recent) | [math.PR](https://arxiv.org/list/math.PR/recent)

For some $\epsilon > 10^{-36}$ we give a $3/2-\epsilon$ approximation algorithm for metric TSP.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">A (Slightly) Improved Approximation Algorithm for Metric TSP <a href="https://t.co/vIMeYC63IS">https://t.co/vIMeYC63IS</a></p>&mdash; TCS blog aggregator (@cstheory) <a href="https://twitter.com/cstheory/status/1279965326743142402?ref_src=twsrc%5Etfw">July 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Anna Karlin, Nathan Klein, and <a href="https://twitter.com/oveisgharan?ref_src=twsrc%5Etfw">@oveisgharan</a> have improved Christofides algorithm!  You gotta love the 1-line abstract:   &quot;For some ϵ&gt;10^−36 we give a 3/2−ϵ approximation algorithm for metric TSP.&quot; <a href="https://t.co/eLjDWGKdGF">https://t.co/eLjDWGKdGF</a></p>&mdash; Bill Cook (@wjcook) <a href="https://twitter.com/wjcook/status/1280108583963758592?ref_src=twsrc%5Etfw">July 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr"><a href="https://t.co/SIrDmYtHv8">https://t.co/SIrDmYtHv8</a><br>arXivにやばいのが出た。Metric TSPの1.5近似 (Christofides) を改善したらしい。</p>&mdash; のぶしみ (@knewknowl) <a href="https://twitter.com/knewknowl/status/1279969344340897793?ref_src=twsrc%5Etfw">July 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. In Search of Lost Domain Generalization

Ishaan Gulrajani, David Lopez-Paz

- retweets: 37, favorites: 229 (07/07/2020 09:58:06)

- links: [abs](https://arxiv.org/abs/2007.01434) | [pdf](https://arxiv.org/pdf/2007.01434)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

The goal of domain generalization algorithms is to predict well on distributions different from those seen during training. While a myriad of domain generalization algorithms exist, inconsistencies in experimental conditions -- datasets, architectures, and model selection criteria -- render fair and realistic comparisons difficult. In this paper, we are interested in understanding how useful domain generalization algorithms are in realistic settings. As a first step, we realize that model selection is non-trivial for domain generalization tasks. Contrary to prior work, we argue that domain generalization algorithms without a model selection strategy should be regarded as incomplete. Next, we implement DomainBed, a testbed for domain generalization including seven multi-domain datasets, nine baseline algorithms, and three model selection criteria. We conduct extensive experiments using DomainBed and find that, when carefully implemented, empirical risk minimization shows state-of-the-art performance across all datasets. Looking forward, we hope that the release of DomainBed, along with contributions from fellow researchers, will streamline reproducible and rigorous research in domain generalization.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Beautiful summary of different learning problems from a new paper by <a href="https://twitter.com/__ishaan?ref_src=twsrc%5Etfw">@__ishaan</a> and David Lopez-Paz <a href="https://t.co/nzxfgxlKVP">https://t.co/nzxfgxlKVP</a> <br><br>(rest of paper also looks interesting) <a href="https://t.co/Zqug6Y2kfX">pic.twitter.com/Zqug6Y2kfX</a></p>&mdash; Phillip Isola (@phillip_isola) <a href="https://twitter.com/phillip_isola/status/1279990131894730752?ref_src=twsrc%5Etfw">July 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Online learning in MDPs with linear function approximation and bandit  feedback

Gergely Neu, Julia Olkhovskaya

- retweets: 8, favorites: 56 (07/07/2020 09:58:06)

- links: [abs](https://arxiv.org/abs/2007.01612) | [pdf](https://arxiv.org/pdf/2007.01612)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

We consider an online learning problem where the learner interacts with a Markov decision process in a sequence of episodes, where the reward function is allowed to change between episodes in an adversarial manner and the learner only gets to observe the rewards associated with its actions. We allow the state space to be arbitrarily large, but we assume that all action-value functions can be represented as linear functions in terms of a known low-dimensional feature map, and that the learner has access to a simulator of the environment that allows generating trajectories from the true MDP dynamics. Our main contribution is developing a computationally efficient algorithm that we call MDP-LinExp3, and prove that its regret is bounded by $\widetilde{\mathcal{O}}\big(H^2 T^{2/3} (dK)^{1/3}\big)$, where $T$ is the number of episodes, $H$ is the number of steps in each episode, $K$ is the number of actions, and $d$ is the dimension of the feature map. We also show that the regret can be improved to $\widetilde{\mathcal{O}}\big(H^2 \sqrt{TdK}\big)$ under much stronger assumptions on the MDP dynamics. To our knowledge, MDP-LinExp3 is the first provably efficient algorithm for this problem setting.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">new paper on adversarial MDPs now online! <br><br>main result: sublinear regret with realizable function approximation &amp; bandit feedback.<br><br>w/ my student Julia Olkhovskaya<a href="https://t.co/8AFi9apOVW">https://t.co/8AFi9apOVW</a> <a href="https://t.co/fiLl9QL69W">pic.twitter.com/fiLl9QL69W</a></p>&mdash; Gergely Neu (@neu_rips) <a href="https://twitter.com/neu_rips/status/1280083343929982976?ref_src=twsrc%5Etfw">July 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Self-Supervised GAN Compression

Chong Yu, Jeff Pool

- retweets: 7, favorites: 45 (07/07/2020 09:58:06)

- links: [abs](https://arxiv.org/abs/2007.01491) | [pdf](https://arxiv.org/pdf/2007.01491)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [eess.IV](https://arxiv.org/list/eess.IV/recent)

Deep learning's success has led to larger and larger models to handle more and more complex tasks; trained models can contain millions of parameters. These large models are compute- and memory-intensive, which makes it a challenge to deploy them with minimized latency, throughput, and storage requirements. Some model compression methods have been successfully applied to image classification and detection or language models, but there has been very little work compressing generative adversarial networks (GANs) performing complex tasks. In this paper, we show that a standard model compression technique, weight pruning, cannot be applied to GANs using existing methods. We then develop a self-supervised compression technique which uses the trained discriminator to supervise the training of a compressed generator. We show that this framework has a compelling performance to high degrees of sparsity, can be easily applied to new tasks and models, and enables meaningful comparisons between different pruning granularities.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Self-Supervised GAN Compression<br>pdf: <a href="https://t.co/5ue2UjOuUE">https://t.co/5ue2UjOuUE</a><br>abs: <a href="https://t.co/t1MdHiNFQv">https://t.co/t1MdHiNFQv</a> <a href="https://t.co/iHw64W31Gg">pic.twitter.com/iHw64W31Gg</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1279943087041777665?ref_src=twsrc%5Etfw">July 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



