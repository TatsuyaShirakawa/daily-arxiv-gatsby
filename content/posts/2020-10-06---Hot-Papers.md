---
title: Hot Papers 2020-10-06
date: 2020-10-07T09:23:53.Z
template: "post"
draft: false
slug: "hot-papers-2020-10-06"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-10-06"
socialImage: "/media/flying-marine.jpg"

---

# 1. Explaining Deep Neural Networks

Oana-Maria Camburu

- retweets: 14500, favorites: 0 (10/07/2020 09:23:53)

- links: [abs](https://arxiv.org/abs/2010.01496) | [pdf](https://arxiv.org/pdf/2010.01496)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

Deep neural networks are becoming more and more popular due to their revolutionary success in diverse areas, such as computer vision, natural language processing, and speech recognition. However, the decision-making processes of these models are generally not interpretable to users. In various domains, such as healthcare, finance, or law, it is critical to know the reasons behind a decision made by an artificial intelligence system. Therefore, several directions for explaining neural models have recently been explored.   In this thesis, I investigate two major directions for explaining deep neural networks. The first direction consists of feature-based post-hoc explanatory methods, that is, methods that aim to explain an already trained and fixed model (post-hoc), and that provide explanations in terms of input features, such as tokens for text and superpixels for images (feature-based). The second direction consists of self-explanatory neural models that generate natural language explanations, that is, models that have a built-in module that generates explanations for the predictions of the model.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Excited to share my PhD thesis on Explaining Deep Neural Networks: <a href="https://t.co/ZVMSImo3b6">https://t.co/ZVMSImo3b6</a>! <br><br>Very grateful to my supervisors and to all the other amazing people I have met during this journey! <a href="https://t.co/Uwx6D6qxtc">pic.twitter.com/Uwx6D6qxtc</a></p>&mdash; Oana Camburu (@oanacamb) <a href="https://twitter.com/oanacamb/status/1313448884782485504?ref_src=twsrc%5Etfw">October 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Evaluating Progress on Machine Learning for Longitudinal Electronic  Healthcare Data

David Bellamy, Leo Celi, Andrew L. Beam

- retweets: 2320, favorites: 139 (10/07/2020 09:23:54)

- links: [abs](https://arxiv.org/abs/2010.01149) | [pdf](https://arxiv.org/pdf/2010.01149)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

The Large Scale Visual Recognition Challenge based on the well-known Imagenet dataset catalyzed an intense flurry of progress in computer vision. Benchmark tasks have propelled other sub-fields of machine learning forward at an equally impressive pace, but in healthcare it has primarily been image processing tasks, such as in dermatology and radiology, that have experienced similar benchmark-driven progress. In the present study, we performed a comprehensive review of benchmarks in medical machine learning for structured data, identifying one based on the Medical Information Mart for Intensive Care (MIMIC-III) that allows the first direct comparison of predictive performance and thus the evaluation of progress on four clinical prediction tasks: mortality, length of stay, phenotyping, and patient decompensation. We find that little meaningful progress has been made over a 3 year period on these tasks, despite significant community engagement. Through our meta-analysis, we find that the performance of deep recurrent models is only superior to logistic regression on certain tasks. We conclude with a synthesis of these results, possible explanations, and a list of desirable qualities for future benchmarks in medical machine learning.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Are we making meaningful progress on machine learning for EHR data?<br><br>New preprint with <a href="https://twitter.com/DavidRBellamy?ref_src=twsrc%5Etfw">@DavidRBellamy</a>  and Leo Celi tries to answer this question through the lens of benchmarks and the answer is, unfortunately,  &quot;probably not&quot;<br><br>Paper: <a href="https://t.co/KjgjvC0eWQ">https://t.co/KjgjvC0eWQ</a><br><br>Some highlights üëá <a href="https://t.co/5xZf5iqJQs">pic.twitter.com/5xZf5iqJQs</a></p>&mdash; Andrew Beam (@AndrewLBeam) <a href="https://twitter.com/AndrewLBeam/status/1313483090891862016?ref_src=twsrc%5Etfw">October 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. End-to-End Differentiable Molecular Mechanics Force Field Construction

Yuanqing Wang, Josh Fass, John D. Chodera

- retweets: 1849, favorites: 169 (10/07/2020 09:23:54)

- links: [abs](https://arxiv.org/abs/2010.01196) | [pdf](https://arxiv.org/pdf/2010.01196)
- [physics.comp-ph](https://arxiv.org/list/physics.comp-ph/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

Molecular mechanics (MM) potentials have long been a workhorse of computational chemistry. Leveraging accuracy and speed, these functional forms find use in a wide variety of applications from rapid virtual screening to detailed free energy calculations. Traditionally, MM potentials have relied on human-curated, inflexible, and poorly extensible discrete chemical perception rules (atom types) for applying parameters to molecules or biopolymers, making them difficult to optimize to fit quantum chemical or physical property data. Here, we propose an alternative approach that uses graph nets to perceive chemical environments, producing continuous atom embeddings from which valence and nonbonded parameters can be predicted using a feed-forward neural network. Since all stages are built using smooth functions, the entire process of chemical perception and parameter assignment is differentiable end-to-end with respect to model parameters, allowing new force fields to be easily constructed, extended, and applied to arbitrary molecules. We show that this approach has the capacity to reproduce legacy atom types and can be fit to MM and QM energies and forces, among other targets.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Biomolecular force field parameterization enters the modern age! This new preprint from <a href="https://twitter.com/YuanqingWang?ref_src=twsrc%5Etfw">@YuanqingWang</a> and <a href="https://twitter.com/maxentile?ref_src=twsrc%5Etfw">@maxentile</a> shows how graph nets can directly assign MM force field parameters, easily learning existing MM force fields or directly fitting QM data.<a href="https://t.co/xPlNHpqee4">https://t.co/xPlNHpqee4</a> <a href="https://t.co/Vj08tkRByM">pic.twitter.com/Vj08tkRByM</a></p>&mdash; John Chodera (he/him) (@jchodera) <a href="https://twitter.com/jchodera/status/1313468551060168717?ref_src=twsrc%5Etfw">October 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. JSSS: free Japanese speech corpus for summarization and simplification

Shinnosuke Takamichi, Mamoru Komachi, Naoko Tanji, Hiroshi Saruwatari

- retweets: 1560, favorites: 100 (10/07/2020 09:23:55)

- links: [abs](https://arxiv.org/abs/2010.01793) | [pdf](https://arxiv.org/pdf/2010.01793)
- [eess.AS](https://arxiv.org/list/eess.AS/recent) | [cs.SD](https://arxiv.org/list/cs.SD/recent)

In this paper, we construct a new Japanese speech corpus for speech-based summarization and simplification, "JSSS" (pronounced "j-triple-s"). Given the success of reading-style speech synthesis from short-form sentences, we aim to design more difficult tasks for delivering information to humans. Our corpus contains voices recorded for two tasks that have a role in providing information under constraints: duration-constrained text-to-speech summarization and speaking-style simplification. It also contains utterances of long-form sentences as an optional task. This paper describes how we designed the corpus, which is available on our project page.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">We are pleased to share our new Japanese speech corpus for summarization &amp; simplification!<br>Ë¶ÅÁ¥Ñ„Å®Âπ≥ÊòìÂåñ„Å´Èñ¢„Åô„ÇãÊó•Êú¨Ë™ûÈü≥Â£∞„Ç≥„Éº„Éë„Çπ„Çí„É™„É™„Éº„Çπ„Åó„Åæ„Åó„ÅüÔºÅ<br><br>pdf: <a href="https://t.co/NaFtAlcz18">https://t.co/NaFtAlcz18</a><br>data: <a href="https://t.co/lnHov4IED1">https://t.co/lnHov4IED1</a> <br><br>1 / n <a href="https://t.co/XxrGZR0Yob">pic.twitter.com/XxrGZR0Yob</a></p>&mdash; Shinnosuke Takamichi (È´òÈÅì ÊÖé‰πã‰ªã) (@forthshinji) <a href="https://twitter.com/forthshinji/status/1313347625949646849?ref_src=twsrc%5Etfw">October 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Representational aspects of depth and conditioning in normalizing flows

Frederic Koehler, Viraj Mehta, Andrej Risteski

- retweets: 1085, favorites: 15 (10/07/2020 09:23:55)

- links: [abs](https://arxiv.org/abs/2010.01155) | [pdf](https://arxiv.org/pdf/2010.01155)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Normalizing flows are among the most popular paradigms in generative modeling, especially for images, primarily because we can efficiently evaluate the likelihood of a data point. Normalizing flows also come with difficulties: models which produce good samples typically need to be extremely deep -- which comes with accompanying vanishing/exploding gradient problems. Relatedly, they are often poorly conditioned since typical training data like images intuitively are lower-dimensional, and the learned maps often have Jacobians that are close to being singular. In our paper, we tackle representational aspects around depth and conditioning of normalizing flows -- both for general invertible architectures, and for a particular common architecture -- affine couplings. For general invertible architectures, we prove that invertibility comes at a cost in terms of depth: we show examples where a much deeper normalizing flow model may need to be used to match the performance of a non-invertible generator. For affine couplings, we first show that the choice of partitions isn't a likely bottleneck for depth: we show that any invertible linear map (and hence a permutation) can be simulated by a constant number of affine coupling layers, using a fixed partition. This shows that the extra flexibility conferred by 1x1 convolution layers, as in GLOW, can in principle be simulated by increasing the size by a constant factor. Next, in terms of conditioning, we show that affine couplings are universal approximators -- provided the Jacobian of the model is allowed to be close to singular. We furthermore empirically explore the benefit of different kinds of padding -- a common strategy for improving conditioning -- on both synthetic and real-life datasets.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Representational aspects of depth and conditioning in normalizing flows. <a href="https://twitter.com/hashtag/DeepLearning?src=hash&amp;ref_src=twsrc%5Etfw">#DeepLearning</a> <a href="https://twitter.com/hashtag/BigData?src=hash&amp;ref_src=twsrc%5Etfw">#BigData</a> <a href="https://twitter.com/hashtag/Analytics?src=hash&amp;ref_src=twsrc%5Etfw">#Analytics</a> <a href="https://twitter.com/hashtag/RStats?src=hash&amp;ref_src=twsrc%5Etfw">#RStats</a> <a href="https://twitter.com/hashtag/Python?src=hash&amp;ref_src=twsrc%5Etfw">#Python</a> <a href="https://twitter.com/hashtag/IIoT?src=hash&amp;ref_src=twsrc%5Etfw">#IIoT</a> <a href="https://twitter.com/hashtag/Java?src=hash&amp;ref_src=twsrc%5Etfw">#Java</a> <a href="https://twitter.com/hashtag/JavaScript?src=hash&amp;ref_src=twsrc%5Etfw">#JavaScript</a> <a href="https://twitter.com/hashtag/ReactJS?src=hash&amp;ref_src=twsrc%5Etfw">#ReactJS</a> <a href="https://twitter.com/hashtag/Serverless?src=hash&amp;ref_src=twsrc%5Etfw">#Serverless</a> <a href="https://twitter.com/hashtag/IoT?src=hash&amp;ref_src=twsrc%5Etfw">#IoT</a> <a href="https://twitter.com/hashtag/Linux?src=hash&amp;ref_src=twsrc%5Etfw">#Linux</a> <a href="https://twitter.com/hashtag/100DaysOfCode?src=hash&amp;ref_src=twsrc%5Etfw">#100DaysOfCode</a> <a href="https://twitter.com/hashtag/Coding?src=hash&amp;ref_src=twsrc%5Etfw">#Coding</a> <a href="https://twitter.com/hashtag/Programming?src=hash&amp;ref_src=twsrc%5Etfw">#Programming</a> <a href="https://twitter.com/hashtag/Cloud?src=hash&amp;ref_src=twsrc%5Etfw">#Cloud</a> <a href="https://twitter.com/hashtag/DataScience?src=hash&amp;ref_src=twsrc%5Etfw">#DataScience</a> <a href="https://twitter.com/hashtag/MachineLearning?src=hash&amp;ref_src=twsrc%5Etfw">#MachineLearning</a> <a href="https://twitter.com/hashtag/AI?src=hash&amp;ref_src=twsrc%5Etfw">#AI</a><a href="https://t.co/WW18STTb8k">https://t.co/WW18STTb8k</a> <a href="https://t.co/x8OdEnyYo2">pic.twitter.com/x8OdEnyYo2</a></p>&mdash; Marcus Borba (@marcusborba) <a href="https://twitter.com/marcusborba/status/1313611786176925698?ref_src=twsrc%5Etfw">October 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. On Losses for Modern Language Models

Stephane Aroca-Ouellette, Frank Rudzicz

- retweets: 650, favorites: 151 (10/07/2020 09:23:55)

- links: [abs](https://arxiv.org/abs/2010.01694) | [pdf](https://arxiv.org/pdf/2010.01694)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

BERT set many state-of-the-art results over varied NLU benchmarks by pre-training over two tasks: masked language modelling (MLM) and next sentence prediction (NSP), the latter of which has been highly criticized. In this paper, we 1) clarify NSP's effect on BERT pre-training, 2) explore fourteen possible auxiliary pre-training tasks, of which seven are novel to modern language models, and 3) investigate different ways to include multiple tasks into pre-training. We show that NSP is detrimental to training due to its context splitting and shallow semantic signal. We also identify six auxiliary pre-training tasks -- sentence ordering, adjacent sentence prediction, TF prediction, TF-IDF prediction, a FastSent variant, and a Quick Thoughts variant -- that outperform a pure MLM baseline. Finally, we demonstrate that using multiple tasks in a multi-task pre-training framework provides better results than using any single auxiliary task. Using these methods, we outperform BERT Base on the GLUE benchmark using fewer than a quarter of the training tokens.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">On Losses for Modern Language Models<br><br>Outperform BERT_Base on the GLUE benchmark using fewer than a quarter of the training tokens by  exploring fourteen possible auxiliary pre-training tasks.<a href="https://t.co/4zOeUEasnD">https://t.co/4zOeUEasnD</a> <a href="https://t.co/Yzpv2zYgsp">pic.twitter.com/Yzpv2zYgsp</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1313298990717263872?ref_src=twsrc%5Etfw">October 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. Supporting large-scale image recognition with out-of-domain samples

Christof Henkel, Philipp Singer

- retweets: 359, favorites: 142 (10/07/2020 09:23:56)

- links: [abs](https://arxiv.org/abs/2010.01650) | [pdf](https://arxiv.org/pdf/2010.01650)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

This article presents an efficient end-to-end method to perform instance-level recognition employed to the task of labeling and ranking landmark images. In a first step, we embed images in a high dimensional feature space using convolutional neural networks trained with an additive angular margin loss and classify images using visual similarity. We then efficiently re-rank predictions and filter noise utilizing similarity to out-of-domain images. Using this approach we achieved the 1st place in the 2020 edition of the Google Landmark Recognition challenge.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Published a paper describing our (\w <a href="https://twitter.com/kagglingdieter?ref_src=twsrc%5Etfw">@kagglingdieter</a>)  winning solution to GLR2020: &quot;Supporting large-scale image recognition with out-of-domain samples&quot; <a href="https://t.co/RBfH9kz9Lu">https://t.co/RBfH9kz9Lu</a> Code will follow soon <a href="https://twitter.com/hashtag/kaggle?src=hash&amp;ref_src=twsrc%5Etfw">#kaggle</a></p>&mdash; Philipp Singer (@ph_singer) <a href="https://twitter.com/ph_singer/status/1313353473853927424?ref_src=twsrc%5Etfw">October 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. A rigorous and robust quantum speed-up in supervised machine learning

Yunchao Liu, Srinivasan Arunachalam, Kristan Temme

- retweets: 374, favorites: 96 (10/07/2020 09:23:56)

- links: [abs](https://arxiv.org/abs/2010.02174) | [pdf](https://arxiv.org/pdf/2010.02174)
- [quant-ph](https://arxiv.org/list/quant-ph/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Over the past few years several quantum machine learning algorithms were proposed that promise quantum speed-ups over their classical counterparts. Most of these learning algorithms assume quantum access to data; and it is unclear if quantum speed-ups still exist without making these strong assumptions. In this paper, we establish a rigorous quantum speed-up for supervised classification using a quantum learning algorithm that only requires classical access to data. Our quantum classifier is a conventional support vector machine that uses a fault-tolerant quantum computer to estimate a kernel function. Data samples are mapped to a quantum feature space and the kernel entries can be estimated as the transition amplitude of a quantum circuit. We construct a family of datasets and show that no classical learner can classify the data inverse-polynomially better than random guessing, assuming the widely-believed hardness of the discrete logarithm problem. Meanwhile, the quantum classifier achieves high accuracy and is robust against additive errors in the kernel entries that arise from finite sampling statistics.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Could be real: A rigorous and robust quantum speed-up in supervised machine learning<br><br> <a href="https://t.co/NyfEsSzMKu">https://t.co/NyfEsSzMKu</a></p>&mdash; Spooky Quantum Correlations (@quantum_graeme) <a href="https://twitter.com/quantum_graeme/status/1313299506499330050?ref_src=twsrc%5Etfw">October 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. D3Net: Densely connected multidilated DenseNet for music source  separation

Naoya Takahashi, Yuki Mitsufuji

- retweets: 250, favorites: 99 (10/07/2020 09:23:56)

- links: [abs](https://arxiv.org/abs/2010.01733) | [pdf](https://arxiv.org/pdf/2010.01733)
- [eess.AS](https://arxiv.org/list/eess.AS/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.SD](https://arxiv.org/list/cs.SD/recent)

Music source separation involves a large input field to model a long-term dependence of an audio signal. Previous convolutional neural network (CNN) -based approaches address the large input field modeling using sequentially down- and up-sampling feature maps or dilated convolution. In this paper, we claim the importance of a rapid growth of a receptive field and a simultaneous modeling of multi-resolution data in a single convolution layer, and propose a novel CNN architecture called densely connected dilated DenseNet (D3Net). D3Net involves a novel multi-dilated convolution that has different dilation factors in a single layer to model different resolutions simultaneously. By combining the multi-dilated convolution with DenseNet architecture, D3Net avoids the aliasing problem that exists when we naively incorporate the dilated convolution in DenseNet. Experimental results on MUSDB18 dataset show that D3Net achieves state-of-the-art performance with an average signal to distortion ratio (SDR) of 6.01 dB.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Happy to share our new work on music source separation that obtained SOTA result on MUSDB18. We improved MMDenseNet architecture by incorporating multidilated convolution.   <a href="https://t.co/0ND2RDOH4O">https://t.co/0ND2RDOH4O</a> <a href="https://t.co/JRzyNCVkAK">pic.twitter.com/JRzyNCVkAK</a></p>&mdash; Naoya Takahashi (@zuNaoya) <a href="https://twitter.com/zuNaoya/status/1313287304983662592?ref_src=twsrc%5Etfw">October 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">We&#39;ve been silent for a few years about our new model since the last SiSEC, but it does not mean that we&#39;ve stopped pursuing the best performance in music separation. Here is our latest model D3Net. <a href="https://twitter.com/hashtag/creativeAI?src=hash&amp;ref_src=twsrc%5Etfw">#creativeAI</a><a href="https://t.co/7YoMpQ6X4h">https://t.co/7YoMpQ6X4h</a> <a href="https://t.co/jRunfVCb4j">https://t.co/jRunfVCb4j</a></p>&mdash; Yuki Mitsufuji (@mittu1204) <a href="https://twitter.com/mittu1204/status/1313365812397264896?ref_src=twsrc%5Etfw">October 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 10. Towards ML Engineering: A Brief History Of TensorFlow Extended (TFX)

Konstantinos, Katsiapis, Abhijit Karmarkar, Ahmet Altay, Aleksandr Zaks, Neoklis Polyzotis, Anusha Ramesh, Ben Mathes, Gautam Vasudevan, Irene Giannoumis, Jarek Wilkiewicz, Jiri Simsa, Justin Hong, Mitch Trott, No√© Lutz, Pavel A. Dournov, Robert Crowe, Sarah Sirajuddin, Tris Brian Warkentin, Zhitao Li

- retweets: 274, favorites: 50 (10/07/2020 09:23:57)

- links: [abs](https://arxiv.org/abs/2010.02013) | [pdf](https://arxiv.org/pdf/2010.02013)
- [cs.SE](https://arxiv.org/list/cs.SE/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Software Engineering, as a discipline, has matured over the past 5+ decades. The modern world heavily depends on it, so the increased maturity of Software Engineering was an eventuality. Practices like testing and reliable technologies help make Software Engineering reliable enough to build industries upon. Meanwhile, Machine Learning (ML) has also grown over the past 2+ decades. ML is used more and more for research, experimentation and production workloads. ML now commonly powers widely-used products integral to our lives. But ML Engineering, as a discipline, has not widely matured as much as its Software Engineering ancestor. Can we take what we have learned and help the nascent field of applied ML evolve into ML Engineering the way Programming evolved into Software Engineering [1]? In this article we will give a whirlwind tour of Sibyl [2] and TensorFlow Extended (TFX) [3], two successive end-to-end (E2E) ML platforms at Alphabet. We will share the lessons learned from over a decade of applied ML built on these platforms, explain both their similarities and their differences, and expand on the shifts (both mental and technical) that helped us on our journey. In addition, we will highlight some of the capabilities of TFX that help realize several aspects of ML Engineering. We argue that in order to unlock the gains ML can bring, organizations should advance the maturity of their ML teams by investing in robust ML infrastructure and promoting ML Engineering education. We also recommend that before focusing on cutting-edge ML modeling techniques, product leaders should invest more time in adopting interoperable ML platforms for their organizations. In closing, we will also share a glimpse into the future of TFX.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Towards ML Engineering - a decade of production ML experience in one article: our <a href="https://twitter.com/hashtag/TFX?src=hash&amp;ref_src=twsrc%5Etfw">#TFX</a> paper is out! &quot;In this article we will give a whirlwind tour of Sibyl and <a href="https://twitter.com/TensorFlow?ref_src=twsrc%5Etfw">@TensorFlow</a> Extended (TFX),  two successive end-to-end (E2E) ML platforms at Alphabet.&quot;<a href="https://t.co/K3YQof2kK8">https://t.co/K3YQof2kK8</a> <a href="https://t.co/jRIJKBdmtI">pic.twitter.com/jRIJKBdmtI</a></p>&mdash; Jarek Wilkiewicz (@wjarek) <a href="https://twitter.com/wjarek/status/1313333263755141120?ref_src=twsrc%5Etfw">October 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 11. Sharpness-Aware Minimization for Efficiently Improving Generalization

Pierre Foret, Ariel Kleiner, Hossein Mobahi, Behnam Neyshabur

- retweets: 255, favorites: 47 (10/07/2020 09:23:57)

- links: [abs](https://arxiv.org/abs/2010.01412) | [pdf](https://arxiv.org/pdf/2010.01412)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

In today's heavily overparameterized models, the value of the training loss provides few guarantees on model generalization ability. Indeed, optimizing only the training loss value, as is commonly done, can easily lead to suboptimal model quality. Motivated by the connection between geometry of the loss landscape and generalization---including a generalization bound that we prove here---we introduce a novel, effective procedure for instead simultaneously minimizing loss value and loss sharpness. In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. We present empirical results showing that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-{10, 100}, ImageNet, finetuning tasks) and models, yielding novel state-of-the-art performance for several. Additionally, we find that SAM natively provides robustness to label noise on par with that provided by state-of-the-art procedures that specifically target learning with noisy labels.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Introducing SAM: An easy-to-use algorithm derived by connecting PAC Bayesian bounds and geometry of the loss landscape. Achieves SOTA on benchmark image tasks (0.3% error on CIFAR10, 3.9% on CIFAR100) and drastically improves label noise robustness.<a href="https://t.co/aONWVTPZsT">https://t.co/aONWVTPZsT</a></p>&mdash; Pierre Foret (@Foret_p) <a href="https://twitter.com/Foret_p/status/1313539609960345600?ref_src=twsrc%5Etfw">October 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 12. My Body is a Cage: the Role of Morphology in Graph-Based Incompatible  Control

Vitaly Kurin, Maximilian Igl, Tim Rockt√§schel, Wendelin Boehmer, Shimon Whiteson

- retweets: 225, favorites: 64 (10/07/2020 09:23:57)

- links: [abs](https://arxiv.org/abs/2010.01856) | [pdf](https://arxiv.org/pdf/2010.01856)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Multitask Reinforcement Learning is a promising way to obtain models with better performance, generalisation, data efficiency, and robustness. Most existing work is limited to compatible settings, where the state and action space dimensions are the same across tasks. Graph Neural Networks (GNN) are one way to address incompatible environments, because they can process graphs of arbitrary size. They also allow practitioners to inject biases encoded in the structure of the input graph. Existing work in graph-based continuous control uses the physical morphology of the agent to construct the input graph, i.e., encoding limb features as node labels and using edges to connect the nodes if their corresponded limbs are physically connected. In this work, we present a series of ablations on existing methods that show that morphological information encoded in the graph does not improve their performance. Motivated by the hypothesis that any benefits GNNs extract from the graph structure are outweighed by difficulties they create for message passing, we also propose Amorpheus, a transformer-based approach. Further results show that, while Amorpheus ignores the morphological information that GNNs encode, it nonetheless substantially outperforms GNN-based methods.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Meet Amorpheus <a href="https://t.co/8VgGw5OE5O">https://t.co/8VgGw5OE5O</a>! Surprisingly, and contrary to previous beliefs, we show that GNNs do not exploit morphological information when doing incompatible continuous control -&gt; we can use transformers here! Work with <a href="https://twitter.com/MaxiIgl?ref_src=twsrc%5Etfw">@MaxiIgl</a> <a href="https://twitter.com/_rockt?ref_src=twsrc%5Etfw">@_rockt</a> <a href="https://twitter.com/WendelinBoehmer?ref_src=twsrc%5Etfw">@WendelinBoehmer</a> <a href="https://twitter.com/shimon8282?ref_src=twsrc%5Etfw">@shimon8282</a> <a href="https://t.co/BZ2uI5aDhe">pic.twitter.com/BZ2uI5aDhe</a></p>&mdash; yobibyte (@y0b1byte) <a href="https://twitter.com/y0b1byte/status/1313380297212395521?ref_src=twsrc%5Etfw">October 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 13. Pareto Probing: Trading Off Accuracy for Complexity

Tiago Pimentel, Naomi Saphra, Adina Williams, Ryan Cotterell

- retweets: 182, favorites: 85 (10/07/2020 09:23:57)

- links: [abs](https://arxiv.org/abs/2010.02180) | [pdf](https://arxiv.org/pdf/2010.02180)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

The question of how to probe contextual word representations in a way that is principled and useful has seen significant recent attention. In our contribution to this discussion, we argue, first, for a probe metric that reflects the trade-off between probe complexity and performance: the Pareto hypervolume. To measure complexity, we present a number of parametric and non-parametric metrics. Our experiments with such metrics show that probe's performance curves often fail to align with widely accepted rankings between language representations (with, e.g., non-contextual representations outperforming contextual ones). These results lead us to argue, second, that common simplistic probe tasks such as POS labeling and dependency arc labeling, are inadequate to evaluate the properties encoded in contextual word representations. We propose full dependency parsing as an example probe task, and demonstrate it with the Pareto hypervolume. In support of our arguments, the results of this illustrative experiment conform closer to accepted rankings among contextual word representations.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Should our probes be simple or accurate? <a href="https://twitter.com/nsaphra?ref_src=twsrc%5Etfw">@nsaphra</a> and I reframe this trade off with Pareto fronts in our new <a href="https://twitter.com/hashtag/emnlp2020?src=hash&amp;ref_src=twsrc%5Etfw">#emnlp2020</a> paper with <a href="https://twitter.com/adinamwilliams?ref_src=twsrc%5Etfw">@adinamwilliams</a> and <a href="https://twitter.com/ryandcotterell?ref_src=twsrc%5Etfw">@ryandcotterell</a> <a href="https://t.co/bNGObN6xME">https://t.co/bNGObN6xME</a><br><br>Follow the thread :) <a href="https://t.co/e234V2TvWV">pic.twitter.com/e234V2TvWV</a></p>&mdash; Tiago Pimentel (@tpimentelms) <a href="https://twitter.com/tpimentelms/status/1313404634350989312?ref_src=twsrc%5Etfw">October 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 14. MagGAN: High-Resolution Face Attribute Editing with Mask-Guided  Generative Adversarial Network

Yi Wei, Zhe Gan, Wenbo Li, Siwei Lyu, Ming-Ching Chang, Lei Zhang, Jianfeng Gao, Pengchuan Zhang

- retweets: 171, favorites: 75 (10/07/2020 09:23:57)

- links: [abs](https://arxiv.org/abs/2010.01424) | [pdf](https://arxiv.org/pdf/2010.01424)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.MM](https://arxiv.org/list/cs.MM/recent)

We present Mask-guided Generative Adversarial Network (MagGAN) for high-resolution face attribute editing, in which semantic facial masks from a pre-trained face parser are used to guide the fine-grained image editing process. With the introduction of a mask-guided reconstruction loss, MagGAN learns to only edit the facial parts that are relevant to the desired attribute changes, while preserving the attribute-irrelevant regions (e.g., hat, scarf for modification `To Bald'). Further, a novel mask-guided conditioning strategy is introduced to incorporate the influence region of each attribute change into the generator. In addition, a multi-level patch-wise discriminator structure is proposed to scale our model for high-resolution ($1024 \times 1024$) face editing. Experiments on the CelebA benchmark show that the proposed method significantly outperforms prior state-of-the-art approaches in terms of both image quality and editing performance.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">MagGAN: High-Resolution Face Attribute Editing with Mask-Guided Generative Adversarial Network<br>pdf: <a href="https://t.co/YYYguAapDI">https://t.co/YYYguAapDI</a><br>abs: <a href="https://t.co/Ys0dFKHUTI">https://t.co/Ys0dFKHUTI</a> <a href="https://t.co/3qUWjyGI6k">pic.twitter.com/3qUWjyGI6k</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1313309359791894529?ref_src=twsrc%5Etfw">October 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 15. No quantum speedup over gradient descent for non-smooth convex  optimization

Ankit Garg, Robin Kothari, Praneeth Netrapalli, Suhail Sherif

- retweets: 162, favorites: 77 (10/07/2020 09:23:57)

- links: [abs](https://arxiv.org/abs/2010.01801) | [pdf](https://arxiv.org/pdf/2010.01801)
- [cs.DS](https://arxiv.org/list/cs.DS/recent) | [math.OC](https://arxiv.org/list/math.OC/recent) | [quant-ph](https://arxiv.org/list/quant-ph/recent)

We study the first-order convex optimization problem, where we have black-box access to a (not necessarily smooth) function $f:\mathbb{R}^n \to \mathbb{R}$ and its (sub)gradient. Our goal is to find an $\epsilon$-approximate minimum of $f$ starting from a point that is distance at most $R$ from the true minimum. If $f$ is $G$-Lipschitz, then the classic gradient descent algorithm solves this problem with $O((GR/\epsilon)^{2})$ queries. Importantly, the number of queries is independent of the dimension $n$ and gradient descent is optimal in this regard: No deterministic or randomized algorithm can achieve better complexity that is still independent of the dimension $n$.   In this paper we reprove the randomized lower bound of $\Omega((GR/\epsilon)^{2})$ using a simpler argument than previous lower bounds. We then show that although the function family used in the lower bound is hard for randomized algorithms, it can be solved using $O(GR/\epsilon)$ quantum queries. We then show an improved lower bound against quantum algorithms using a different set of instances and establish our main result that in general even quantum algorithms need $\Omega((GR/\epsilon)^2)$ queries to solve the problem. Hence there is no quantum speedup over gradient descent for black-box first-order convex optimization without further assumptions on the function family.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">New paper on the arXiv with Ankit Garg (<a href="https://twitter.com/Ankit_garg06?ref_src=twsrc%5Etfw">@Ankit_garg06</a>), Praneeth Netrapalli (<a href="https://twitter.com/PNetrapalli?ref_src=twsrc%5Etfw">@PNetrapalli</a>), and Suhail Sherif (<a href="https://twitter.com/shlshrf?ref_src=twsrc%5Etfw">@shlshrf</a>): &quot;No quantum speedup over gradient descent<br>for non-smooth convex optimization.&quot;<a href="https://t.co/zqz2sIWNFg">https://t.co/zqz2sIWNFg</a> <a href="https://t.co/NQWLzRjyuL">pic.twitter.com/NQWLzRjyuL</a></p>&mdash; Robin Kothari (@RobinKothari) <a href="https://twitter.com/RobinKothari/status/1313295288203800576?ref_src=twsrc%5Etfw">October 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 16. Optimal bounds for approximate counting

Jelani Nelson, Huacheng Yu

- retweets: 64, favorites: 63 (10/07/2020 09:23:58)

- links: [abs](https://arxiv.org/abs/2010.02116) | [pdf](https://arxiv.org/pdf/2010.02116)
- [cs.DS](https://arxiv.org/list/cs.DS/recent) | [cs.DM](https://arxiv.org/list/cs.DM/recent)

Storing a counter incremented $N$ times would naively consume $O(\log N)$ bits of memory. In 1978 Morris described the very first streaming algorithm: the "Morris Counter" [Morris78]. His algorithm has been shown to use $O(\log\log N + \log(1/\varepsilon) + \log(1/\delta))$ bits of memory to provide a $(1+\varepsilon)$-approximation with probability $1-\delta$ to the counter's value. We provide a new simple algorithm with a simple analysis showing that $O(\log\log N + \log(1/\varepsilon) + \log\log(1/\delta))$ bits suffice for the same task, i.e. an exponentially improved dependence on the inverse failure probability. We then provide a more technical analysis showing that the original Morris Counter itself, after a minor but necessary tweak, actually also enjoys this same improved upper bound. Lastly, we prove a new lower bound for this task showing optimality of our upper bound. We thus completely resolve the asymptotic space complexity of approximate counting.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">A cool looking paper by Jelani Nelson (<a href="https://twitter.com/minilek?ref_src=twsrc%5Etfw">@minilek</a>) and Huacheng Yu: presents a better analysis of the Morris counter, getting an exponentially better bound in terms of the error parameter.<br>Be sure to check out Section 4 on the value of implementation :)<a href="https://t.co/Euev7DKduw">https://t.co/Euev7DKduw</a> <a href="https://t.co/cX4MMKCuur">pic.twitter.com/cX4MMKCuur</a></p>&mdash; Gautam Kamath (@thegautamkamath) <a href="https://twitter.com/thegautamkamath/status/1313288285033291776?ref_src=twsrc%5Etfw">October 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 17. Non-trivial informational closure of a Bayesian hyperparameter

Martin Biehl, Ryota Kanai

- retweets: 104, favorites: 22 (10/07/2020 09:23:58)

- links: [abs](https://arxiv.org/abs/2010.01855) | [pdf](https://arxiv.org/pdf/2010.01855)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

We investigate the non-trivial informational closure (NTIC) of a Bayesian hyperparameter inferring the underlying distribution of an identically and independently distributed finite random variable. For this we embed both the Bayesian hyper-parameter updating process and the random data process into a Markov chain. The original publication by Bertschinger et al. (2006) mentioned that NTIC may be able to capture an abstract notion of modeling that is agnostic to the specific internal structure of and existence of explicit representations within the modeling process. The Bayesian hyperparameter is of interest since it has a well defined interpretation as a model of the data process and at the same time its dynamics can be specified without reference to this interpretation. On the one hand we show explicitly that the NTIC of the hyperparameter increases indefinitely over time. On the other hand we attempt to establish a connection between a quantity that is a feature of the interpretation of the hyperparameter as a model, namely the information gain, and the one-step pointwise NTIC which is a quantity that does not depend on this interpretation. We find that in general we cannot use the one-step pointwise NTIC as an indicator for information gain. We hope this exploratory work can lead to further rigorous studies of the relation between NTIC and modeling.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">New paper on NTIC with Martin Biehl (<a href="https://twitter.com/36zimmer?ref_src=twsrc%5Etfw">@36zimmer</a> ) to be presented at SSCI 2020. <br><br>Non-trivial informational closure of a Bayesian hyperparameter.<a href="https://t.co/1NR7PvHZvr">https://t.co/1NR7PvHZvr</a></p>&mdash; Ryota Kanai (@kanair) <a href="https://twitter.com/kanair/status/1313297946251677696?ref_src=twsrc%5Etfw">October 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 18. Codes for Distributed Storage

Vinayak Ramkumar, Myna Vajha, S. B. Balaji, M. Nikhil Krishnan, Birenjith Sasidharan, P. Vijay Kumar

- retweets: 49, favorites: 52 (10/07/2020 09:23:58)

- links: [abs](https://arxiv.org/abs/2010.01344) | [pdf](https://arxiv.org/pdf/2010.01344)
- [cs.IT](https://arxiv.org/list/cs.IT/recent)

This chapter deals with the topic of designing reliable and efficient codes for the storage and retrieval of large quantities of data over storage devices that are prone to failure. For long, the traditional objective has been one of ensuring reliability against data loss while minimizing storage overhead. More recently, a third concern has surfaced, namely of the need to efficiently recover from the failure of a single storage unit, corresponding to recovery from the erasure of a single code symbol. We explain here, how coding theory has evolved to tackle this fresh challenge.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">A comprehensive and updated survey on Coding theory for Distributed storage, a topic very close to my heart since my PhD. Beautiful combinatorial constructions and real impact in systems like Hadoop HDFS, Azure cloud and Ceph. <a href="https://t.co/5YUqn9QEp4">https://t.co/5YUqn9QEp4</a></p>&mdash; Alex Dimakis (@AlexGDimakis) <a href="https://twitter.com/AlexGDimakis/status/1313367857242177538?ref_src=twsrc%5Etfw">October 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 19. Mossad: Defeating Software Plagiarism Detection

Breanna Devore-McDonald, Emery D. Berger

- retweets: 63, favorites: 38 (10/07/2020 09:23:58)

- links: [abs](https://arxiv.org/abs/2010.01700) | [pdf](https://arxiv.org/pdf/2010.01700)
- [cs.CR](https://arxiv.org/list/cs.CR/recent) | [cs.CY](https://arxiv.org/list/cs.CY/recent) | [cs.NE](https://arxiv.org/list/cs.NE/recent) | [cs.PL](https://arxiv.org/list/cs.PL/recent)

Automatic software plagiarism detection tools are widely used in educational settings to ensure that submitted work was not copied. These tools have grown in use together with the rise in enrollments in computer science programs and the widespread availability of code on-line. Educators rely on the robustness of plagiarism detection tools; the working assumption is that the effort required to evade detection is as high as that required to actually do the assigned work.   This paper shows this is not the case. It presents an entirely automatic program transformation approach, Mossad, that defeats popular software plagiarism detection tools. Mossad comprises a framework that couples techniques inspired by genetic programming with domain-specific knowledge to effectively undermine plagiarism detectors. Mossad is effective at defeating four plagiarism detectors, including Moss and JPlag. Mossad is both fast and effective: it can, in minutes, generate modified versions of programs that are likely to escape detection. More insidiously, because of its non-deterministic approach, Mossad can, from a single program, generate dozens of variants, which are classified as no more suspicious than legitimate assignments. A detailed study of Mossad across a corpus of real student assignments demonstrates its efficacy at evading detection. A user study shows that graduate student assistants consistently rate Mossad-generated code as just as readable as authentic student code. This work motivates the need for both research on more robust plagiarism detection tools and greater integration of naturally plagiarism-resistant methodologies like code review into computer science education.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">&quot;Mossad: Defeating Software Plagiarism Detection&quot; (including Moss and JPlag) - <a href="https://t.co/7AAU2NDRvy">https://t.co/7AAU2NDRvy</a>, to appear OOPSLA 2020 <a href="https://twitter.com/bdevorem?ref_src=twsrc%5Etfw">@bdevorem</a> <a href="https://twitter.com/emeryberger?ref_src=twsrc%5Etfw">@emeryberger</a> <a href="https://t.co/y1gnfeioAv">pic.twitter.com/y1gnfeioAv</a></p>&mdash; Emery Berger (@emeryberger) <a href="https://twitter.com/emeryberger/status/1313550999051739137?ref_src=twsrc%5Etfw">October 6, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 20. A Survey of Unsupervised Dependency Parsing

Wenjuan Han, Yong Jiang, Hwee Tou Ng, Kewei Tu

- retweets: 42, favorites: 18 (10/07/2020 09:23:58)

- links: [abs](https://arxiv.org/abs/2010.01535) | [pdf](https://arxiv.org/pdf/2010.01535)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

Syntactic dependency parsing is an important task in natural language processing. Unsupervised dependency parsing aims to learn a dependency parser from sentences that have no annotation of their correct parse trees. Despite its difficulty, unsupervised parsing is an interesting research direction because of its capability of utilizing almost unlimited unannotated text data. It also serves as the basis for other research in low-resource parsing. In this paper, we survey existing approaches to unsupervised dependency parsing, identify two major classes of approaches, and discuss recent trends. We hope that our survey can provide insights for researchers and facilitate future research on this topic.




# 21. Episodic Memory for Learning Subjective-Timescale Models

Alexey Zakharov, Matthew Crosby, Zafeirios Fountas

- retweets: 40, favorites: 19 (10/07/2020 09:23:58)

- links: [abs](https://arxiv.org/abs/2010.01430) | [pdf](https://arxiv.org/pdf/2010.01430)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [q-bio.NC](https://arxiv.org/list/q-bio.NC/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

In model-based learning, an agent's model is commonly defined over transitions between consecutive states of an environment even though planning often requires reasoning over multi-step timescales, with intermediate states either unnecessary, or worse, accumulating prediction error. In contrast, intelligent behaviour in biological organisms is characterised by the ability to plan over varying temporal scales depending on the context. Inspired by the recent works on human time perception, we devise a novel approach to learning a transition dynamics model, based on the sequences of episodic memories that define the agent's subjective timescale - over which it learns world dynamics and over which future planning is performed. We implement this in the framework of active inference and demonstrate that the resulting subjective-timescale model (STM) can systematically vary the temporal extent of its predictions while preserving the same computational efficiency. Additionally, we show that STM predictions are more likely to introduce future salient events (for example new objects coming into view), incentivising exploration of new areas of the environment. As a result, STM produces more informative action-conditioned roll-outs that assist the agent in making better decisions. We validate significant improvement in our STM agent's performance in the Animal-AI environment against a baseline system, trained using the environment's objective-timescale dynamics.



