---
title: Hot Papers 2021-05-04
date: 2021-05-05T15:23:25.Z
template: "post"
draft: false
slug: "hot-papers-2021-05-04"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2021-05-04"
socialImage: "/media/flying-marine.jpg"

---

# 1. Audio Transformers:Transformer Architectures For Large Scale Audio  Understanding. Adieu Convolutions

Prateek Verma, Jonathan Berger

- retweets: 1719, favorites: 194 (05/05/2021 15:23:25)

- links: [abs](https://arxiv.org/abs/2105.00335) | [pdf](https://arxiv.org/pdf/2105.00335)
- [cs.SD](https://arxiv.org/list/cs.SD/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [eess.AS](https://arxiv.org/list/eess.AS/recent)

Over the past two decades, CNN architectures have produced compelling models of sound perception and cognition, learning hierarchical organizations of features. Analogous to successes in computer vision, audio feature classification can be optimized for a particular task of interest, over a wide variety of datasets and labels. In fact similar architectures designed for image understanding have proven effective for acoustic scene analysis. Here we propose applying Transformer based architectures without convolutional layers to raw audio signals. On a standard dataset of Free Sound 50K,comprising of 200 categories, our model outperforms convolutional models to produce state of the art results. This is significant as unlike in natural language processing and computer vision, we do not perform unsupervised pre-training for outperforming convolutional architectures. On the same training set, with respect mean aver-age precision benchmarks, we show a significant improvement. We further improve the performance of Transformer architectures by using techniques such as pooling inspired from convolutional net-work designed in the past few years. In addition, we also show how multi-rate signal processing ideas inspired from wavelets, can be applied to the Transformer embeddings to improve the results. We also show how our models learns a non-linear non constant band-width filter-bank, which shows an adaptable time frequency front end representation for the task of audio understanding, different from other tasks e.g. pitch estimation.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Audio Transformers:Transformer Architectures For Large Scale Audio Understanding. Adieu Convolutions<br>pdf: <a href="https://t.co/4Bgj34xfP8">https://t.co/4Bgj34xfP8</a><br>abs: <a href="https://t.co/q7IqEITpGU">https://t.co/q7IqEITpGU</a><br><br>Transformer architecture without using any convolutional filters can be adapted for large scale audio understanding <a href="https://t.co/wxccxGWbP4">pic.twitter.com/wxccxGWbP4</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1389387607159754755?ref_src=twsrc%5Etfw">May 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Larger-Scale Transformers for Multilingual Masked Language Modeling

Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau

- retweets: 908, favorites: 163 (05/05/2021 15:23:26)

- links: [abs](https://arxiv.org/abs/2105.00572) | [pdf](https://arxiv.org/pdf/2105.00572)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

Recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. Our two new models dubbed XLM-R XL and XLM-R XXL outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages. This suggests pretrained models with larger capacity may obtain both strong performance on high-resource languages while greatly improving low-resource languages. We make our code and models publicly available.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Larger-Scale Transformers for Multilingual Masked Language Modeling<br><br>Presents 10B XLM-R, which obtains both strong performance on high-resource languages while greatly improving low-resource languages.<a href="https://t.co/GoBsRVVMVg">https://t.co/GoBsRVVMVg</a> <a href="https://t.co/hIDen2yEbX">pic.twitter.com/hIDen2yEbX</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1389387616739500034?ref_src=twsrc%5Etfw">May 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Data-driven discovery of physical laws with human-understandable deep  learning

Nicolas Boull√©, Christopher J. Earls, Alex Townsend

- retweets: 363, favorites: 94 (05/05/2021 15:23:26)

- links: [abs](https://arxiv.org/abs/2105.00266) | [pdf](https://arxiv.org/pdf/2105.00266)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [math.NA](https://arxiv.org/list/math.NA/recent)

There is an opportunity for deep learning to revolutionize science and technology by revealing its findings in a human interpretable manner. We develop a novel data-driven approach for creating a human-machine partnership to accelerate scientific discovery. By collecting physical system responses, under carefully selected excitations, we train rational neural networks to learn Green's functions of hidden partial differential equation. These solutions reveal human-understandable properties and features, such as linear conservation laws, and symmetries, along with shock and singularity locations, boundary effects, and dominant modes. We illustrate this technique on several examples and capture a range of physics, including advection-diffusion, viscous shocks, and Stokes flow in a lid-driven cavity.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">‚ÄúData-driven discovery of physical laws with human-understandable deep learning‚Äù. Exciting new preprint from Nicolas Boull√©, Chris Earls and Alex Townsend (<a href="https://t.co/c6R6x8KbQk">https://t.co/c6R6x8KbQk</a>) on developing a partnership between humans and AI to solve hard scientific problems understandably. <a href="https://t.co/7QgjMSNKIj">pic.twitter.com/7QgjMSNKIj</a></p>&mdash; Steven Strogatz (@stevenstrogatz) <a href="https://twitter.com/stevenstrogatz/status/1389721208464285696?ref_src=twsrc%5Etfw">May 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Neural Monocular 3D Human Motion Capture with Physical Awareness

Soshi Shimada, Vladislav Golyanik, Weipeng Xu, Patrick P√©rez, Christian Theobalt

- retweets: 306, favorites: 88 (05/05/2021 15:23:26)

- links: [abs](https://arxiv.org/abs/2105.01057) | [pdf](https://arxiv.org/pdf/2105.01057)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.GR](https://arxiv.org/list/cs.GR/recent) | [cs.HC](https://arxiv.org/list/cs.HC/recent)

We present a new trainable system for physically plausible markerless 3D human motion capture, which achieves state-of-the-art results in a broad range of challenging scenarios. Unlike most neural methods for human motion capture, our approach, which we dub physionical, is aware of physical and environmental constraints. It combines in a fully differentiable way several key innovations, i.e., 1. a proportional-derivative controller, with gains predicted by a neural network, that reduces delays even in the presence of fast motions, 2. an explicit rigid body dynamics model and 3. a novel optimisation layer that prevents physically implausible foot-floor penetration as a hard constraint. The inputs to our system are 2D joint keypoints, which are canonicalised in a novel way so as to reduce the dependency on intrinsic camera parameters -- both at train and test time. This enables more accurate global translation estimation without generalisability loss. Our model can be finetuned only with 2D annotations when the 3D annotations are not available. It produces smooth and physically principled 3D motions in an interactive frame rate in a wide variety of challenging scenes, including newly recorded ones. Its advantages are especially noticeable on in-the-wild sequences that significantly differ from common 3D pose estimation benchmarks such as Human 3.6M and MPI-INF-3DHP. Qualitative results are available at http://gvv.mpi-inf.mpg.de/projects/PhysAware/

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Neural Monocular 3D Human Motion Capture with Physical Awareness<br>pdf: <a href="https://t.co/RJuNz1TLFO">https://t.co/RJuNz1TLFO</a><br>abs: <a href="https://t.co/NtVFc5AyPY">https://t.co/NtVFc5AyPY</a><br>project page: <a href="https://t.co/Q8in1ThmY4">https://t.co/Q8in1ThmY4</a> <a href="https://t.co/Vgch18Mtet">pic.twitter.com/Vgch18Mtet</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1389399230456938501?ref_src=twsrc%5Etfw">May 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Searchable Hidden Intermediates for End-to-End Models of Decomposable  Sequence Tasks

Siddharth Dalmia, Brian Yan, Vikas Raunak, Florian Metze, Shinji Watanabe

- retweets: 272, favorites: 43 (05/05/2021 15:23:26)

- links: [abs](https://arxiv.org/abs/2105.00573) | [pdf](https://arxiv.org/pdf/2105.00573)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [eess.AS](https://arxiv.org/list/eess.AS/recent)

End-to-end approaches for sequence tasks are becoming increasingly popular. Yet for complex sequence tasks, like speech translation, systems that cascade several models trained on sub-tasks have shown to be superior, suggesting that the compositionality of cascaded systems simplifies learning and enables sophisticated search capabilities. In this work, we present an end-to-end framework that exploits compositionality to learn searchable hidden representations at intermediate stages of a sequence model using decomposed sub-tasks. These hidden intermediates can be improved using beam search to enhance the overall performance and can also incorporate external models at intermediate stages of the network to re-score or adapt towards out-of-domain data. One instance of the proposed framework is a Multi-Decoder model for speech translation that extracts the searchable hidden intermediates from a speech recognition sub-task. The model demonstrates the aforementioned benefits and outperforms the previous state-of-the-art by around +6 and +3 BLEU on the two test sets of Fisher-CallHome and by around +3 and +4 BLEU on the English-German and English-French test sets of MuST-C.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">For complex sequence tasks, natural decomposition in cascaded models yields superior inference over E2E models. <br>In our <a href="https://twitter.com/hashtag/NAACL2021?src=hash&amp;ref_src=twsrc%5Etfw">#NAACL2021</a> paper <a href="https://t.co/cvGEGyFsWp">https://t.co/cvGEGyFsWp</a>, we present Searchable Hidden Intermediates to decompose sequence models while maintaining E2E differentiability. (1/N) <a href="https://t.co/BiASkVf69W">pic.twitter.com/BiASkVf69W</a></p>&mdash; Siddharth Dalmia (@siddalmia05) <a href="https://twitter.com/siddalmia05/status/1389625930818199560?ref_src=twsrc%5Etfw">May 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. Self-supervised Augmentation Consistency for Adapting Semantic  Segmentation

Nikita Araslanov, Stefan Roth

- retweets: 195, favorites: 105 (05/05/2021 15:23:26)

- links: [abs](https://arxiv.org/abs/2105.00097) | [pdf](https://arxiv.org/pdf/2105.00097)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

We propose an approach to domain adaptation for semantic segmentation that is both practical and highly accurate. In contrast to previous work, we abandon the use of computationally involved adversarial objectives, network ensembles and style transfer. Instead, we employ standard data augmentation techniques $-$ photometric noise, flipping and scaling $-$ and ensure consistency of the semantic predictions across these image transformations. We develop this principle in a lightweight self-supervised framework trained on co-evolving pseudo labels without the need for cumbersome extra training rounds. Simple in training from a practitioner's standpoint, our approach is remarkably effective. We achieve significant improvements of the state-of-the-art segmentation accuracy after adaptation, consistent both across different choices of the backbone architecture and adaptation scenarios.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Self-supervised Augmentation Consistency for Adapting Semantic Segmentation<br>pdf: <a href="https://t.co/4eSx78aV93">https://t.co/4eSx78aV93</a><br>abs: <a href="https://t.co/19u97Oko0H">https://t.co/19u97Oko0H</a><br>github: <a href="https://t.co/aQ4cd3ifT6">https://t.co/aQ4cd3ifT6</a> <a href="https://t.co/WN8CUpPATH">pic.twitter.com/WN8CUpPATH</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1389392784810024968?ref_src=twsrc%5Etfw">May 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. BERT memorisation and pitfalls in low-resource scenarios

Michael T√§nzer, Sebastian Ruder, Marek Rei

- retweets: 196, favorites: 64 (05/05/2021 15:23:27)

- links: [abs](https://arxiv.org/abs/2105.00828) | [pdf](https://arxiv.org/pdf/2105.00828)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

State-of-the-art pre-trained models have been shown to memorise facts and perform well with limited amounts of training data. To gain a better understanding of how these models learn, we study their generalisation and memorisation capabilities in noisy and low-resource scenarios. We find that the training of these models is almost unaffected by label noise and that it is possible to reach near-optimal performances even on extremely noisy datasets. Conversely, we also find that they completely fail when tested on low-resource tasks such as few-shot learning and rare entity recognition. To mitigate such limitations, we propose a novel architecture based on BERT and prototypical networks that improves performance in low-resource named entity recognition tasks.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">BERT memorisation and pitfalls in low-resource scenarios<br>pdf: <a href="https://t.co/XS8uUh81bG">https://t.co/XS8uUh81bG</a><br>abs: <a href="https://t.co/Adfmq5Etix">https://t.co/Adfmq5Etix</a><br><br>enabling BERT to perform well in extremely low-resource scenarios and also achieves comparable performance in non low-resource settings <a href="https://t.co/P8dD9oQe2k">pic.twitter.com/P8dD9oQe2k</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1389435319154511873?ref_src=twsrc%5Etfw">May 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. One Model to Rule them All: Towards Zero-Shot Learning for Databases

Benjamin Hilprecht, Carsten Binnig

- retweets: 182, favorites: 66 (05/05/2021 15:23:27)

- links: [abs](https://arxiv.org/abs/2105.00642) | [pdf](https://arxiv.org/pdf/2105.00642)
- [cs.DB](https://arxiv.org/list/cs.DB/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

In this paper, we present our vision of so called zero-shot learning for databases which is a new learning approach for database components. Zero-shot learning for databases is inspired by recent advances in transfer learning of models such as GPT-3 and can support a new database out-of-the box without the need to train a new model. As a first concrete contribution in this paper, we show the feasibility of zero-shot learning for the task of physical cost estimation and present very promising initial results. Moreover, as a second contribution we discuss the core challenges related to zero-shot learning for databases and present a roadmap to extend zero-shot learning towards many other tasks beyond cost estimation or even beyond classical database systems and workloads.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">One Model to Rule them All: Towards Zero-Shot Learning for Databases<br>pdf: <a href="https://t.co/hoNBr4qONK">https://t.co/hoNBr4qONK</a><br>abs: <a href="https://t.co/Sb9IRmIoYI">https://t.co/Sb9IRmIoYI</a><br><br>a new approach for learned database components that can support new databases without running any training query on that database <a href="https://t.co/O7SnSTegGp">pic.twitter.com/O7SnSTegGp</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1389386069968359431?ref_src=twsrc%5Etfw">May 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. ISTR: End-to-End Instance Segmentation with Transformers

Jie Hu, Liujuan Cao, Lu Yao, ShengChuan Zhang, Yan Wang, Ke Li, Feiyue Huang, Rongrong Ji, Ling Shao

- retweets: 188, favorites: 53 (05/05/2021 15:23:27)

- links: [abs](https://arxiv.org/abs/2105.00637) | [pdf](https://arxiv.org/pdf/2105.00637)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

End-to-end paradigms significantly improve the accuracy of various deep-learning-based computer vision models. To this end, tasks like object detection have been upgraded by replacing non-end-to-end components, such as removing non-maximum suppression by training with a set loss based on bipartite matching. However, such an upgrade is not applicable to instance segmentation, due to its significantly higher output dimensions compared to object detection. In this paper, we propose an instance segmentation Transformer, termed ISTR, which is the first end-to-end framework of its kind. ISTR predicts low-dimensional mask embeddings, and matches them with ground truth mask embeddings for the set loss. Besides, ISTR concurrently conducts detection and segmentation with a recurrent refinement strategy, which provides a new way to achieve instance segmentation compared to the existing top-down and bottom-up frameworks. Benefiting from the proposed end-to-end mechanism, ISTR demonstrates state-of-the-art performance even with approximation-based suboptimal embeddings. Specifically, ISTR obtains a 46.8/38.6 box/mask AP using ResNet50-FPN, and a 48.1/39.9 box/mask AP using ResNet101-FPN, on the MS COCO dataset. Quantitative and qualitative results reveal the promising potential of ISTR as a solid baseline for instance-level recognition. Code has been made available at: https://github.com/hujiecpp/ISTR.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">ISTR: End-to-End Instance Segmentation with Transformers<br>pdf: <a href="https://t.co/74h64ISrYA">https://t.co/74h64ISrYA</a><br>abs: <a href="https://t.co/IwkjtwjGBZ">https://t.co/IwkjtwjGBZ</a><br>github: <a href="https://t.co/NPH3BMiBoW">https://t.co/NPH3BMiBoW</a><br><br>ISTR predicts low-dimensional mask embeddings, and matches them with ground truth mask embeddings for the set loss <a href="https://t.co/A9EgdYr6Vy">pic.twitter.com/A9EgdYr6Vy</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1389426119586222087?ref_src=twsrc%5Etfw">May 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 10. Learning Visually Guided Latent Actions for Assistive Teleoperation

Siddharth Karamcheti, Albert J. Zhai, Dylan P. Losey, Dorsa Sadigh

- retweets: 156, favorites: 58 (05/05/2021 15:23:27)

- links: [abs](https://arxiv.org/abs/2105.00580) | [pdf](https://arxiv.org/pdf/2105.00580)
- [cs.RO](https://arxiv.org/list/cs.RO/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.HC](https://arxiv.org/list/cs.HC/recent) | [eess.SY](https://arxiv.org/list/eess.SY/recent)

It is challenging for humans -- particularly those living with physical disabilities -- to control high-dimensional, dexterous robots. Prior work explores learning embedding functions that map a human's low-dimensional inputs (e.g., via a joystick) to complex, high-dimensional robot actions for assistive teleoperation; however, a central problem is that there are many more high-dimensional actions than available low-dimensional inputs. To extract the correct action and maximally assist their human controller, robots must reason over their context: for example, pressing a joystick down when interacting with a coffee cup indicates a different action than when interacting with knife. In this work, we develop assistive robots that condition their latent embeddings on visual inputs. We explore a spectrum of visual encoders and show that incorporating object detectors pretrained on small amounts of cheap, easy-to-collect structured data enables i) accurately and robustly recognizing the current context and ii) generalizing control embeddings to new objects and tasks. In user studies with a high-dimensional physical robot arm, participants leverage this approach to perform new tasks with unseen objects. Our results indicate that structured visual representations improve few-shot performance and are subjectively preferred by users.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">How do we build visually guided controllers that help humans operate complex robots?<br><br>Thrilled to share our <a href="https://twitter.com/hashtag/L4DC?src=hash&amp;ref_src=twsrc%5Etfw">#L4DC</a> paper &quot;Learning Visually Guided Latent Actions for Assistive Teleoperation&quot; with Albert Zhai, <a href="https://twitter.com/loseydp?ref_src=twsrc%5Etfw">@loseydp</a>, and <a href="https://twitter.com/DorsaSadigh?ref_src=twsrc%5Etfw">@DorsaSadigh</a>!<br><br>Paper: <a href="https://t.co/oe2IvzFZzV">https://t.co/oe2IvzFZzV</a><br><br>A üßµ [1/7] <a href="https://t.co/gmPCj0Wzzi">pic.twitter.com/gmPCj0Wzzi</a></p>&mdash; Siddharth Karamcheti (@siddkaramcheti) <a href="https://twitter.com/siddkaramcheti/status/1389622396512923652?ref_src=twsrc%5Etfw">May 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 11. Deployment Archetypes for Cloud Applications

Anna Berenberg, Brad Calder

- retweets: 156, favorites: 36 (05/05/2021 15:23:27)

- links: [abs](https://arxiv.org/abs/2105.00560) | [pdf](https://arxiv.org/pdf/2105.00560)
- [cs.DC](https://arxiv.org/list/cs.DC/recent) | [cs.NI](https://arxiv.org/list/cs.NI/recent)

This is a survey paper that explores six Cloud-based deployment archetypes for Cloud applications and the tradeoffs between them to achieve high availability, low end-user latency, and acceptable costs. These are (1) Zonal, (2) Regional, (3) Multi-Regional, (4) Global, (5) Hybrid, and (6) Multi-Cloud deployment archetypes. The goal is to classify cloud applications into a set of deployment archetypes and deployment models that tradeoff their needs around availability, latency, and geographical constraints with a focus on serving applications. This enables application owners to better examine the tradeoffs of each deployment model and what is needed for achieving the availability and latency goals for their application.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr"><a href="https://twitter.com/CalderBrad?ref_src=twsrc%5Etfw">@CalderBrad</a> and I published <a href="https://t.co/USwdvCd22P">https://t.co/USwdvCd22P</a> about how to build HA cloud apps based on 6 deployment archetypes 1/3<a href="https://twitter.com/googlecloud?ref_src=twsrc%5Etfw">@googlecloud</a> <a href="https://twitter.com/GoogleCloudTech?ref_src=twsrc%5Etfw">@googlecloudtech</a> <a href="https://twitter.com/hashtag/reliability?src=hash&amp;ref_src=twsrc%5Etfw">#reliability</a> <a href="https://twitter.com/hashtag/CloudComputing?src=hash&amp;ref_src=twsrc%5Etfw">#CloudComputing</a></p>&mdash; Anna Berenberg (@kniga) <a href="https://twitter.com/kniga/status/1389406448430456835?ref_src=twsrc%5Etfw">May 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 12. GridToPix: Training Embodied Agents with Minimal Supervision

Unnat Jain, Iou-Jen Liu, Svetlana Lazebnik, Aniruddha Kembhavi, Luca Weihs, Alexander Schwing

- retweets: 156, favorites: 23 (05/05/2021 15:23:27)

- links: [abs](https://arxiv.org/abs/2105.00931) | [pdf](https://arxiv.org/pdf/2105.00931)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.MA](https://arxiv.org/list/cs.MA/recent)

While deep reinforcement learning (RL) promises freedom from hand-labeled data, great successes, especially for Embodied AI, require significant work to create supervision via carefully shaped rewards. Indeed, without shaped rewards, i.e., with only terminal rewards, present-day Embodied AI results degrade significantly across Embodied AI problems from single-agent Habitat-based PointGoal Navigation (SPL drops from 55 to 0) and two-agent AI2-THOR-based Furniture Moving (success drops from 58% to 1%) to three-agent Google Football-based 3 vs. 1 with Keeper (game score drops from 0.6 to 0.1). As training from shaped rewards doesn't scale to more realistic tasks, the community needs to improve the success of training with terminal rewards. For this we propose GridToPix: 1) train agents with terminal rewards in gridworlds that generically mirror Embodied AI environments, i.e., they are independent of the task; 2) distill the learned policy into agents that reside in complex visual worlds. Despite learning from only terminal rewards with identical models and RL algorithms, GridToPix significantly improves results across tasks: from PointGoal Navigation (SPL improves from 0 to 64) and Furniture Moving (success improves from 1% to 25%) to football gameplay (game score improves from 0.1 to 0.6). GridToPix even helps to improve the results of shaped reward training.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">GridToPix: Training Embodied Agents with Minimal Supervision<br>pdf: <a href="https://t.co/wMgNpsLQRW">https://t.co/wMgNpsLQRW</a><br>abs: <a href="https://t.co/DgYWQ1PbSI">https://t.co/DgYWQ1PbSI</a><br>project page: <a href="https://t.co/N6FbcQci6Z">https://t.co/N6FbcQci6Z</a> <a href="https://t.co/N97DyMpFxZ">pic.twitter.com/N97DyMpFxZ</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1389428301119229952?ref_src=twsrc%5Etfw">May 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 13. Learning to drive from a world on rails

Dian Chen, Vladlen Koltun, Philipp Kr√§henb√ºhl

- retweets: 112, favorites: 40 (05/05/2021 15:23:28)

- links: [abs](https://arxiv.org/abs/2105.00636) | [pdf](https://arxiv.org/pdf/2105.00636)
- [cs.RO](https://arxiv.org/list/cs.RO/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

We learn an interactive vision-based driving policy from pre-recorded driving logs via a model-based approach. A forward model of the world supervises a driving policy that predicts the outcome of any potential driving trajectory. To support learning from pre-recorded logs, we assume that the world is on rails, meaning neither the agent nor its actions influence the environment. This assumption greatly simplifies the learning problem, factorizing the dynamics into a nonreactive world model and a low-dimensional and compact forward model of the ego-vehicle. Our approach computes action-values for each training trajectory using a tabular dynamic-programming evaluation of the Bellman equations; these action-values in turn supervise the final vision-based driving policy. Despite the world-on-rails assumption, the final driving policy acts well in a dynamic and reactive world. Our method ranks first on the CARLA leaderboard, attaining a 25% higher driving score while using 40 times less data. Our method is also an order of magnitude more sample-efficient than state-of-the-art model-free reinforcement learning techniques on navigational tasks in the ProcGen benchmark.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Learning to drive from a world on rails<br>pdf: <a href="https://t.co/n8TH2TmO41">https://t.co/n8TH2TmO41</a><br>abs: <a href="https://t.co/dE5GDRURN7">https://t.co/dE5GDRURN7</a><br>project page: <a href="https://t.co/ZIkUuIbdt7">https://t.co/ZIkUuIbdt7</a><br>github: <a href="https://t.co/I1fiJ2Z8Nz">https://t.co/I1fiJ2Z8Nz</a><br><br>an agent trained in a world-on-rails learns to drive better than state-of-the-art imitation learning agents <a href="https://t.co/9KiteacReD">pic.twitter.com/9KiteacReD</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1389424844085858305?ref_src=twsrc%5Etfw">May 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 14. Curious Representation Learning for Embodied Intelligence

Yilun Du, Chuang Gan, Phillip Isola

- retweets: 110, favorites: 38 (05/05/2021 15:23:28)

- links: [abs](https://arxiv.org/abs/2105.01060) | [pdf](https://arxiv.org/pdf/2105.01060)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.RO](https://arxiv.org/list/cs.RO/recent)

Self-supervised representation learning has achieved remarkable success in recent years. By subverting the need for supervised labels, such approaches are able to utilize the numerous unlabeled images that exist on the Internet and in photographic datasets. Yet to build truly intelligent agents, we must construct representation learning algorithms that can learn not only from datasets but also learn from environments. An agent in a natural environment will not typically be fed curated data. Instead, it must explore its environment to acquire the data it will learn from. We propose a framework, curious representation learning (CRL), which jointly learns a reinforcement learning policy and a visual representation model. The policy is trained to maximize the error of the representation learner, and in doing so is incentivized to explore its environment. At the same time, the learned representation becomes stronger and stronger as the policy feeds it ever harder data to learn from. Our learned representations enable promising transfer to downstream navigation tasks, performing better than or comparably to ImageNet pretraining without using any supervision at all. In addition, despite being trained in simulation, our learned representations can obtain interpretable results on real images.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Curious Representation Learning for Embodied Intelligence<br>pdf: <a href="https://t.co/4VpNnAjacB">https://t.co/4VpNnAjacB</a><br>abs: <a href="https://t.co/BUx9tuHwTY">https://t.co/BUx9tuHwTY</a><br><br>a generic framework to learn task-agnostic visual representations in embodied environments <a href="https://t.co/K4J5YwZ4z1">pic.twitter.com/K4J5YwZ4z1</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1389395284581568514?ref_src=twsrc%5Etfw">May 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 15. Generative Art Using Neural Visual Grammars and Dual Encoders

Chrisantha Fernando, S. M. Ali Eslami, Jean-Baptiste Alayrac, Piotr Mirowski, Dylan Banarse, Simon Osindero

- retweets: 90, favorites: 45 (05/05/2021 15:23:28)

- links: [abs](https://arxiv.org/abs/2105.00162) | [pdf](https://arxiv.org/pdf/2105.00162)
- [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.NE](https://arxiv.org/list/cs.NE/recent)

Whilst there are perhaps only a few scientific methods, there seem to be almost as many artistic methods as there are artists. Artistic processes appear to inhabit the highest order of open-endedness. To begin to understand some of the processes of art making it is helpful to try to automate them even partially. In this paper, a novel algorithm for producing generative art is described which allows a user to input a text string, and which in a creative response to this string, outputs an image which interprets that string. It does so by evolving images using a hierarchical neural Lindenmeyer system, and evaluating these images along the way using an image text dual encoder trained on billions of images and their associated text from the internet. In doing so we have access to and control over an instance of an artistic process, allowing analysis of which aspects of the artistic process become the task of the algorithm, and which elements remain the responsibility of the artist.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Happy to share our new work producing <a href="https://twitter.com/hashtag/GenerativeArt?src=hash&amp;ref_src=twsrc%5Etfw">#GenerativeArt</a> using neuroevolution along with a pretrained multimodal critic.<a href="https://t.co/jQRWDwC87R">https://t.co/jQRWDwC87R</a><br><br>Delightful results from this collaboration led by <a href="https://twitter.com/chrisantha_f?ref_src=twsrc%5Etfw">@chrisantha_f</a> with DM colleagues  <a href="https://twitter.com/arkitus?ref_src=twsrc%5Etfw">@arkitus</a>, JB Alayrac, <a href="https://twitter.com/MirowskiPiotr?ref_src=twsrc%5Etfw">@MirowskiPiotr</a> &amp; Dylan Banarse <a href="https://t.co/uL3veXL6sO">pic.twitter.com/uL3veXL6sO</a></p>&mdash; Dr Simon Osindero (@sindero) <a href="https://twitter.com/sindero/status/1389711805572919296?ref_src=twsrc%5Etfw">May 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 16. Teaching NLP outside Linguistics and Computer Science classrooms: Some  challenges and some opportunities

Sowmya Vajjala

- retweets: 74, favorites: 17 (05/05/2021 15:23:28)

- links: [abs](https://arxiv.org/abs/2105.00895) | [pdf](https://arxiv.org/pdf/2105.00895)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

NLP's sphere of influence went much beyond computer science research and the development of software applications in the past decade. We see people using NLP methods in a range of academic disciplines from Asian Studies to Clinical Oncology. We also notice the presence of NLP as a module in most of the data science curricula within and outside of regular university setups. These courses are taken by students from very diverse backgrounds. This paper takes a closer look at some issues related to teaching NLP to these diverse audiences based on my classroom experiences, and identifies some challenges the instructors face, particularly when there is no ecosystem of related courses for the students. In this process, it also identifies a few challenge areas for both NLP researchers and tool developers.




# 17. Applied Language Technology: NLP for the Humanities

Tuomo Hiippala

- retweets: 72, favorites: 17 (05/05/2021 15:23:28)

- links: [abs](https://arxiv.org/abs/2105.01052) | [pdf](https://arxiv.org/pdf/2105.01052)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

This contribution describes a two-course module that seeks to provide humanities majors with a basic understanding of language technology and its applications using Python. The learning materials consist of interactive Jupyter Notebooks and accompanying YouTube videos, which are openly available with a Creative Commons licence.




# 18. SUPERB: Speech processing Universal PERformance Benchmark

Shu-wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y. Lin, Andy T. Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, Tzu-Hsien Huang, Wei-Cheng Tseng, Ko-tik Lee, Da-Rong Liu, Zili Huang, Shuyan Dong, Shang-Wen Li, Shinji Watanabe, Abdelrahman Mohamed, Hung-yi Lee

- retweets: 36, favorites: 28 (05/05/2021 15:23:28)

- links: [abs](https://arxiv.org/abs/2105.01051) | [pdf](https://arxiv.org/pdf/2105.01051)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.SD](https://arxiv.org/list/cs.SD/recent) | [eess.AS](https://arxiv.org/list/eess.AS/recent)

Self-supervised learning (SSL) has proven vital for advancing research in natural language processing (NLP) and computer vision (CV). The paradigm pretrains a shared model on large volumes of unlabeled data and achieves state-of-the-art (SOTA) for various tasks with minimal adaptation. However, the speech processing community lacks a similar setup to systematically explore the paradigm. To bridge this gap, we introduce Speech processing Universal PERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the performance of a shared model across a wide range of speech processing tasks with minimal architecture changes and labeled data. Among multiple usages of the shared model, we especially focus on extracting the representation learned from SSL due to its preferable re-usability. We present a simple framework to solve SUPERB tasks by learning task-specialized lightweight prediction heads on top of the frozen shared model. Our results demonstrate that the framework is promising as SSL representations show competitive generalizability and accessibility across SUPERB tasks. We release SUPERB as a challenge with a leaderboard and a benchmark toolkit to fuel the research in representation learning and general speech processing.




# 19. Universal Weakly Supervised Segmentation by Pixel-to-Segment Contrastive  Learning

Tsung-Wei Ke, Jyh-Jing Hwang, Stella X. Yu

- retweets: 25, favorites: 31 (05/05/2021 15:23:28)

- links: [abs](https://arxiv.org/abs/2105.00957) | [pdf](https://arxiv.org/pdf/2105.00957)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Weakly supervised segmentation requires assigning a label to every pixel based on training instances with partial annotations such as image-level tags, object bounding boxes, labeled points and scribbles. This task is challenging, as coarse annotations (tags, boxes) lack precise pixel localization whereas sparse annotations (points, scribbles) lack broad region coverage. Existing methods tackle these two types of weak supervision differently: Class activation maps are used to localize coarse labels and iteratively refine the segmentation model, whereas conditional random fields are used to propagate sparse labels to the entire image.   We formulate weakly supervised segmentation as a semi-supervised metric learning problem, where pixels of the same (different) semantics need to be mapped to the same (distinctive) features. We propose 4 types of contrastive relationships between pixels and segments in the feature space, capturing low-level image similarity, semantic annotation, co-occurrence, and feature affinity They act as priors; the pixel-wise feature can be learned from training images with any partial annotations in a data-driven fashion. In particular, unlabeled pixels in training images participate not only in data-driven grouping within each image, but also in discriminative feature learning within and across images. We deliver a universal weakly supervised segmenter with significant gains on Pascal VOC and DensePose.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Universal Weakly Supervised Segmentation by Pixel-to-Segment Contrastive Learning<br>pdf: <a href="https://t.co/Y9ZSKfeZfg">https://t.co/Y9ZSKfeZfg</a><br>abs: <a href="https://t.co/T7AfXgakGx">https://t.co/T7AfXgakGx</a> <a href="https://t.co/Kzq1Xowhvv">pic.twitter.com/Kzq1Xowhvv</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1389397236753252360?ref_src=twsrc%5Etfw">May 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 20. Semantic Journeys: Quantifying Change in Emoji Meaning from 2012-2018

Alexander Robertson, Farhana Ferdousi Liza, Dong Nguyen, Barbara McGillivray, Scott A. Hale

- retweets: 42, favorites: 10 (05/05/2021 15:23:28)

- links: [abs](https://arxiv.org/abs/2105.00846) | [pdf](https://arxiv.org/pdf/2105.00846)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

The semantics of emoji has, to date, been considered from a static perspective. We offer the first longitudinal study of how emoji semantics changes over time, applying techniques from computational linguistics to six years of Twitter data. We identify five patterns in emoji semantic development and find evidence that the less abstract an emoji is, the more likely it is to undergo semantic change. In addition, we analyse select emoji in more detail, examining the effect of seasonality and world events on emoji semantics. To aid future work on emoji and semantics, we make our data publicly available along with a web-based interface that anyone can use to explore semantic change in emoji.



