---
title: Hot Papers 2020-08-28
date: 2020-08-31T09:40:30.Z
template: "post"
draft: false
slug: "hot-papers-2020-08-28"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-08-28"
socialImage: "/media/flying-marine.jpg"

---

# 1. Open is not forever: a study of vanished open access journals

Mikael Laakso, Lisa Matthias, Najko Jahn

- retweets: 147, favorites: 263 (08/31/2020 09:40:30)

- links: [abs](https://arxiv.org/abs/2008.11933) | [pdf](https://arxiv.org/pdf/2008.11933)
- [cs.DL](https://arxiv.org/list/cs.DL/recent)

The preservation of the scholarly record has been a point of concern since the beginning of knowledge production. With print publications, the responsibility rested primarily with librarians, but the shift towards digital publishing and, in particular, the introduction of open access (OA) have caused ambiguity and complexity. Consequently, the long-term accessibility of journals is not always guaranteed, and they can even disappear from the web completely. The purpose of this exploratory study is to systematically study the phenomenon of vanished journals, something that has not been done before. For the analysis, we consulted several major bibliographic indexes, such as Scopus, Ulrichsweb, and the Directory of Open Access Journals, and traced the journals through the Internet Archive's Wayback Machine. We found 192 OA journals that vanished from the web between 2000 and 2019, spanning all major research disciplines and geographic regions of the world. Our results raise vital concern for the integrity of the scholarly record and highlight the urgency to take collaborative action to ensure continued access and prevent the loss of more scholarly knowledge. We encourage those interested in the phenomenon of vanished journals to use the public dataset for their own research.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">So, <a href="https://twitter.com/mikaellaakso?ref_src=twsrc%5Etfw">@mikaellaakso</a>, <a href="https://twitter.com/najkoja?ref_src=twsrc%5Etfw">@najkoja</a>, and I have a new preprint out, where we look at vanished journals. We found 192 journals that were once openly available but have since vanished, and that this affects all disciplines and geographical regions. However.... <a href="https://t.co/nsHoOLnvST">https://t.co/nsHoOLnvST</a> <a href="https://t.co/DhXKFL2xnt">pic.twitter.com/DhXKFL2xnt</a></p>&mdash; Lisa Matthias (@l_matthia) <a href="https://twitter.com/l_matthia/status/1299396268326871040?ref_src=twsrc%5Etfw">August 28, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">&quot;We found 192 <a href="https://twitter.com/hashtag/openaccess?src=hash&amp;ref_src=twsrc%5Etfw">#openaccess</a> journals that vanished from the web between 2000 and 2019, spanning all major research disciplines and geographic regions of the world.&quot;<a href="https://t.co/p0eiZW32Vz">https://t.co/p0eiZW32Vz</a><br><br>Comment: OA needs preservation, just as preservation needs OA.</p>&mdash; Peter Suber (@petersuber) <a href="https://twitter.com/petersuber/status/1299403104878620672?ref_src=twsrc%5Etfw">August 28, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">‚ÄúWe found 192 OA journals that vanished from the web between 2000 and 2019, spanning all major research disciplines and geographic regions of the world.‚Äù<a href="https://t.co/B4tuRH5Gke">https://t.co/B4tuRH5Gke</a> <a href="https://t.co/euR2ZIUa7P">pic.twitter.com/euR2ZIUa7P</a></p>&mdash; Retraction Watch (@RetractionWatch) <a href="https://twitter.com/RetractionWatch/status/1300055767635066880?ref_src=twsrc%5Etfw">August 30, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Be sure to look at this important new scholarship abt hundreds (if not thousands) of <a href="https://twitter.com/hashtag/OpenAccess?src=hash&amp;ref_src=twsrc%5Etfw">#OpenAccess</a> Journals disappearing. <a href="https://twitter.com/internetarchive?ref_src=twsrc%5Etfw">@internetarchive</a> has been ramping up our efforts to help fill these gaps. More on this soon.<a href="https://t.co/y9Rjmiw3wX">https://t.co/y9Rjmiw3wX</a> <a href="https://t.co/v5RsIFprJA">https://t.co/v5RsIFprJA</a></p>&mdash; Internet Archive (@internetarchive) <a href="https://twitter.com/internetarchive/status/1300179633707470848?ref_src=twsrc%5Etfw">August 30, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. CenterHMR: a Bottom-up Single-shot Method for Multi-person 3D Mesh  Recovery from a Single Image

Yu Sun, Qian Bao, Wu Liu, Yili Fu, Tao Mei

- retweets: 33, favorites: 138 (08/31/2020 09:40:31)

- links: [abs](https://arxiv.org/abs/2008.12272) | [pdf](https://arxiv.org/pdf/2008.12272)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

In this paper, we propose a method to recover multi-person 3D mesh from a single image. Existing methods follow a multi-stage detection-based pipeline, where the 3D mesh of each person is regressed from the cropped image patch. They have to suffer from the high complexity of the multi-stage process and the ambiguity of the image-level features. For example, it is hard for them to estimate multi-person 3D mesh from the inseparable crowded cases. Instead, in this paper, we present a novel bottom-up single-shot method, Center-based Human Mesh Recovery network (CenterHMR). The model is trained to simultaneously predict two maps, which represent the location of each human body center and the corresponding parameter vector of 3D human mesh at each center. This explicit center-based representation guarantees the pixel-level feature encoding. Besides, the 3D mesh result of each person is estimated from the features centered at the visible body parts, which improves the robustness under occlusion. CenterHMR surpasses previous methods on multi-person in-the-wild benchmark 3DPW and occlusion dataset 3DOH50K. Besides, CenterHMR has achieved a 2-nd place on ECCV 2020 3DPW Challenge. The code is released on https://github.com/Arthur151/CenterHMR.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">CenterHMR: a Bottom-up Single-shot Method for Multi-person 3D Mesh Recovery from a Single Image<br>pdf: <a href="https://t.co/0MGvbOXhAb">https://t.co/0MGvbOXhAb</a><br>abs: <a href="https://t.co/OmG6Ad0smy">https://t.co/OmG6Ad0smy</a><br>github: <a href="https://t.co/1mW2mh9Y5V">https://t.co/1mW2mh9Y5V</a> <a href="https://t.co/xqHyL3xQke">pic.twitter.com/xqHyL3xQke</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1299149096243154945?ref_src=twsrc%5Etfw">August 28, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization

Xinsong Zhang, Hang Li

- retweets: 29, favorites: 139 (08/31/2020 09:40:31)

- links: [abs](https://arxiv.org/abs/2008.11869) | [pdf](https://arxiv.org/pdf/2008.11869)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Pre-trained language models such as BERT have exhibited remarkable performances in many tasks in natural language understanding (NLU). The tokens in the models are usually fine-grained in the sense that for languages like English they are words or sub-words and for languages like Chinese they are characters. In English, for example, there are multi-word expressions which form natural lexical units and thus the use of coarse-grained tokenization also appears to be reasonable. In fact, both fine-grained and coarse-grained tokenizations have advantages and disadvantages for learning of pre-trained language models. In this paper, we propose a novel pre-trained language model, referred to as AMBERT (A Multi-grained BERT), on the basis of both fine-grained and coarse-grained tokenizations. For English, AMBERT takes both the sequence of words (fine-grained tokens) and the sequence of phrases (coarse-grained tokens) as input after tokenization, employs one encoder for processing the sequence of words and the other encoder for processing the sequence of the phrases, utilizes shared parameters between the two encoders, and finally creates a sequence of contextualized representations of the words and a sequence of contextualized representations of the phrases. Experiments have been conducted on benchmark datasets for Chinese and English, including CLUE, GLUE, SQuAD and RACE. The results show that AMBERT outperforms the existing best performing models in almost all cases, particularly the improvements are significant for Chinese.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Our new archive paper &quot;AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization&quot;.  It works better than BERT,  Albert, XLNet, etc at CLUE and GLUE.  <a href="https://t.co/2jdiy0U1fy">https://t.co/2jdiy0U1fy</a></p>&mdash; Hang Li (@dr_hang_li) <a href="https://twitter.com/dr_hang_li/status/1299171177449963520?ref_src=twsrc%5Etfw">August 28, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. GPU-accelerating ImageJ Macro image processing workflows using CLIJ

Daniela Vorkel, Robert Haase

- retweets: 21, favorites: 85 (08/31/2020 09:40:31)

- links: [abs](https://arxiv.org/abs/2008.11799) | [pdf](https://arxiv.org/pdf/2008.11799)
- [cs.MS](https://arxiv.org/list/cs.MS/recent) | [cs.DC](https://arxiv.org/list/cs.DC/recent) | [q-bio.QM](https://arxiv.org/list/q-bio.QM/recent)

This chapter introduces GPU-accelerated image processing in ImageJ/FIJI. The reader is expected to have some pre-existing knowledge of ImageJ Macro programming. Core concepts such as variables, for-loops, and functions are essential. The chapter provides basic guidelines for improved performance in typical image processing workflows. We present in a step-by-step tutorial how to translate a pre-existing ImageJ macro into a GPU-accelerated macro.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Fast, faster, GPU - the preprint for <a href="https://twitter.com/hashtag/GPU?src=hash&amp;ref_src=twsrc%5Etfw">#GPU</a>-accelerated macro processing workflows in <a href="https://twitter.com/FijiSc?ref_src=twsrc%5Etfw">@FijiSc</a> is online: <a href="https://t.co/1qXal4GmvC">https://t.co/1qXal4GmvC</a>.üôÉ Big THANKS to my colleague ‚≠êÔ∏è<a href="https://twitter.com/haesleinhuepf?ref_src=twsrc%5Etfw">@haesleinhuepf</a>‚≠êÔ∏è!!! üòé, the mastemind behind the code of <a href="https://twitter.com/hashtag/clij?src=hash&amp;ref_src=twsrc%5Etfw">#clij</a>! ü™≤+üî¨+ üñ•Ô∏è=üòÉ <a href="https://twitter.com/hashtag/Neubias?src=hash&amp;ref_src=twsrc%5Etfw">#Neubias</a> <a href="https://twitter.com/hashtag/bioimageanalysis?src=hash&amp;ref_src=twsrc%5Etfw">#bioimageanalysis</a></p>&mdash; Daniela (@happifocus) <a href="https://twitter.com/happifocus/status/1299268960760852480?ref_src=twsrc%5Etfw">August 28, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">New preprint book chapter on how to use <a href="https://twitter.com/hashtag/CLIJ?src=hash&amp;ref_src=twsrc%5Etfw">#CLIJ</a> GPU-accelerated image processing. It has made my image processing ~60 times faster. Images that took an hour for me to process now can get done in just under a minute with these easy-to-use commands. <a href="https://t.co/ZHoPnbO45r">https://t.co/ZHoPnbO45r</a></p>&mdash; Tanner Fadero (@TanFad) <a href="https://twitter.com/TanFad/status/1299371835495198725?ref_src=twsrc%5Etfw">August 28, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Visual Concept Reasoning Networks

Taesup Kim, Sungwoong Kim, Yoshua Bengio

- retweets: 35, favorites: 36 (08/31/2020 09:40:32)

- links: [abs](https://arxiv.org/abs/2008.11783) | [pdf](https://arxiv.org/pdf/2008.11783)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

A split-transform-merge strategy has been broadly used as an architectural constraint in convolutional neural networks for visual recognition tasks. It approximates sparsely connected networks by explicitly defining multiple branches to simultaneously learn representations with different visual concepts or properties. Dependencies or interactions between these representations are typically defined by dense and local operations, however, without any adaptiveness or high-level reasoning. In this work, we propose to exploit this strategy and combine it with our Visual Concept Reasoning Networks (VCRNet) to enable reasoning between high-level visual concepts. We associate each branch with a visual concept and derive a compact concept state by selecting a few local descriptors through an attention module. These concept states are then updated by graph-based interaction and used to adaptively modulate the local descriptors. We describe our proposed model by split-transform-attend-interact-modulate-merge stages, which are implemented by opting for a highly modularized architecture. Extensive experiments on visual recognition tasks such as image classification, semantic segmentation, object detection, scene recognition, and action recognition show that our proposed model, VCRNet, consistently improves the performance by increasing the number of parameters by less than 1%.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Visual Concept Reasoning Networks. <a href="https://twitter.com/hashtag/ArtificialIntelligence?src=hash&amp;ref_src=twsrc%5Etfw">#ArtificialIntelligence</a> <a href="https://twitter.com/hashtag/DataScience?src=hash&amp;ref_src=twsrc%5Etfw">#DataScience</a> <a href="https://twitter.com/hashtag/BigData?src=hash&amp;ref_src=twsrc%5Etfw">#BigData</a> <a href="https://twitter.com/hashtag/Analytics?src=hash&amp;ref_src=twsrc%5Etfw">#Analytics</a> <a href="https://twitter.com/hashtag/Python?src=hash&amp;ref_src=twsrc%5Etfw">#Python</a> <a href="https://twitter.com/hashtag/RStats?src=hash&amp;ref_src=twsrc%5Etfw">#RStats</a> <a href="https://twitter.com/hashtag/TensorFlow?src=hash&amp;ref_src=twsrc%5Etfw">#TensorFlow</a> <a href="https://twitter.com/hashtag/IoT?src=hash&amp;ref_src=twsrc%5Etfw">#IoT</a> <a href="https://twitter.com/hashtag/Java?src=hash&amp;ref_src=twsrc%5Etfw">#Java</a> <a href="https://twitter.com/hashtag/JavaScript?src=hash&amp;ref_src=twsrc%5Etfw">#JavaScript</a> <a href="https://twitter.com/hashtag/ReactJS?src=hash&amp;ref_src=twsrc%5Etfw">#ReactJS</a> <a href="https://twitter.com/hashtag/GoLang?src=hash&amp;ref_src=twsrc%5Etfw">#GoLang</a> <a href="https://twitter.com/hashtag/Serverless?src=hash&amp;ref_src=twsrc%5Etfw">#Serverless</a> <a href="https://twitter.com/hashtag/Linux?src=hash&amp;ref_src=twsrc%5Etfw">#Linux</a> <a href="https://twitter.com/hashtag/Programmer?src=hash&amp;ref_src=twsrc%5Etfw">#Programmer</a> <a href="https://twitter.com/hashtag/DataViz?src=hash&amp;ref_src=twsrc%5Etfw">#DataViz</a> <a href="https://twitter.com/hashtag/DataScientists?src=hash&amp;ref_src=twsrc%5Etfw">#DataScientists</a> <a href="https://twitter.com/hashtag/DeepLearning?src=hash&amp;ref_src=twsrc%5Etfw">#DeepLearning</a> <a href="https://twitter.com/hashtag/MachineLearning?src=hash&amp;ref_src=twsrc%5Etfw">#MachineLearning</a> <a href="https://twitter.com/hashtag/AI?src=hash&amp;ref_src=twsrc%5Etfw">#AI</a> <a href="https://t.co/DNOm79X8Li">https://t.co/DNOm79X8Li</a> <a href="https://t.co/Vz13iiVYxV">pic.twitter.com/Vz13iiVYxV</a></p>&mdash; Marcus Borba (@marcusborba) <a href="https://twitter.com/marcusborba/status/1299491410681487363?ref_src=twsrc%5Etfw">August 28, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. Traces of Class/Cross-Class Structure Pervade Deep Learning Spectra

Vardan Papyan

- retweets: 42, favorites: 17 (08/31/2020 09:40:32)

- links: [abs](https://arxiv.org/abs/2008.11865) | [pdf](https://arxiv.org/pdf/2008.11865)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Numerous researchers recently applied empirical spectral analysis to the study of modern deep learning classifiers. We identify and discuss an important formal class/cross-class structure and show how it lies at the origin of the many visually striking features observed in deepnet spectra, some of which were reported in recent articles, others are unveiled here for the first time. These include spectral outliers, "spikes", and small but distinct continuous distributions, "bumps", often seen beyond the edge of a "main bulk".   The significance of the cross-class structure is illustrated in three ways: (i) we prove the ratio of outliers to bulk in the spectrum of the Fisher information matrix is predictive of misclassification, in the context of multinomial logistic regression; (ii) we demonstrate how, gradually with depth, a network is able to separate class-distinctive information from class variability, all while orthogonalizing the class-distinctive information; and (iii) we propose a correction to KFAC, a well-known second-order optimization algorithm for training deepnets.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Traces of Class/Cross-Class Structure Pervade Deep Learning Spectra. <a href="https://twitter.com/hashtag/ArtificialIntelligence?src=hash&amp;ref_src=twsrc%5Etfw">#ArtificialIntelligence</a> <a href="https://twitter.com/hashtag/DataScience?src=hash&amp;ref_src=twsrc%5Etfw">#DataScience</a> <a href="https://twitter.com/hashtag/BigData?src=hash&amp;ref_src=twsrc%5Etfw">#BigData</a> <a href="https://twitter.com/hashtag/Analytics?src=hash&amp;ref_src=twsrc%5Etfw">#Analytics</a> <a href="https://twitter.com/hashtag/Python?src=hash&amp;ref_src=twsrc%5Etfw">#Python</a> <a href="https://twitter.com/hashtag/RStats?src=hash&amp;ref_src=twsrc%5Etfw">#RStats</a> <a href="https://twitter.com/hashtag/TensorFlow?src=hash&amp;ref_src=twsrc%5Etfw">#TensorFlow</a> <a href="https://twitter.com/hashtag/Java?src=hash&amp;ref_src=twsrc%5Etfw">#Java</a> <a href="https://twitter.com/hashtag/JavaScript?src=hash&amp;ref_src=twsrc%5Etfw">#JavaScript</a> <a href="https://twitter.com/hashtag/ReactJS?src=hash&amp;ref_src=twsrc%5Etfw">#ReactJS</a> <a href="https://twitter.com/hashtag/GoLang?src=hash&amp;ref_src=twsrc%5Etfw">#GoLang</a> <a href="https://twitter.com/hashtag/Serverless?src=hash&amp;ref_src=twsrc%5Etfw">#Serverless</a> <a href="https://twitter.com/hashtag/Linux?src=hash&amp;ref_src=twsrc%5Etfw">#Linux</a> <a href="https://twitter.com/hashtag/Programmer?src=hash&amp;ref_src=twsrc%5Etfw">#Programmer</a> <a href="https://twitter.com/hashtag/MachineLearning?src=hash&amp;ref_src=twsrc%5Etfw">#MachineLearning</a> <a href="https://twitter.com/hashtag/DeepLearning?src=hash&amp;ref_src=twsrc%5Etfw">#DeepLearning</a> <a href="https://twitter.com/hashtag/AI?src=hash&amp;ref_src=twsrc%5Etfw">#AI</a><a href="https://t.co/BAUtP7MoPc">https://t.co/BAUtP7MoPc</a> <a href="https://t.co/DpoS0eBgcq">pic.twitter.com/DpoS0eBgcq</a></p>&mdash; Marcus Borba (@marcusborba) <a href="https://twitter.com/marcusborba/status/1299848834613473281?ref_src=twsrc%5Etfw">August 29, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. Multi-scale approach for the prediction of atomic scale properties

Andrea Grisafi, Jigyasa Nigam, Michele Ceriotti

- retweets: 14, favorites: 43 (08/31/2020 09:40:32)

- links: [abs](https://arxiv.org/abs/2008.12122) | [pdf](https://arxiv.org/pdf/2008.12122)
- [physics.comp-ph](https://arxiv.org/list/physics.comp-ph/recent) | [cond-mat.mtrl-sci](https://arxiv.org/list/cond-mat.mtrl-sci/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Electronic nearsightedness is one of the fundamental principles governing the behavior of condensed matter and supporting its description in terms of local entities such as chemical bonds. Locality also underlies the tremendous success of machine-learning schemes that predict quantum mechanical observables -- such as the cohesive energy, the electron density, or a variety of response properties -- as a sum of atom-centred contributions, based on a short-range representation of atomic environments. One of the main shortcomings of these approaches is their inability to capture physical effects, ranging from electrostatic interactions to quantum delocalization, which have a long-range nature. Here we show how to build a multi-scale scheme that combines in the same framework local and non-local information, overcoming such limitations. We show that the simplest version of such features can be put in formal correspondence with a multipole expansion of permanent electrostatics. The data-driven nature of the model construction, however, makes this simple form suitable to tackle also different types of delocalized and collective effects. We present several examples that range from molecular physics, to surface science and biophysics, demonstrating the ability of this multi-scale approach to model interactions driven by electrostatics, polarization and dispersion, as well as the cooperative behavior of dielectric response properties.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Physical interactions are multi-scale, so should be your <a href="https://twitter.com/hashtag/machinelearning?src=hash&amp;ref_src=twsrc%5Etfw">#machinelearning</a> model. Hot off the <a href="https://twitter.com/hashtag/preprint?src=hash&amp;ref_src=twsrc%5Etfw">#preprint</a> press, exquisite work by Andrea Grisafi and <a href="https://twitter.com/nccr_marvel?ref_src=twsrc%5Etfw">@nccr_marvel</a> <a href="https://twitter.com/hashtag/inspirepotentials?src=hash&amp;ref_src=twsrc%5Etfw">#inspirepotentials</a> fellow Jigyasa Nigam will get permanent &amp; polarizable electrostatics, &amp; more! <a href="https://t.co/BavPlI0Tjz">https://t.co/BavPlI0Tjz</a> <a href="https://t.co/qiPMj845fA">pic.twitter.com/qiPMj845fA</a></p>&mdash; cosmo-epfl (@COSMO_EPFL) <a href="https://twitter.com/COSMO_EPFL/status/1299245732424282117?ref_src=twsrc%5Etfw">August 28, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



