---
title: Hot Papers 2020-07-09
date: 2020-07-10T07:46:45.Z
template: "post"
draft: false
slug: "hot-papers-2020-07-09"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-07-09"
socialImage: "/media/42-line-bible.jpg"

---

# 1. NVAE: A Deep Hierarchical Variational Autoencoder

Arash Vahdat, Jan Kautz

- retweets: 261, favorites: 1160 (07/10/2020 07:46:45)

- links: [abs](https://arxiv.org/abs/2007.03898) | [pdf](https://arxiv.org/pdf/2007.03898)
- [stat.ML](https://arxiv.org/list/stat.ML/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ as shown in Fig. 1. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256$\times$256 pixels.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">üì¢üì¢üì¢ Introducing NVAE üì¢üì¢üì¢<br><br>We show that deep hierarchical VAEs w/ carefully designed network architecture, generate high-quality images &amp; achieve SOTA likelihood, even when trained w/ original VAE loss.<br><br>paper: <a href="https://t.co/L4GuiIKci8">https://t.co/L4GuiIKci8</a><br>with <a href="https://twitter.com/jankautz?ref_src=twsrc%5Etfw">@jankautz</a> at <a href="https://twitter.com/NVIDIAAI?ref_src=twsrc%5Etfw">@NVIDIAAI</a> <br><br>(1/n) <a href="https://t.co/g6GQT7jkdC">pic.twitter.com/g6GQT7jkdC</a></p>&mdash; Arash Vahdat (@ArashVahdat) <a href="https://twitter.com/ArashVahdat/status/1281036985981825024?ref_src=twsrc%5Etfw">July 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="pt" dir="ltr">NVAE: A Deep Hierarchical Variational Autoencoder<br>pdf: <a href="https://t.co/7rCHQUb57O">https://t.co/7rCHQUb57O</a><br>abs: <a href="https://t.co/Er1voAVvc2">https://t.co/Er1voAVvc2</a> <a href="https://t.co/RHNTHffjQh">pic.twitter.com/RHNTHffjQh</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1281031340994035712?ref_src=twsrc%5Etfw">July 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Decolonial AI: Decolonial Theory as Sociotechnical Foresight in  Artificial Intelligence

Shakir Mohamed, Marie-Therese Png, William Isaac

- retweets: 85, favorites: 294 (07/10/2020 07:46:46)

- links: [abs](https://arxiv.org/abs/2007.04068) | [pdf](https://arxiv.org/pdf/2007.04068)
- [cs.CY](https://arxiv.org/list/cs.CY/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

This paper explores the important role of critical science, and in particular of post-colonial and decolonial theories, in understanding and shaping the ongoing advances in artificial intelligence. Artificial Intelligence (AI) is viewed as amongst the technological advances that will reshape modern societies and their relations. Whilst the design and deployment of systems that continually adapt holds the promise of far-reaching positive change, they simultaneously pose significant risks, especially to already vulnerable peoples. Values and power are central to this discussion. Decolonial theories use historical hindsight to explain patterns of power that shape our intellectual, political, economic, and social world. By embedding a decolonial critical approach within its technical practice, AI communities can develop foresight and tactics that can better align research and technology development with established ethical principles, centring vulnerable peoples who continue to bear the brunt of negative impacts of innovation and scientific progress. We highlight problematic applications that are instances of coloniality, and using a decolonial lens, submit three tactics that can form a decolonial field of artificial intelligence: creating a critical technical practice of AI, seeking reverse tutelage and reverse pedagogies, and the renewal of affective and political communities. The years ahead will usher in a wave of new scientific breakthroughs and technologies driven by AI research, making it incumbent upon AI communities to strengthen the social contract through ethical foresight and the multiplicity of intellectual perspectives available to us; ultimately supporting future technologies that enable greater well-being, with the goal of beneficence and justice for all.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Excited to share a new paper on Decolonisation and AI. With the amazing <a href="https://twitter.com/png_marie?ref_src=twsrc%5Etfw">@png_marie</a>  <a href="https://twitter.com/wsisaac?ref_src=twsrc%5Etfw">@wsisaac</a> ü§© we explore why Decolonial theory matters for our field, and tactics to decolonise and reshape our field into a Decolonial AI. Feedback please üôèüèæ Thread üëáüèæ<br> <a href="https://t.co/oR7XD0ocuk">https://t.co/oR7XD0ocuk</a> <a href="https://t.co/xv9lYtJwLW">pic.twitter.com/xv9lYtJwLW</a></p>&mdash; Shakir Mohamed (@shakir_za) <a href="https://twitter.com/shakir_za/status/1281275899204136960?ref_src=twsrc%5Etfw">July 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I am extremely here for this academic paper, which applies post-colonial and decolonial critical theories to AI. I&#39;m basically just reading it while occasionally yelling &quot;Preach! Power dynamics matter!&quot; <a href="https://t.co/BsZo0eOU50">https://t.co/BsZo0eOU50</a></p>&mdash; Eva (@evacide) <a href="https://twitter.com/evacide/status/1281339999682564097?ref_src=twsrc%5Etfw">July 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Fantastic new paper on post-colonial theory and the ethics (and future) of artificial intelligence:<a href="https://t.co/saPxFeuHX8">https://t.co/saPxFeuHX8</a><br><br>Congratulations <a href="https://twitter.com/shakir_za?ref_src=twsrc%5Etfw">@shakir_za</a> üî• <a href="https://twitter.com/png_marie?ref_src=twsrc%5Etfw">@png_marie</a> üî• and <a href="https://twitter.com/wsisaac?ref_src=twsrc%5Etfw">@wsisaac</a> üî•</p>&mdash; Iason Gabriel (@IasonGabriel) <a href="https://twitter.com/IasonGabriel/status/1281271954402693120?ref_src=twsrc%5Etfw">July 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Pitfalls to Avoid when Interpreting Machine Learning Models

Christoph Molnar, Gunnar K√∂nig, Julia Herbinger, Timo Freiesleben, Susanne Dandl, Christian A. Scholbeck, Giuseppe Casalicchio, Moritz Grosse-Wentrup, Bernd Bischl

- retweets: 48, favorites: 142 (07/10/2020 07:46:46)

- links: [abs](https://arxiv.org/abs/2007.04131) | [pdf](https://arxiv.org/pdf/2007.04131)
- [stat.ML](https://arxiv.org/list/stat.ML/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Modern requirements for machine learning (ML) models include both high predictive performance and model interpretability. A growing number of techniques provide model interpretations, but can lead to wrong conclusions if applied incorrectly. We illustrate pitfalls of ML model interpretation such as bad model generalization, dependent features, feature interactions or unjustified causal interpretations. Our paper addresses ML practitioners by raising awareness of pitfalls and pointing out solutions for correct model interpretation, as well as ML researchers by discussing open issues for further research.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Our new paper &quot;Pitfalls to Avoid when Interpreting Machine Learning Models&quot; was accepted at the XXAI ICML workshop ü•≥<a href="https://t.co/u8F2EmsJg7">https://t.co/u8F2EmsJg7</a><a href="https://t.co/dFrVbkhpa2">https://t.co/dFrVbkhpa2</a></p>&mdash; Christoph Molnar (@ChristophMolnar) <a href="https://twitter.com/ChristophMolnar/status/1281117822547120129?ref_src=twsrc%5Etfw">July 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">A thread about the pitfalls we identified when interpreting ML models. Featuring poorly drawn comics from me. <a href="https://t.co/avPcASMITx">https://t.co/avPcASMITx</a> <br><br>üëá</p>&mdash; Christoph Molnar (@ChristophMolnar) <a href="https://twitter.com/ChristophMolnar/status/1281272026192326656?ref_src=twsrc%5Etfw">July 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Self-Supervised Policy Adaptation during Deployment

Nicklas Hansen, Yu Sun, Pieter Abbeel, Alexei A. Efros, Lerrel Pinto, Xiaolong Wang

- retweets: 26, favorites: 132 (07/10/2020 07:46:46)

- links: [abs](https://arxiv.org/abs/2007.04309) | [pdf](https://arxiv.org/pdf/2007.04309)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.RO](https://arxiv.org/list/cs.RO/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

In most real world scenarios, a policy trained by reinforcement learning in one environment needs to be deployed in another, potentially quite different environment. However, generalization across different environments is known to be hard. A natural solution would be to keep training after deployment in the new environment, but this cannot be done if the new environment offers no reward signal. Our work explores the use of self-supervision to allow the policy to continue training after deployment without using any rewards. While previous methods explicitly anticipate changes in the new environment, we assume no prior knowledge of those changes yet still obtain significant improvements. Empirical evaluations are performed on diverse environments from DeepMind Control suite and ViZDoom. Our method improves generalization in 25 out of 30 environments across various tasks, and outperforms domain randomization on a majority of environments.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Self-Supervised Policy Adaptation during Deployment<br>pdf: <a href="https://t.co/cNeCKQqacH">https://t.co/cNeCKQqacH</a><br>abs: <a href="https://t.co/kyaEv4kNQU">https://t.co/kyaEv4kNQU</a><br>project page: <a href="https://t.co/0LtChbJlr4">https://t.co/0LtChbJlr4</a><br>github: <a href="https://t.co/KspCOwciGE">https://t.co/KspCOwciGE</a> <a href="https://t.co/vzGkTpv2CB">pic.twitter.com/vzGkTpv2CB</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1281025224889249793?ref_src=twsrc%5Etfw">July 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Another cool paper to add to this list:<br><br>Self-Supervised Policy Adaptation during Deployment, by <a href="https://twitter.com/ncklashansen?ref_src=twsrc%5Etfw">@ncklashansen</a> et al.<a href="https://t.co/eTDMJsyPcv">https://t.co/eTDMJsyPcv</a><a href="https://t.co/HuaqwJFODY">https://t.co/HuaqwJFODY</a> <a href="https://t.co/mtLlzGxnh2">pic.twitter.com/mtLlzGxnh2</a></p>&mdash; hardmaru (@hardmaru) <a href="https://twitter.com/hardmaru/status/1281036983414865920?ref_src=twsrc%5Etfw">July 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Self-Supervised Policy Adaptation during Deployment<a href="https://t.co/JVAzxSubMf">https://t.co/JVAzxSubMf</a> <a href="https://twitter.com/hashtag/Robotics?src=hash&amp;ref_src=twsrc%5Etfw">#Robotics</a> <a href="https://t.co/grw9HlYF7B">pic.twitter.com/grw9HlYF7B</a></p>&mdash; Tomasz Malisiewicz (@quantombone) <a href="https://twitter.com/quantombone/status/1281069292818018305?ref_src=twsrc%5Etfw">July 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. The Scattering Compositional Learner: Discovering Objects, Attributes,  Relationships in Analogical Reasoning

Yuhuai Wu, Honghua Dong, Roger Grosse, Jimmy Ba

- retweets: 25, favorites: 91 (07/10/2020 07:46:47)

- links: [abs](https://arxiv.org/abs/2007.04212) | [pdf](https://arxiv.org/pdf/2007.04212)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LO](https://arxiv.org/list/cs.LO/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

In this work, we focus on an analogical reasoning task that contains rich compositional structures, Raven's Progressive Matrices (RPM). To discover compositional structures of the data, we propose the Scattering Compositional Learner (SCL), an architecture that composes neural networks in a sequence. Our SCL achieves state-of-the-art performance on two RPM datasets, with a 48.7% relative improvement on Balanced-RAVEN and 26.4% on PGM over the previous state-of-the-art. We additionally show that our model discovers compositional representations of objects' attributes (e.g., shape color, size), and their relationships (e.g., progression, union). We also find that the compositional representation makes the SCL significantly more robust to test-time domain shifts and greatly improves zero-shot generalization to previously unseen analogies.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Can Neural Networks solve IQ tests? We propose Scattering Compositional Learner (SCL) for RPM Task. SCL improves SOTA from 63.9% to 95.0%. It is even capable of zero-shot generalization and learns disentangled representations!<br><br>paper: <a href="https://t.co/9p5CFbQJ4m">https://t.co/9p5CFbQJ4m</a><br><br>(1/n) <a href="https://t.co/Adc1fzkKKi">pic.twitter.com/Adc1fzkKKi</a></p>&mdash; Yuhuai (Tony) Wu (@Yuhu_ai_) <a href="https://twitter.com/Yuhu_ai_/status/1281306072930750465?ref_src=twsrc%5Etfw">July 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. Language Modeling with Reduced Densities

Tai-Danae Bradley, Yiannis Vlassopoulos

- retweets: 17, favorites: 75 (07/10/2020 07:46:47)

- links: [abs](https://arxiv.org/abs/2007.03834) | [pdf](https://arxiv.org/pdf/2007.03834)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [math.CT](https://arxiv.org/list/math.CT/recent) | [quant-ph](https://arxiv.org/list/quant-ph/recent)

We present a framework for modeling words, phrases, and longer expressions in a natural language using reduced density operators. We show these operators capture something of the meaning of these expressions and, under the Loewner order on positive semidefinite operators, preserve both a simple form of entailment and the relevant statistics therein. Pulling back the curtain, the assignment is shown to be a functor between categories enriched over probabilities.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Today on the arXiv, I‚Äôve a new paper with Yiannis Vlassopoulos: <a href="https://t.co/pnfKANW2ed">https://t.co/pnfKANW2ed</a>. We describe an assignment of linear operators to expressions in a natural language that captures something of the statistics therein. <a href="https://t.co/8xhRXOGhJj">https://t.co/8xhRXOGhJj</a></p>&mdash; Tai-Danae Bradley (@math3ma) <a href="https://twitter.com/math3ma/status/1281242846201360391?ref_src=twsrc%5Etfw">July 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. A Free Viewpoint Portrait Generator with Dynamic Styling

Anpei Chen, Ruiyang Liu, Ling Xie, Jingyi Yu

- retweets: 14, favorites: 70 (07/10/2020 07:46:48)

- links: [abs](https://arxiv.org/abs/2007.03780) | [pdf](https://arxiv.org/pdf/2007.03780)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.GR](https://arxiv.org/list/cs.GR/recent)

Generating portrait images from a single latent space facing the problem of entangled attributes, making it difficult to explicitly adjust the generation on specific attributes, e.g., contour and viewpoint control or dynamic styling. Therefore, we propose to decompose the generation space into two subspaces: geometric and texture space. We first encode portrait scans with a semantic occupancy field (SOF), which represents semantic-embedded geometry structure and output free-viewpoint semantic segmentation maps. Then we design a semantic instance wised(SIW) StyleGAN to regionally styling the segmentation map. We capture 664 3D portrait scans for our SOF training and use real capture photos(FFHQ and CelebA-HQ) for SIW StyleGAN training. Adequate experiments show that our representations enable appearance consistent shape, pose, regional styles controlling, achieve state-of-the-art results, and generalize well in various application scenarios.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">A Free Viewpoint Portrait Generator with Dynamic Styling<br>pdf: <a href="https://t.co/bd4KVxxu1b">https://t.co/bd4KVxxu1b</a><br>abs: <a href="https://t.co/UP98OgmZMD">https://t.co/UP98OgmZMD</a> <a href="https://t.co/WGxHc3CQK3">pic.twitter.com/WGxHc3CQK3</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1281032139732070406?ref_src=twsrc%5Etfw">July 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



