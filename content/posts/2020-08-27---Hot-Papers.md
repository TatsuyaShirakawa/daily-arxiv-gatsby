---
title: Hot Papers 2020-08-27
date: 2020-08-28T10:08:57.Z
template: "post"
draft: false
slug: "hot-papers-2020-08-27"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-08-27"
socialImage: "/media/flying-marine.jpg"

---

# 1. SMAP: Single-Shot Multi-Person Absolute 3D Pose Estimation

Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang, Hujun Bao, Xiaowei Zhou

- retweets: 45, favorites: 153 (08/28/2020 10:08:57)

- links: [abs](https://arxiv.org/abs/2008.11469) | [pdf](https://arxiv.org/pdf/2008.11469)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Recovering multi-person 3D poses with absolute scales from a single RGB image is a challenging problem due to the inherent depth and scale ambiguity from a single view. Addressing this ambiguity requires to aggregate various cues over the entire image, such as body sizes, scene layouts, and inter-person relationships. However, most previous methods adopt a top-down scheme that first performs 2D pose detection and then regresses the 3D pose and scale for each detected person individually, ignoring global contextual cues. In this paper, we propose a novel system that first regresses a set of 2.5D representations of body parts and then reconstructs the 3D absolute poses based on these 2.5D representations with a depth-aware part association algorithm. Such a single-shot bottom-up scheme allows the system to better learn and reason about the inter-person depth relationship, improving both 3D and 2D pose estimation. The experiments demonstrate that the proposed approach achieves the state-of-the-art performance on the CMU Panoptic and MuPoTS-3D datasets and is applicable to in-the-wild videos.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">SMAP: Single-Shot Multi-Person Absolute 3D Pose Estimation<br>pdf: <a href="https://t.co/t3j8UChX2G">https://t.co/t3j8UChX2G</a><br>abs: <a href="https://t.co/He0TLboPbF">https://t.co/He0TLboPbF</a><br>project page: <a href="https://t.co/xyb9qX2IkV">https://t.co/xyb9qX2IkV</a><br>github: <a href="https://t.co/91b43ywVv2">https://t.co/91b43ywVv2</a> <a href="https://t.co/ovYUFM9brA">pic.twitter.com/ovYUFM9brA</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1298784433458352128?ref_src=twsrc%5Etfw">August 27, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. What is being transferred in transfer learning?

Behnam Neyshabur, Hanie Sedghi, Chiyuan Zhang

- retweets: 19, favorites: 117 (08/28/2020 10:08:58)

- links: [abs](https://arxiv.org/abs/2008.11687) | [pdf](https://arxiv.org/pdf/2008.11687)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

One desired capability for machines is the ability to transfer their knowledge of one domain to another where data is (usually) scarce. Despite ample adaptation of transfer learning in various deep learning applications, we yet do not understand what enables a successful transfer and which part of the network is responsible for that. In this paper, we provide new tools and analyses to address these fundamental questions. Through a series of analyses on transferring to block-shuffled images, we separate the effect of feature reuse from learning low-level statistics of data and show that some benefit of transfer learning comes from the latter. We present that when training from pre-trained weights, the model stays in the same basin in the loss landscape and different instances of such model are similar in feature space and close in parameter space.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Have you been thinking about “what is being transferred in transfer learning?” and what parts of the network are in charge of that? We have some answers for you! <a href="https://t.co/74I8ec1Tix">https://t.co/74I8ec1Tix</a> with <a href="https://twitter.com/bneyshabur?ref_src=twsrc%5Etfw">@bneyshabur</a> and Chiyuan Zhang <a href="https://t.co/YcOvNToFtG">pic.twitter.com/YcOvNToFtG</a></p>&mdash; Hanie Sedghi (@HanieSedghi) <a href="https://twitter.com/HanieSedghi/status/1299094508517224448?ref_src=twsrc%5Etfw">August 27, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. NAS-DIP: Learning Deep Image Prior with Neural Architecture Search

Yun-Chun Chen, Chen Gao, Esther Robb, Jia-Bin Huang

- retweets: 14, favorites: 105 (08/28/2020 10:08:58)

- links: [abs](https://arxiv.org/abs/2008.11713) | [pdf](https://arxiv.org/pdf/2008.11713)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Recent work has shown that the structure of deep convolutional neural networks can be used as a structured image prior for solving various inverse image restoration tasks. Instead of using hand-designed architectures, we propose to search for neural architectures that capture stronger image priors. Building upon a generic U-Net architecture, our core contribution lies in designing new search spaces for (1) an upsampling cell and (2) a pattern of cross-scale residual connections. We search for an improved network by leveraging an existing neural architecture search algorithm (using reinforcement learning with a recurrent neural network controller). We validate the effectiveness of our method via a wide variety of applications, including image restoration, dehazing, image-to-image translation, and matrix factorization. Extensive experimental results show that our algorithm performs favorably against state-of-the-art learning-free approaches and reaches competitive performance with existing learning-based methods in some cases.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">NAS-DIP: Learning Deep Image Prior with Neural Architecture Search<br>pdf: <a href="https://t.co/o632NYfCEp">https://t.co/o632NYfCEp</a><br>abs: <a href="https://t.co/ykz6jSMtrj">https://t.co/ykz6jSMtrj</a><br>project page: <a href="https://t.co/BVS8CNjzMA">https://t.co/BVS8CNjzMA</a><br>github: <a href="https://t.co/1pZkDP8Huh">https://t.co/1pZkDP8Huh</a><br>colab: <a href="https://t.co/1JXhONeQ1l">https://t.co/1JXhONeQ1l</a> <a href="https://t.co/G8Cl6XXGw3">pic.twitter.com/G8Cl6XXGw3</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1298787521397170176?ref_src=twsrc%5Etfw">August 27, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Anime-to-Real Clothing: Cosplay Costume Generation via Image-to-Image  Translation

Koya Tango, Marie Katsurai, Hayato Maki, Ryosuke Goto

- retweets: 24, favorites: 58 (08/28/2020 10:08:58)

- links: [abs](https://arxiv.org/abs/2008.11479) | [pdf](https://arxiv.org/pdf/2008.11479)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [eess.IV](https://arxiv.org/list/eess.IV/recent)

Cosplay has grown from its origins at fan conventions into a billion-dollar global dress phenomenon. To facilitate imagination and reinterpretation from animated images to real garments, this paper presents an automatic costume image generation method based on image-to-image translation. Cosplay items can be significantly diverse in their styles and shapes, and conventional methods cannot be directly applied to the wide variation in clothing images that are the focus of this study. To solve this problem, our method starts by collecting and preprocessing web images to prepare a cleaned, paired dataset of the anime and real domains. Then, we present a novel architecture for generative adversarial networks (GANs) to facilitate high-quality cosplay image generation. Our GAN consists of several effective techniques to fill the gap between the two domains and improve both the global and local consistency of generated images. Experiments demonstrated that, with two types of evaluation metrics, the proposed GAN achieves better performance than existing methods. We also showed that the images generated by the proposed method are more realistic than those generated by the conventional methods. Our codes and pretrained model are available on the web.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Anime-to-Real Clothing: Cosplay Costume Generation via<br>Image-to-Image Translation<br>pdf: <a href="https://t.co/0M5azjVglX">https://t.co/0M5azjVglX</a><br>abs: <a href="https://t.co/OcbOxB9amV">https://t.co/OcbOxB9amV</a> <a href="https://t.co/WdX3aYZJV7">pic.twitter.com/WdX3aYZJV7</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1298785874864467971?ref_src=twsrc%5Etfw">August 27, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



