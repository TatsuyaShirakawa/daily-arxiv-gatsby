---
title: Hot Papers 2020-07-31
date: 2020-08-02T21:40:23.Z
template: "post"
draft: false
slug: "hot-papers-2020-07-31"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-07-31"
socialImage: "/media/flying-marine.jpg"

---

# 1. Unselfie: Translating Selfies to Neutral-pose Portraits in the Wild

Liqian Ma, Zhe Lin, Connelly Barnes, Alexei A. Efros, Jingwan Lu

- retweets: 124, favorites: 412 (08/02/2020 21:40:23)

- links: [abs](https://arxiv.org/abs/2007.15068) | [pdf](https://arxiv.org/pdf/2007.15068)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Due to the ubiquity of smartphones, it is popular to take photos of one's self, or "selfies." Such photos are convenient to take, because they do not require specialized equipment or a third-party photographer. However, in selfies, constraints such as human arm length often make the body pose look unnatural. To address this issue, we introduce $\textit{unselfie}$, a novel photographic transformation that automatically translates a selfie into a neutral-pose portrait. To achieve this, we first collect an unpaired dataset, and introduce a way to synthesize paired training data for self-supervised learning. Then, to $\textit{unselfie}$ a photo, we propose a new three-stage pipeline, where we first find a target neutral pose, inpaint the body texture, and finally refine and composite the person on the background. To obtain a suitable target neutral pose, we propose a novel nearest pose search module that makes the reposing task easier and enables the generation of multiple neutral-pose results among which users can choose the best one they like. Qualitative and quantitative evaluations show the superiority of our pipeline over alternatives.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Unselfie: Translating Selfies to Neutral-pose Portraits in the Wild<br>pdf: <a href="https://t.co/OH0auVxUaj">https://t.co/OH0auVxUaj</a><br>abs: <a href="https://t.co/DEysx6npIa">https://t.co/DEysx6npIa</a> <a href="https://t.co/TTETeUMWKV">pic.twitter.com/TTETeUMWKV</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1289022285593169920?ref_src=twsrc%5Etfw">July 31, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Functionality-Driven Musculature Retargeting

Hoseok Ryu, Minseok Kim, Seunghwan Lee, Moon Seok Park, Kyoungmin Lee, Jehee Lee

- retweets: 30, favorites: 133 (08/02/2020 21:40:24)

- links: [abs](https://arxiv.org/abs/2007.15311) | [pdf](https://arxiv.org/pdf/2007.15311)
- [cs.GR](https://arxiv.org/list/cs.GR/recent)

We present a novel retargeting algorithm that transfers the musculature of a reference anatomical model to new bodies with different sizes, body proportions, muscle capability, and joint range of motion while preserving the functionality of the original musculature as closely as possible. The geometric configuration and physiological parameters of musculotendon units are estimated and optimized to adapt to new bodies. The range of motion around joints is estimated from a motion capture dataset and edited further for individual models. The retargeted model is simulation-ready, so we can physically simulate muscle-actuated motor skills with the model. Our system is capable of generating a wide variety of anatomical bodies that can be simulated to walk, run, jump and dance while maintaining balance under gravity. We will also demonstrate the construction of individualized musculoskeletal models from bi-planar X-ray images and medical examinations.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Functionality-Driven Musculature Retargeting<br>pdf: <a href="https://t.co/iDdRNVsrJY">https://t.co/iDdRNVsrJY</a><br>abs: <a href="https://t.co/KUe1avdeXG">https://t.co/KUe1avdeXG</a> <a href="https://t.co/sb28LbMsvn">pic.twitter.com/sb28LbMsvn</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1289017751990874114?ref_src=twsrc%5Etfw">July 31, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Contrastive Learning for Unpaired Image-to-Image Translation

Taesung Park, Alexei A. Efros, Richard Zhang, Jun-Yan Zhu

- retweets: 29, favorites: 129 (08/02/2020 21:40:24)

- links: [abs](https://arxiv.org/abs/2007.15651) | [pdf](https://arxiv.org/pdf/2007.15651)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

In image-to-image translation, each patch in the output should reflect the content of the corresponding patch in the input, independent of domain. We propose a straightforward method for doing so -- maximizing mutual information between the two, using a framework based on contrastive learning. The method encourages two elements (corresponding patches) to map to a similar point in a learned feature space, relative to other elements (other patches) in the dataset, referred to as negatives. We explore several critical design choices for making contrastive learning effective in the image synthesis setting. Notably, we use a multilayer, patch-based approach, rather than operate on entire images. Furthermore, we draw negatives from within the input image itself, rather than from the rest of the dataset. We demonstrate that our framework enables one-sided translation in the unpaired image-to-image translation setting, while improving quality and reducing training time. In addition, our method can even be extended to the training setting where each "domain" is only a single image.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Contrastive Learning for Unpaired Image-to-Image Translation<br>pdf: <a href="https://t.co/RUkNQEhBfj">https://t.co/RUkNQEhBfj</a><br>abs: <a href="https://t.co/BpyXJta9f6">https://t.co/BpyXJta9f6</a><br>project page: <a href="https://t.co/lNnxwXpshp">https://t.co/lNnxwXpshp</a><br>github: <a href="https://t.co/rdp9jVbgh0">https://t.co/rdp9jVbgh0</a> <a href="https://t.co/Mozr1hbVCi">pic.twitter.com/Mozr1hbVCi</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1289015892546924544?ref_src=twsrc%5Etfw">July 31, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Perceiving 3D Human-Object Spatial Arrangements from a Single Image in  the Wild

Jason Y. Zhang, Sam Pepose, Hanbyul Joo, Deva Ramanan, Jitendra Malik, Angjoo Kanazawa

- retweets: 35, favorites: 108 (08/02/2020 21:40:24)

- links: [abs](https://arxiv.org/abs/2007.15649) | [pdf](https://arxiv.org/pdf/2007.15649)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

We present a method that infers spatial arrangements and shapes of humans and objects in a globally consistent 3D scene, all from a single image in-the-wild captured in an uncontrolled environment. Notably, our method runs on datasets without any scene- or object-level 3D supervision. Our key insight is that considering humans and objects jointly gives rise to "3D common sense" constraints that can be used to resolve ambiguity. In particular, we introduce a scale loss that learns the distribution of object size from data; an occlusion-aware silhouette re-projection loss to optimize object pose; and a human-object interaction loss to capture the spatial layout of objects with which humans interact. We empirically validate that our constraints dramatically reduce the space of likely 3D spatial configurations. We demonstrate our approach on challenging, in-the-wild images of humans interacting with large objects (such as bicycles, motorcycles, and surfboards) and handheld objects (such as laptops, tennis rackets, and skateboards). We quantify the ability of our approach to recover human-object arrangements and outline remaining challenges in this relatively domain. The project webpage can be found at https://jasonyzhang.com/phosa.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Perceiving 3D Human-Object Spatial Arrangements from a Single Image in the Wild<br>pdf: <a href="https://t.co/9fSd2fsmGl">https://t.co/9fSd2fsmGl</a><br>abs: <a href="https://t.co/2Hexo6eAKU">https://t.co/2Hexo6eAKU</a><br>project page: <a href="https://t.co/jqg9NZhoNH">https://t.co/jqg9NZhoNH</a> <a href="https://t.co/NgzhqpnjlV">pic.twitter.com/NgzhqpnjlV</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1289006654902743040?ref_src=twsrc%5Etfw">July 31, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Neural Modeling for Named Entities and Morphology (NEMO^2)

Dan Bareket, Reut Tsarfaty

- retweets: 15, favorites: 76 (08/02/2020 21:40:24)

- links: [abs](https://arxiv.org/abs/2007.15620) | [pdf](https://arxiv.org/pdf/2007.15620)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

Named Entity Recognition (NER) is a fundamental NLP task, commonly formulated as classification over a sequence of tokens. Morphologically-Rich Languages (MRLs) pose a challenge to this basic formulation, as the boundaries of Named Entities do not coincide with token boundaries, rather, they respect morphological boundaries. To address NER in MRLs we then need to answer two fundamental modeling questions: (i) What should be the basic units to be identified and labeled, are they token-based or morpheme-based? and (ii) How can morphological units be encoded and accurately obtained in realistic (non-gold) scenarios? We empirically investigate these questions on a novel parallel NER benchmark we deliver, with parallel token-level and morpheme-level NER annotations for Modern Hebrew, a morphologically complex language. Our results show that explicitly modeling morphological boundaries consistently leads to improved NER performance, and that a novel hybrid architecture that we propose, in which NER precedes and prunes the morphological decomposition (MD) space, greatly outperforms the standard pipeline approach, on both Hebrew NER and Hebrew MD in realistic scenarios.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Our Hebrew Name Entity Recognition (NER) work is up on arxiv! Data/code to be released soon(ish). In the meantime, we are interested in any comments or thoughts you might have. Enjoy! <br><br>With <a href="https://twitter.com/dbreqt?ref_src=twsrc%5Etfw">@dbreqt</a><a href="https://t.co/vPCXWDaWQj">https://t.co/vPCXWDaWQj</a></p>&mdash; Reut Tsarfaty (@rtsarfaty) <a href="https://twitter.com/rtsarfaty/status/1289100429582831616?ref_src=twsrc%5Etfw">July 31, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. VocGAN: A High-Fidelity Real-time Vocoder with a Hierarchically-nested  Adversarial Network

Jinhyeok Yang, Junmo Lee, Youngik Kim, Hoonyoung Cho, Injung Kim

- retweets: 19, favorites: 63 (08/02/2020 21:40:24)

- links: [abs](https://arxiv.org/abs/2007.15256) | [pdf](https://arxiv.org/pdf/2007.15256)
- [eess.AS](https://arxiv.org/list/eess.AS/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.SD](https://arxiv.org/list/cs.SD/recent) | [eess.SP](https://arxiv.org/list/eess.SP/recent)

We present a novel high-fidelity real-time neural vocoder called VocGAN. A recently developed GAN-based vocoder, MelGAN, produces speech waveforms in real-time. However, it often produces a waveform that is insufficient in quality or inconsistent with acoustic characteristics of the input mel spectrogram. VocGAN is nearly as fast as MelGAN, but it significantly improves the quality and consistency of the output waveform. VocGAN applies a multi-scale waveform generator and a hierarchically-nested discriminator to learn multiple levels of acoustic properties in a balanced way. It also applies the joint conditional and unconditional objective, which has shown successful results in high-resolution image synthesis. In experiments, VocGAN synthesizes speech waveforms 416.7x faster on a GTX 1080Ti GPU and 3.24x faster on a CPU than real-time. Compared with MelGAN, it also exhibits significantly improved quality in multiple evaluation metrics including mean opinion score (MOS) with minimal additional overhead. Additionally, compared with Parallel WaveGAN, another recently developed high-fidelity vocoder, VocGAN is 6.98x faster on a CPU and exhibits higher MOS.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">VocGAN: A High-Fidelity Real-time Vocoder with a Hierarchically-nested Adversarial Network<br>pdf: <a href="https://t.co/3hRn2ienHi">https://t.co/3hRn2ienHi</a><br>abs: <a href="https://t.co/ofmvPItvab">https://t.co/ofmvPItvab</a><br>samples: <a href="https://t.co/TYkJPmEFFz">https://t.co/TYkJPmEFFz</a> <a href="https://t.co/wRS8747LWi">pic.twitter.com/wRS8747LWi</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1289130886189940736?ref_src=twsrc%5Etfw">July 31, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. Music FaderNets: Controllable Music Generation Based On High-Level  Features via Low-Level Feature Modelling

Hao Hao Tan, Dorien Herremans

- retweets: 12, favorites: 47 (08/02/2020 21:40:24)

- links: [abs](https://arxiv.org/abs/2007.15474) | [pdf](https://arxiv.org/pdf/2007.15474)
- [eess.AS](https://arxiv.org/list/eess.AS/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.SD](https://arxiv.org/list/cs.SD/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

High-level musical qualities (such as emotion) are often abstract, subjective, and hard to quantify. Given these difficulties, it is not easy to learn good feature representations with supervised learning techniques, either because of the insufficiency of labels, or the subjectiveness (and hence large variance) in human-annotated labels. In this paper, we present a framework that can learn high-level feature representations with a limited amount of data, by first modelling their corresponding quantifiable low-level attributes. We refer to our proposed framework as Music FaderNets, which is inspired by the fact that low-level attributes can be continuously manipulated by separate "sliding faders" through feature disentanglement and latent regularization techniques. High-level features are then inferred from the low-level representations through semi-supervised clustering using Gaussian Mixture Variational Autoencoders (GM-VAEs). Using arousal as an example of a high-level feature, we show that the "faders" of our model are disentangled and change linearly w.r.t. the modelled low-level attributes of the generated output music. Furthermore, we demonstrate that the model successfully learns the intrinsic relationship between arousal and its corresponding low-level attributes (rhythm and note density), with only 1% of the training set being labelled. Finally, using the learnt high-level feature representations, we explore the application of our framework in style transfer tasks across different arousal states. The effectiveness of this approach is verified through a subjective listening test.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Music FaderNets: Controllable Music Generation Based On High-Level Features via Low-Level Feature Modelling<br>pdf: <a href="https://t.co/DlJmVx9z8g">https://t.co/DlJmVx9z8g</a><br>abs: <a href="https://t.co/xQ7lhbHsEA">https://t.co/xQ7lhbHsEA</a><br>samples: <a href="https://t.co/lc72R9tfF2">https://t.co/lc72R9tfF2</a> <a href="https://t.co/RnfMoiKl4r">pic.twitter.com/RnfMoiKl4r</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1289133101466230784?ref_src=twsrc%5Etfw">July 31, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. Learning from Few Samples: A Survey

Nihar Bendre, Hugo Terashima Marín, Peyman Najafirad

- retweets: 12, favorites: 42 (08/02/2020 21:40:24)

- links: [abs](https://arxiv.org/abs/2007.15484) | [pdf](https://arxiv.org/pdf/2007.15484)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Deep neural networks have been able to outperform humans in some cases like image recognition and image classification. However, with the emergence of various novel categories, the ability to continuously widen the learning capability of such networks from limited samples, still remains a challenge. Techniques like Meta-Learning and/or few-shot learning showed promising results, where they can learn or generalize to a novel category/task based on prior knowledge. In this paper, we perform a study of the existing few-shot meta-learning techniques in the computer vision domain based on their method and evaluation metrics. We provide a taxonomy for the techniques and categorize them as data-augmentation, embedding, optimization and semantics based learning for few-shot, one-shot and zero-shot settings. We then describe the seminal work done in each category and discuss their approach towards solving the predicament of learning from few samples. Lastly we provide a comparison of these techniques on the commonly used benchmark datasets: Omniglot, and MiniImagenet, along with a discussion towards the future direction of improving the performance of these techniques towards the final goal of outperforming humans.



