---
title: Hot Papers 2021-08-25
date: 2021-08-26T07:51:57.Z
template: "post"
draft: false
slug: "hot-papers-2021-08-25"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2021-08-25"
socialImage: "/media/flying-marine.jpg"

---

# 1. Isaac Gym: High Performance GPU-Based Physics Simulation For Robot  Learning

Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, Gavriel State

- retweets: 4692, favorites: 303 (08/26/2021 07:51:57)

- links: [abs](https://arxiv.org/abs/2108.10470) | [pdf](https://arxiv.org/pdf/2108.10470)
- [cs.RO](https://arxiv.org/list/cs.RO/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Isaac Gym offers a high performance learning platform to train policies for wide variety of robotics tasks directly on GPU. Both physics simulation and the neural network policy training reside on GPU and communicate by directly passing data from physics buffers to PyTorch tensors without ever going through any CPU bottlenecks. This leads to blazing fast training times for complex robotics tasks on a single GPU with 1-2 orders of magnitude improvements compared to conventional RL training that uses a CPU based simulator and GPU for neural networks. We host the results and videos at \url{https://sites.google.com/view/isaacgym-nvidia} and isaac gym can be download at \url{https://developer.nvidia.com/isaac-gym}.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning<br>pdf: <a href="https://t.co/mxHvCmGETT">https://t.co/mxHvCmGETT</a><br>abs: <a href="https://t.co/SwDA2pGRby">https://t.co/SwDA2pGRby</a><br>project page: <a href="https://t.co/PnPSfutrCN">https://t.co/PnPSfutrCN</a> <a href="https://t.co/YOFSEyp6GK">pic.twitter.com/YOFSEyp6GK</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1430346728117571586?ref_src=twsrc%5Etfw">August 25, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras

Zachary Teed, Jia Deng

- retweets: 1056, favorites: 173 (08/26/2021 07:51:57)

- links: [abs](https://arxiv.org/abs/2108.10869) | [pdf](https://arxiv.org/pdf/2108.10869)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

We introduce DROID-SLAM, a new deep learning based SLAM system. DROID-SLAM consists of recurrent iterative updates of camera pose and pixelwise depth through a Dense Bundle Adjustment layer. DROID-SLAM is accurate, achieving large improvements over prior work, and robust, suffering from substantially fewer catastrophic failures. Despite training on monocular video, it can leverage stereo or RGB-D video to achieve improved performance at test time. The URL to our open source code is https://github.com/princeton-vl/DROID-SLAM.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras <a href="https://t.co/dKYhIVtAHC">https://t.co/dKYhIVtAHC</a> <a href="https://twitter.com/hashtag/computervision?src=hash&amp;ref_src=twsrc%5Etfw">#computervision</a> <a href="https://twitter.com/hashtag/robotics?src=hash&amp;ref_src=twsrc%5Etfw">#robotics</a><br><br>The method uses recurrent iterative updates of camera pose and pixelwise depth through a Dense Bundle Adjustment layer. Runs in real-time with two 3090 GPUs. <a href="https://t.co/1z3sFY0ctN">pic.twitter.com/1z3sFY0ctN</a></p>&mdash; Tomasz Malisiewicz (@quantombone) <a href="https://twitter.com/quantombone/status/1430369800887341058?ref_src=twsrc%5Etfw">August 25, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. imGHUM: Implicit Generative Models of 3D Human Shape and Articulated  Pose

Thiemo Alldieck, Hongyi Xu, Cristian Sminchisescu

- retweets: 180, favorites: 122 (08/26/2021 07:51:57)

- links: [abs](https://arxiv.org/abs/2108.10842) | [pdf](https://arxiv.org/pdf/2108.10842)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

We present imGHUM, the first holistic generative model of 3D human shape and articulated pose, represented as a signed distance function. In contrast to prior work, we model the full human body implicitly as a function zero-level-set and without the use of an explicit template mesh. We propose a novel network architecture and a learning paradigm, which make it possible to learn a detailed implicit generative model of human pose, shape, and semantics, on par with state-of-the-art mesh-based models. Our model features desired detail for human models, such as articulated pose including hand motion and facial expressions, a broad spectrum of shape variations, and can be queried at arbitrary resolutions and spatial locations. Additionally, our model has attached spatial semantics making it straightforward to establish correspondences between different shape instances, thus enabling applications that are difficult to tackle using classical implicit representations. In extensive experiments, we demonstrate the model accuracy and its applicability to current research problems.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">imGHUM: Implicit Generative Models of 3D Human Shape and Articulated Pose<br>pdf: <a href="https://t.co/T8g4lt6q1s">https://t.co/T8g4lt6q1s</a><br>abs: <a href="https://t.co/aYkTV8wLOG">https://t.co/aYkTV8wLOG</a><br><br>the first holistic generative model of 3D human shape and articulated pose, represented as a signed distance function <a href="https://t.co/mTyavcZ9kW">pic.twitter.com/mTyavcZ9kW</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1430334293038047239?ref_src=twsrc%5Etfw">August 25, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Let me tell you about &quot;imGHUM: Implicit Generative Models of 3D Human Shape and Articulated Pose&quot; joint work with Hongyi Xu (equal contr.) and <a href="https://twitter.com/CSminchisescu?ref_src=twsrc%5Etfw">@CSminchisescu</a> to appear ICCV 2021!<br><br>üìú <a href="https://t.co/q9GGOcFBSO">https://t.co/q9GGOcFBSO</a><br>üíª <a href="https://t.co/pz0wV9UaPe">https://t.co/pz0wV9UaPe</a><br><br>A thread. üßµ <a href="https://t.co/PftcWpLIqO">pic.twitter.com/PftcWpLIqO</a></p>&mdash; Thiemo Alldieck (@thmo_a) <a href="https://twitter.com/thmo_a/status/1430431483823525888?ref_src=twsrc%5Etfw">August 25, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. One TTS Alignment To Rule Them All

Rohan Badlani, Adrian ≈Åancucki, Kevin J. Shih, Rafael Valle, Wei Ping, Bryan Catanzaro

- retweets: 126, favorites: 55 (08/26/2021 07:51:58)

- links: [abs](https://arxiv.org/abs/2108.10447) | [pdf](https://arxiv.org/pdf/2108.10447)
- [cs.SD](https://arxiv.org/list/cs.SD/recent) | [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [eess.AS](https://arxiv.org/list/eess.AS/recent)

Speech-to-text alignment is a critical component of neural textto-speech (TTS) models. Autoregressive TTS models typically use an attention mechanism to learn these alignments on-line. However, these alignments tend to be brittle and often fail to generalize to long utterances and out-of-domain text, leading to missing or repeating words. Most non-autoregressive endto-end TTS models rely on durations extracted from external sources. In this paper we leverage the alignment mechanism proposed in RAD-TTS as a generic alignment learning framework, easily applicable to a variety of neural TTS models. The framework combines forward-sum algorithm, the Viterbi algorithm, and a simple and efficient static prior. In our experiments, the alignment learning framework improves all tested TTS architectures, both autoregressive (Flowtron, Tacotron 2) and non-autoregressive (FastPitch, FastSpeech 2, RAD-TTS). Specifically, it improves alignment convergence speed of existing attention-based mechanisms, simplifies the training pipeline, and makes the models more robust to errors on long utterances. Most importantly, the framework improves the perceived speech synthesis quality, as judged by human evaluators.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">One TTS Alignment To Rule Them All<br>pdf: <a href="https://t.co/yQu2GWB6uw">https://t.co/yQu2GWB6uw</a><br>abs: <a href="https://t.co/cAAxzOuGGg">https://t.co/cAAxzOuGGg</a><br><br>present an alignment framework that is broadly applicable to various TTS architectures, both autoregressive and parallel <a href="https://t.co/e4tpCSNwWC">pic.twitter.com/e4tpCSNwWC</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1430335575408758790?ref_src=twsrc%5Etfw">August 25, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Lessons from AlphaZero for Optimal, Model Predictive, and Adaptive  Control

Dimitri Bertsekas

- retweets: 92, favorites: 76 (08/26/2021 07:51:58)

- links: [abs](https://arxiv.org/abs/2108.10315) | [pdf](https://arxiv.org/pdf/2108.10315)
- [math.OC](https://arxiv.org/list/math.OC/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

In this paper we aim to provide analysis and insights (often based on visualization), which explain the beneficial effects of on-line decision making on top of off-line training. In particular, through a unifying abstract mathematical framework, we show that the principal AlphaZero/TD-Gammon ideas of approximation in value space and rollout apply very broadly to deterministic and stochastic optimal control problems, involving both discrete and continuous search spaces. Moreover, these ideas can be effectively integrated with other important methodologies such as model predictive control, adaptive control, decentralized control, discrete and Bayesian optimization, neural network-based value and policy approximations, and heuristic algorithms for discrete optimization.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">[2108.10315] Lessons from AlphaZero for Optimal, Model Predictive, and Adaptive Control (by D.P. Bertsekas) <a href="https://t.co/xS1he2phUh">https://t.co/xS1he2phUh</a></p>&mdash; Maxim Raginsky (@mraginsky) <a href="https://twitter.com/mraginsky/status/1430350711162822656?ref_src=twsrc%5Etfw">August 25, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. ComSum: Commit Messages Summarization and Meaning Preservation

Leshem Choshen, Idan Amit

- retweets: 43, favorites: 29 (08/26/2021 07:51:58)

- links: [abs](https://arxiv.org/abs/2108.10763) | [pdf](https://arxiv.org/pdf/2108.10763)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.SE](https://arxiv.org/list/cs.SE/recent)

We present ComSum, a data set of 7 million commit messages for text summarization. When documenting commits, software code changes, both a message and its summary are posted. We gather and filter those to curate developers' work summarization data set. Along with its growing size, practicality and challenging language domain, the data set benefits from the living field of empirical software engineering. As commits follow a typology, we propose to not only evaluate outputs by Rouge, but by their meaning preservation.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">ComSum: Commit Messages Summarization and<br>Meaning Preservation<br>pdf: <a href="https://t.co/27xqDmpwyY">https://t.co/27xqDmpwyY</a><br>abs: <a href="https://t.co/xYFGxWoGQS">https://t.co/xYFGxWoGQS</a><br><br>a data set of 7 million commit messages for text summarization <a href="https://t.co/ajsZZuiYOb">pic.twitter.com/ajsZZuiYOb</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1430355156244307968?ref_src=twsrc%5Etfw">August 25, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



