---
title: Hot Papers 2020-12-10
date: 2020-12-11T09:48:05.Z
template: "post"
draft: false
slug: "hot-papers-2020-12-10"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-12-10"
socialImage: "/media/flying-marine.jpg"

---

# 1. A guide to choosing and implementing reference models for social network  analysis

Elizabeth A. Hobson, Matthew J. Silk, Nina H. Fefferman, Daniel B. Larremore, Puck Rombach, Saray Shai, Noa Pinter-Wollman

- retweets: 1514, favorites: 162 (12/11/2020 09:48:05)

- links: [abs](https://arxiv.org/abs/2012.04720) | [pdf](https://arxiv.org/pdf/2012.04720)
- [cs.SI](https://arxiv.org/list/cs.SI/recent) | [physics.soc-ph](https://arxiv.org/list/physics.soc-ph/recent)

Analyzing social networks is challenging. Key features of relational data require the use of non-standard statistical methods such as developing system-specific null, or reference, models that randomize one or more components of the observed data. Here we review a variety of randomization procedures that generate reference models for social network analysis. Reference models provide an expectation for hypothesis-testing when analyzing network data. We outline the key stages in producing an effective reference model and detail four approaches for generating reference distributions: permutation, resampling, sampling from a distribution, and generative models. We highlight when each type of approach would be appropriate and note potential pitfalls for researchers to avoid. Throughout, we illustrate our points with examples from a simulated social system. Our aim is to provide social network researchers with a deeper understanding of analytical approaches to enhance their confidence when tailoring reference models to specific research questions.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">New preprint! Figuring out how to best analyze social network data can be a brain breaker. In this paper, we discuss several considerations, potential pitfalls, and best practices for thinking through your analyses. <a href="https://t.co/ZslasOfsSf">https://t.co/ZslasOfsSf</a> <a href="https://t.co/j44BbIeCoS">pic.twitter.com/j44BbIeCoS</a></p>&mdash; Dr. Elizabeth Hobson (@Eliz_Hobson) <a href="https://twitter.com/Eliz_Hobson/status/1336868143189417984?ref_src=twsrc%5Etfw">December 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. EvoCraft: A New Challenge for Open-Endedness

Djordje Grbic, Rasmus Berg Palm, Elias Najarro, Claire Glanois, Sebastian Risi

- retweets: 1406, favorites: 170 (12/11/2020 09:48:06)

- links: [abs](https://arxiv.org/abs/2012.04751) | [pdf](https://arxiv.org/pdf/2012.04751)
- [cs.AI](https://arxiv.org/list/cs.AI/recent)

This paper introduces EvoCraft, a framework for Minecraft designed to study open-ended algorithms. We introduce an API that provides an open-source Python interface for communicating with Minecraft to place and track blocks. In contrast to previous work in Minecraft that focused on learning to play the game, the grand challenge we pose here is to automatically search for increasingly complex artifacts in an open-ended fashion. Compared to other environments used to study open-endedness, Minecraft allows the construction of almost any kind of structure, including actuated machines with circuits and mechanical components. We present initial baseline results in evolving simple Minecraft creations through both interactive and automated evolution. While evolution succeeds when tasked to grow a structure towards a specific target, it is unable to find a solution when rewarded for creating a simple machine that moves. Thus, EvoCraft offers a challenging new environment for automated search methods (such as evolution) to find complex artifacts that we hope will spur the development of more open-ended algorithms. A Python implementation of the EvoCraft framework is available at: https://github.com/real-itu/Evocraft-py.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">EvoCraft: A New Challenge for Open-Endedness!<br><br>Can we automatically search for increasingly complex artifacts/machines in an open-ended fashion in Minecraft?<br>PDF: <a href="https://t.co/ysW4VUXXMb">https://t.co/ysW4VUXXMb</a><br>Python API: <a href="https://t.co/7jPBCx6aLG">https://t.co/7jPBCx6aLG</a><br>First competition on open-endedness coming: <a href="https://twitter.com/GeccoConf?ref_src=twsrc%5Etfw">@GeccoConf</a>!ðŸ‘‡ <a href="https://t.co/nQCbEb9ep1">pic.twitter.com/nQCbEb9ep1</a></p>&mdash; Sebastian Risi (@risi1979) <a href="https://twitter.com/risi1979/status/1337000376353087490?ref_src=twsrc%5Etfw">December 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. On the Binding Problem in Artificial Neural Networks

Klaus Greff, Sjoerd van Steenkiste, JÃ¼rgen Schmidhuber

- retweets: 787, favorites: 132 (12/11/2020 09:48:06)

- links: [abs](https://arxiv.org/abs/2012.05208) | [pdf](https://arxiv.org/pdf/2012.05208)
- [cs.NE](https://arxiv.org/list/cs.NE/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Contemporary neural networks still fall short of human-level generalization, which extends far beyond our direct experiences. In this paper, we argue that the underlying cause for this shortcoming is their inability to dynamically and flexibly bind information that is distributed throughout the network. This binding problem affects their capacity to acquire a compositional understanding of the world in terms of symbol-like entities (like objects), which is crucial for generalizing in predictable and systematic ways. To address this issue, we propose a unifying framework that revolves around forming meaningful entities from unstructured sensory inputs (segregation), maintaining this separation of information at a representational level (representation), and using these entities to construct new inferences, predictions, and behaviors (composition). Our analysis draws inspiration from a wealth of research in neuroscience and cognitive psychology, and surveys relevant mechanisms from the machine learning literature, to help identify a combination of inductive biases that allow symbolic information processing to emerge naturally in neural networks. We believe that a compositional approach to AI, in terms of grounded symbol-like representations, is of fundamental importance for realizing human-level generalization, and we hope that this paper may contribute towards that goal as a reference and inspiration.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Klaus and I are excited to finally share our survey/perspectives paper, which was 3 years in the making ðŸŽ‰. It links systematic generalization, symbols, objects, and the binding problem in NNs. w/ <a href="https://twitter.com/SchmidhuberAI?ref_src=twsrc%5Etfw">@SchmidhuberAI</a><a href="https://t.co/J9PAUR4AKu">https://t.co/J9PAUR4AKu</a> <a href="https://t.co/wAZ95Di2gg">pic.twitter.com/wAZ95Di2gg</a></p>&mdash; Sjoerd van Steenkiste @NeurIPS2020 (@vansteenkiste_s) <a href="https://twitter.com/vansteenkiste_s/status/1337053434596483083?ref_src=twsrc%5Etfw">December 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Positional Encoding as Spatial Inductive Bias in GANs

Rui Xu, Xintao Wang, Kai Chen, Bolei Zhou, Chen Change Loy

- retweets: 576, favorites: 176 (12/11/2020 09:48:06)

- links: [abs](https://arxiv.org/abs/2012.05217) | [pdf](https://arxiv.org/pdf/2012.05217)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

SinGAN shows impressive capability in learning internal patch distribution despite its limited effective receptive field. We are interested in knowing how such a translation-invariant convolutional generator could capture the global structure with just a spatially i.i.d. input. In this work, taking SinGAN and StyleGAN2 as examples, we show that such capability, to a large extent, is brought by the implicit positional encoding when using zero padding in the generators. Such positional encoding is indispensable for generating images with high fidelity. The same phenomenon is observed in other generative architectures such as DCGAN and PGGAN. We further show that zero padding leads to an unbalanced spatial bias with a vague relation between locations. To offer a better spatial inductive bias, we investigate alternative positional encodings and analyze their effects. Based on a more flexible positional encoding explicitly, we propose a new multi-scale training strategy and demonstrate its effectiveness in the state-of-the-art unconditional generator StyleGAN2. Besides, the explicit spatial inductive bias substantially improve SinGAN for more versatile image manipulation.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Positional Encoding as Spatial Inductive Bias in GANs<br>pdf: <a href="https://t.co/Q7p4wKdgM7">https://t.co/Q7p4wKdgM7</a><br>abs: <a href="https://t.co/RiKBDJzD7c">https://t.co/RiKBDJzD7c</a><br>project page: <a href="https://t.co/Q55c7mIcqQ">https://t.co/Q55c7mIcqQ</a> <a href="https://t.co/NgHp5yWTJa">pic.twitter.com/NgHp5yWTJa</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1336850905635954689?ref_src=twsrc%5Etfw">December 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Gauge equivariant neural networks for quantum lattice gauge theories

Di Luo, Giuseppe Carleo, Bryan K. Clark, James Stokes

- retweets: 383, favorites: 81 (12/11/2020 09:48:06)

- links: [abs](https://arxiv.org/abs/2012.05232) | [pdf](https://arxiv.org/pdf/2012.05232)
- [cond-mat.str-el](https://arxiv.org/list/cond-mat.str-el/recent) | [cond-mat.dis-nn](https://arxiv.org/list/cond-mat.dis-nn/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [hep-lat](https://arxiv.org/list/hep-lat/recent) | [quant-ph](https://arxiv.org/list/quant-ph/recent)

Gauge symmetries play a key role in physics appearing in areas such as quantum field theories of the fundamental particles and emergent degrees of freedom in quantum materials. Motivated by the desire to efficiently simulate many-body quantum systems with exact local gauge invariance, gauge equivariant neural-network quantum states are introduced, which exactly satisfy the local Hilbert space constraints necessary for the description of quantum lattice gauge theory with Zd gauge group on different geometries. Focusing on the special case of Z2 gauge group on a periodically identified square lattice, the equivariant architecture is analytically shown to contain the loop-gas solution as a special case. Gauge equivariant neural-network quantum states are used in combination with variational quantum Monte Carlo to obtain compact descriptions of the ground state wavefunction for the Z2 theory away from the exactly solvable limit, and to demonstrate the confining/deconfining phase transition of the Wilson loop order parameter.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Check this out! We construct gauge equivariant neural quantum states to simulate lattice gauge theories, powered by the amazing Di Luo, J Stokes, B Clark, <a href="https://twitter.com/EPFL_en?ref_src=twsrc%5Etfw">@EPFL_en</a>  <a href="https://twitter.com/FlatironCCQ?ref_src=twsrc%5Etfw">@FlatironCCQ</a> <a href="https://twitter.com/Illinois_Alma?ref_src=twsrc%5Etfw">@Illinois_Alma</a> (inspired by great work from <a href="https://twitter.com/KyleCranmer?ref_src=twsrc%5Etfw">@KyleCranmer</a> &amp; co in hep) <a href="https://t.co/q4gFGvt081">https://t.co/q4gFGvt081</a></p>&mdash; Giuseppe Carleo (@gppcarleo) <a href="https://twitter.com/gppcarleo/status/1336924062539534342?ref_src=twsrc%5Etfw">December 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. SongMASS: Automatic Song Writing with Pre-training and Alignment  Constraint

Zhonghao Sheng, Kaitao Song, Xu Tan, Yi Ren, Wei Ye, Shikun Zhang, Tao Qin

- retweets: 169, favorites: 70 (12/11/2020 09:48:07)

- links: [abs](https://arxiv.org/abs/2012.05168) | [pdf](https://arxiv.org/pdf/2012.05168)
- [cs.SD](https://arxiv.org/list/cs.SD/recent) | [cs.CL](https://arxiv.org/list/cs.CL/recent) | [eess.AS](https://arxiv.org/list/eess.AS/recent)

Automatic song writing aims to compose a song (lyric and/or melody) by machine, which is an interesting topic in both academia and industry. In automatic song writing, lyric-to-melody generation and melody-to-lyric generation are two important tasks, both of which usually suffer from the following challenges: 1) the paired lyric and melody data are limited, which affects the generation quality of the two tasks, considering a lot of paired training data are needed due to the weak correlation between lyric and melody; 2) Strict alignments are required between lyric and melody, which relies on specific alignment modeling. In this paper, we propose SongMASS to address the above challenges, which leverages masked sequence to sequence (MASS) pre-training and attention based alignment modeling for lyric-to-melody and melody-to-lyric generation. Specifically, 1) we extend the original sentence-level MASS pre-training to song level to better capture long contextual information in music, and use a separate encoder and decoder for each modality (lyric or melody); 2) we leverage sentence-level attention mask and token-level attention constraint during training to enhance the alignment between lyric and melody. During inference, we use a dynamic programming strategy to obtain the alignment between each word/syllable in lyric and note in melody. We pre-train SongMASS on unpaired lyric and melody datasets, and both objective and subjective evaluations demonstrate that SongMASS generates lyric and melody with significantly better quality than the baseline method without pre-training or alignment constraint.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">SongMASS: Automatic Song Writing with Pre-training and Alignment Constraint<br>pdf: <a href="https://t.co/sGqifnrVty">https://t.co/sGqifnrVty</a><br>abs: <a href="https://t.co/qktAvRfHkN">https://t.co/qktAvRfHkN</a><br>project page: <a href="https://t.co/FhUYBJjQJR">https://t.co/FhUYBJjQJR</a> <a href="https://t.co/o2SxQn7iB4">pic.twitter.com/o2SxQn7iB4</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1336876998115938306?ref_src=twsrc%5Etfw">December 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. You Only Need Adversarial Supervision for Semantic Image Synthesis

Vadim Sushko, Edgar SchÃ¶nfeld, Dan Zhang, Juergen Gall, Bernt Schiele, Anna Khoreva

- retweets: 57, favorites: 52 (12/11/2020 09:48:07)

- links: [abs](https://arxiv.org/abs/2012.04781) | [pdf](https://arxiv.org/pdf/2012.04781)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [eess.IV](https://arxiv.org/list/eess.IV/recent)

Despite their recent successes, GAN models for semantic image synthesis still suffer from poor image quality when trained with only adversarial supervision. Historically, additionally employing the VGG-based perceptual loss has helped to overcome this issue, significantly improving the synthesis quality, but at the same time limiting the progress of GAN models for semantic image synthesis. In this work, we propose a novel, simplified GAN model, which needs only adversarial supervision to achieve high quality results. We re-design the discriminator as a semantic segmentation network, directly using the given semantic label maps as the ground truth for training. By providing stronger supervision to the discriminator as well as to the generator through spatially- and semantically-aware discriminator feedback, we are able to synthesize images of higher fidelity with better alignment to their input label maps, making the use of the perceptual loss superfluous. Moreover, we enable high-quality multi-modal image synthesis through global and local sampling of a 3D noise tensor injected into the generator, which allows complete or partial image change. We show that images synthesized by our model are more diverse and follow the color and texture distributions of real images more closely. We achieve an average improvement of $6$ FID and $5$ mIoU points over the state of the art across different datasets using only adversarial supervision.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">You Only Need Adversarial Supervision for Semantic Image Synthesis<br>pdf: <a href="https://t.co/5WbZzMxzGC">https://t.co/5WbZzMxzGC</a><br>abs: <a href="https://t.co/q9yJV34TNZ">https://t.co/q9yJV34TNZ</a> <a href="https://t.co/j8fDCkXCNv">pic.twitter.com/j8fDCkXCNv</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1336855907096031233?ref_src=twsrc%5Etfw">December 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. Counting methods introduced into the bibliometric research literature  1970-2018: A review

Marianne Gauffriau

- retweets: 56, favorites: 29 (12/11/2020 09:48:07)

- links: [abs](https://arxiv.org/abs/2012.04986) | [pdf](https://arxiv.org/pdf/2012.04986)
- [cs.DL](https://arxiv.org/list/cs.DL/recent)

The present review of bibliometric counting methods investigates 1) the number of unique counting methods in the bibliometric research literature, 2) to what extent the counting methods can be categorized according to selected characteristics of the counting methods, 3) methods and elements to assess the internal validity of the counting methods, and 4) to what extent and with which characteristics the counting methods are used in research evaluations.   The review identifies 32 counting methods introduced during the period 1981 - 2018. Two frameworks categorize these counting methods. Framework 1 describes selected mathematical properties of counting methods, and Framework 2 describes arguments for choosing a counting method. Twenty of the 32 counting methods are rank-dependent, fractionalized, and introduced to measure contribution, participation, etc. of an object of study. Next, three criteria for internal validity are used to identify five methods that test the adequacy of counting methods, two elements that test sensitivity, and three elements that test homogeneity of the counting methods. These methods and elements may be used to assess the internal validity of counting methods. Finally, a literature search finds research evaluations that use the counting methods. Only three of the 32 counting methods are used by four research evaluations or more. Of these three counting methods, two are used with the same characteristics as defined in the studies that introduced the counting methods.   The review provides practitioners in research evaluation and researchers in bibliometrics with a detailed foundation for working with counting methods. At the same time, many of the findings in the review provide bases for future investigations of counting methods.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">proudly present a new preprint covering 32 bibliometric counting methods introduced into the bibliometric research literature in the period 1970-2018: <a href="https://t.co/76FJ4eOmYF">https://t.co/76FJ4eOmYF</a></p>&mdash; Marianne Gauffriau (@MGauffriau) <a href="https://twitter.com/MGauffriau/status/1336975494269513728?ref_src=twsrc%5Etfw">December 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. Canonical Capsules: Unsupervised Capsules in Canonical Pose

Weiwei Sun, Andrea Tagliasacchi, Boyang Deng, Sara Sabour, Soroosh Yazdani, Geoffrey Hinton, Kwang Moo Yi

- retweets: 20, favorites: 37 (12/11/2020 09:48:07)

- links: [abs](https://arxiv.org/abs/2012.04718) | [pdf](https://arxiv.org/pdf/2012.04718)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

We propose an unsupervised capsule architecture for 3D point clouds. We compute capsule decompositions of objects through permutation-equivariant attention, and self-supervise the process by training with pairs of randomly rotated objects. Our key idea is to aggregate the attention masks into semantic keypoints, and use these to supervise a decomposition that satisfies the capsule invariance/equivariance properties. This not only enables the training of a semantically consistent decomposition, but also allows us to learn a canonicalization operation that enables object-centric reasoning. In doing so, we require neither classification labels nor manually-aligned training datasets to train. Yet, by learning an object-centric representation in an unsupervised manner, our method outperforms the state-of-the-art on 3D point cloud reconstruction, registration, and unsupervised classification. We will release the code and dataset to reproduce our results as soon as the paper is published.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Canonical Capsules: Unsupervised Capsules in Canonical Pose<br>pdf: <a href="https://t.co/iCSruvT5fd">https://t.co/iCSruvT5fd</a><br>abs: <a href="https://t.co/Tf2JdkmOqF">https://t.co/Tf2JdkmOqF</a> <a href="https://t.co/DwcLRfB2H5">pic.twitter.com/DwcLRfB2H5</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1336883611044343811?ref_src=twsrc%5Etfw">December 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



