---
title: Hot Papers 2021-04-06
date: 2021-04-07T10:15:27.Z
template: "post"
draft: false
slug: "hot-papers-2021-04-06"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2021-04-06"
socialImage: "/media/flying-marine.jpg"

---

# 1. AST: Audio Spectrogram Transformer

Yuan Gong, Yu-An Chung, James Glass

- retweets: 2570, favorites: 333 (04/07/2021 10:15:27)

- links: [abs](https://arxiv.org/abs/2104.01778) | [pdf](https://arxiv.org/pdf/2104.01778)
- [cs.SD](https://arxiv.org/list/cs.SD/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

In the past decade, convolutional neural networks (CNNs) have been widely adopted as the main building block for end-to-end audio classification models, which aim to learn a direct mapping from audio spectrograms to corresponding labels. To better capture long-range global context, a recent trend is to add a self-attention mechanism on top of the CNN, forming a CNN-attention hybrid model. However, it is unclear whether the reliance on a CNN is necessary, and if neural networks purely based on attention are sufficient to obtain good performance in audio classification. In this paper, we answer the question by introducing the Audio Spectrogram Transformer (AST), the first convolution-free, purely attention-based model for audio classification. We evaluate AST on various audio classification benchmarks, where it achieves new state-of-the-art results of 0.485 mAP on AudioSet, 95.6% accuracy on ESC-50, and 98.1% accuracy on Speech Commands V2.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">AST: Audio Spectrogram Transformer<br><br>Proposes Audio Spectrogram Transformer, the first purely attention-based model for audio classification that achieves the new SotA on various benchmarks.<a href="https://t.co/LbW5zb5hDM">https://t.co/LbW5zb5hDM</a> <a href="https://t.co/sZiqyamFLY">pic.twitter.com/sZiqyamFLY</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1379238772961812481?ref_src=twsrc%5Etfw">April 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">AST: Audio Spectrogram Transformer<br>pdf: <a href="https://t.co/mbtNQap583">https://t.co/mbtNQap583</a><br>abs: <a href="https://t.co/5PhyMokTkS">https://t.co/5PhyMokTkS</a> <a href="https://t.co/hC3qr4uxQt">pic.twitter.com/hC3qr4uxQt</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1379237749471993856?ref_src=twsrc%5Etfw">April 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. An Empirical Study of Training Self-Supervised Visual Transformers

Xinlei Chen, Saining Xie, Kaiming He

- retweets: 563, favorites: 141 (04/07/2021 10:15:27)

- links: [abs](https://arxiv.org/abs/2104.02057) | [pdf](https://arxiv.org/pdf/2104.02057)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

This paper does not describe a novel method. Instead, it studies a straightforward, incremental, yet must-know baseline given the recent progress in computer vision: self-supervised learning for Visual Transformers (ViT). While the training recipes for standard convolutional networks have been highly mature and robust, the recipes for ViT are yet to be built, especially in the self-supervised scenarios where training becomes more challenging. In this work, we go back to basics and investigate the effects of several fundamental components for training self-supervised ViT. We observe that instability is a major issue that degrades accuracy, and it can be hidden by apparently good results. We reveal that these results are indeed partial failure, and they can be improved when training is made more stable. We benchmark ViT results in MoCo v3 and several other self-supervised frameworks, with ablations in various aspects. We discuss the currently positive evidence as well as challenges and open questions. We hope that this work will provide useful data points and experience for future research.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">An Empirical Study of Training Self-Supervised Visual Transformers<br><br>Observes that self-supervised ViT has a major instability issue that degrades accuracy, and this can be improved when training is made more stable.<a href="https://t.co/4LKsEjYEes">https://t.co/4LKsEjYEes</a> <a href="https://t.co/fZ4u0m5TLT">pic.twitter.com/fZ4u0m5TLT</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1379237453488222208?ref_src=twsrc%5Etfw">April 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">An Empirical Study of Training Self-Supervised Visual Transformers<br>pdf: <a href="https://t.co/vDLciuid1W">https://t.co/vDLciuid1W</a><br>abs: <a href="https://t.co/DaXNSi5MDZ">https://t.co/DaXNSi5MDZ</a> <a href="https://t.co/7Y5dqBF24Q">pic.twitter.com/7Y5dqBF24Q</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1379237436115533825?ref_src=twsrc%5Etfw">April 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Removing Pixel Noises and Spatial Artifacts with Generative Diversity  Denoising Methods

Mangal Prakash, Mauricio Delbracio, Peyman Milanfar, Florian Jug

- retweets: 522, favorites: 150 (04/07/2021 10:15:28)

- links: [abs](https://arxiv.org/abs/2104.01374) | [pdf](https://arxiv.org/pdf/2104.01374)
- [eess.IV](https://arxiv.org/list/eess.IV/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [q-bio.QM](https://arxiv.org/list/q-bio.QM/recent)

Image denoising and artefact removal are complex inverse problems admitting many potential solutions. Variational Autoencoders (VAEs) can be used to learn a whole distribution of sensible solutions, from which one can sample efficiently. However, such a generative approach to image restoration is only studied in the context of pixel-wise noise removal (e.g. Poisson or Gaussian noise). While important, a plethora of application domains suffer from imaging artefacts (structured noises) that alter groups of pixels in correlated ways. In this work we show, for the first time, that generative diversity denoising (GDD) approaches can learn to remove structured noises without supervision. To this end, we investigate two existing GDD architectures, introduce a new one based on hierarchical VAEs, and compare their performances against a total of seven state-of-the-art baseline methods on five sources of structured noise (including tomography reconstruction artefacts and microscopy artefacts). We find that GDD methods outperform all unsupervised baselines and in many cases not lagging far behind supervised results (in some occasions even superseding them). In addition to structured noise removal, we also show that our new GDD method produces new state-of-the-art (SOTA) results on seven out of eight benchmark datasets for pixel-noise removal. Finally, we offer insights into the daunting question of how GDD methods distinguish structured noise, which we like to see removed, from image signals, which we want to see retained.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">In <a href="https://twitter.com/Mangal_Prakash_?ref_src=twsrc%5Etfw">@Mangal_Prakash_</a>’s latest work, we teamed up with <a href="https://twitter.com/docmilanfar?ref_src=twsrc%5Etfw">@docmilanfar</a> and <a href="https://twitter.com/2ptmvd?ref_src=twsrc%5Etfw">@2ptmvd</a> and show how Generative Diversity Denoising methods can not only lead to SOTA denoising results, but also used to remove spatial artifacts (e.g. in tomography)!<a href="https://twitter.com/hashtag/HDivNoising?src=hash&amp;ref_src=twsrc%5Etfw">#HDivNoising</a><a href="https://t.co/adZfKHWhxP">https://t.co/adZfKHWhxP</a> <a href="https://t.co/fWcnzWE2bM">pic.twitter.com/fWcnzWE2bM</a></p>&mdash; Florian Jug (@florianjug) <a href="https://twitter.com/florianjug/status/1379328553637060609?ref_src=twsrc%5Etfw">April 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Removing Pixel Noises and Spatial Artifacts with Generative Diversity Denoising Methods<br>pdf: <a href="https://t.co/75D9hhAe4f">https://t.co/75D9hhAe4f</a><br>abs: <a href="https://t.co/lWROxgm0c9">https://t.co/lWROxgm0c9</a> <a href="https://t.co/gVs6cTRvDJ">pic.twitter.com/gVs6cTRvDJ</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1379241259877986305?ref_src=twsrc%5Etfw">April 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Efficient DETR: Improving End-to-End Object Detector with Dense Prior

Zhuyu Yao, Jiangbo Ai, Boxun Li, Chi Zhang

- retweets: 419, favorites: 166 (04/07/2021 10:15:28)

- links: [abs](https://arxiv.org/abs/2104.01318) | [pdf](https://arxiv.org/pdf/2104.01318)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

The recently proposed end-to-end transformer detectors, such as DETR and Deformable DETR, have a cascade structure of stacking 6 decoder layers to update object queries iteratively, without which their performance degrades seriously. In this paper, we investigate that the random initialization of object containers, which include object queries and reference points, is mainly responsible for the requirement of multiple iterations. Based on our findings, we propose Efficient DETR, a simple and efficient pipeline for end-to-end object detection. By taking advantage of both dense detection and sparse set detection, Efficient DETR leverages dense prior to initialize the object containers and brings the gap of the 1-decoder structure and 6-decoder structure. Experiments conducted on MS COCO show that our method, with only 3 encoder layers and 1 decoder layer, achieves competitive performance with state-of-the-art object detection methods. Efficient DETR is also robust in crowded scenes. It outperforms modern detectors on CrowdHuman dataset by a large margin.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Efficient DETR: Improving End-to-End Object Detector with Dense Prior<br>pdf: <a href="https://t.co/vC9MiooCnz">https://t.co/vC9MiooCnz</a><br>abs: <a href="https://t.co/WGgBSihQM5">https://t.co/WGgBSihQM5</a> <a href="https://t.co/h27NpI2crA">pic.twitter.com/h27NpI2crA</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1379236594142621696?ref_src=twsrc%5Etfw">April 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Efficient DETR: Improving End-to-End Object Detector with Dense Prior<a href="https://t.co/CCRybhms9W">https://t.co/CCRybhms9W</a> <a href="https://t.co/0M9MQrjBKH">pic.twitter.com/0M9MQrjBKH</a></p>&mdash; phalanx (@ZFPhalanx) <a href="https://twitter.com/ZFPhalanx/status/1379309471428571136?ref_src=twsrc%5Etfw">April 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Convolutional Neural Opacity Radiance Fields

Haimin Luo, Anpei Chen, Qixuan Zhang, Bai Pang, Minye Wu, Lan Xu, Jingyi Yu

- retweets: 380, favorites: 127 (04/07/2021 10:15:28)

- links: [abs](https://arxiv.org/abs/2104.01772) | [pdf](https://arxiv.org/pdf/2104.01772)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Photo-realistic modeling and rendering of fuzzy objects with complex opacity are critical for numerous immersive VR/AR applications, but it suffers from strong view-dependent brightness, color. In this paper, we propose a novel scheme to generate opacity radiance fields with a convolutional neural renderer for fuzzy objects, which is the first to combine both explicit opacity supervision and convolutional mechanism into the neural radiance field framework so as to enable high-quality appearance and global consistent alpha mattes generation in arbitrary novel views. More specifically, we propose an efficient sampling strategy along with both the camera rays and image plane, which enables efficient radiance field sampling and learning in a patch-wise manner, as well as a novel volumetric feature integration scheme that generates per-patch hybrid feature embeddings to reconstruct the view-consistent fine-detailed appearance and opacity output. We further adopt a patch-wise adversarial training scheme to preserve both high-frequency appearance and opacity details in a self-supervised framework. We also introduce an effective multi-view image capture system to capture high-quality color and alpha maps for challenging fuzzy objects. Extensive experiments on existing and our new challenging fuzzy object dataset demonstrate that our method achieves photo-realistic, globally consistent, and fined detailed appearance and opacity free-viewpoint rendering for various fuzzy objects.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Convolutional Neural Opacity Radiance Fields<br>pdf: <a href="https://t.co/TF69E8Yfde">https://t.co/TF69E8Yfde</a><br>abs: <a href="https://t.co/848c7VHVNA">https://t.co/848c7VHVNA</a> <a href="https://t.co/rNvBY6LkC9">pic.twitter.com/rNvBY6LkC9</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1379242937704402944?ref_src=twsrc%5Etfw">April 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. Generating Furry Cars: Disentangling Object Shape & Appearance across  Multiple Domains

Utkarsh Ojha, Krishna Kumar Singh, Yong Jae Lee

- retweets: 275, favorites: 113 (04/07/2021 10:15:28)

- links: [abs](https://arxiv.org/abs/2104.02052) | [pdf](https://arxiv.org/pdf/2104.02052)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.GR](https://arxiv.org/list/cs.GR/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

We consider the novel task of learning disentangled representations of object shape and appearance across multiple domains (e.g., dogs and cars). The goal is to learn a generative model that learns an intermediate distribution, which borrows a subset of properties from each domain, enabling the generation of images that did not exist in any domain exclusively. This challenging problem requires an accurate disentanglement of object shape, appearance, and background from each domain, so that the appearance and shape factors from the two domains can be interchanged. We augment an existing approach that can disentangle factors within a single domain but struggles to do so across domains. Our key technical contribution is to represent object appearance with a differentiable histogram of visual features, and to optimize the generator so that two images with the same latent appearance factor but different latent shape factors produce similar histograms. On multiple multi-domain datasets, we demonstrate our method leads to accurate and consistent appearance and shape transfer across domains.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Generating Furry Cars: Disentangling Object Shape &amp; Appearance across Multiple Domains<br>pdf: <a href="https://t.co/M6vcCiAWuP">https://t.co/M6vcCiAWuP</a><br>abs: <a href="https://t.co/T7U1PbIYE0">https://t.co/T7U1PbIYE0</a><br>project page: <a href="https://t.co/k3DGXXbm5U">https://t.co/k3DGXXbm5U</a> <a href="https://t.co/p8JZRPmjC6">pic.twitter.com/p8JZRPmjC6</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1379258639404441605?ref_src=twsrc%5Etfw">April 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr"><a href="https://t.co/H9SI3iteL9">https://t.co/H9SI3iteL9</a><br><br>全く異なるドメイン同士の形状と表面模様を組み合わせた新しい画像を生成できるドメイン変換の提案論文<br>異なるドメインの形状と表面模様を独立して制御して融合させた画像が出力できる。<br>従来モデルではできない毛の生えた車などの複雑な生成制御が可能<br>(1/3) <a href="https://t.co/QRixt5hrnv">pic.twitter.com/QRixt5hrnv</a></p>&mdash; Hカップの論文を読むタイプのMLエンジニアver0.02 (@SupervisedAi) <a href="https://twitter.com/SupervisedAi/status/1379473787410784256?ref_src=twsrc%5Etfw">April 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. Hierarchical Pyramid Representations for Semantic Segmentation

Hiroaki Aizawa, Yukihiro Domae, Kunihito Kato

- retweets: 165, favorites: 101 (04/07/2021 10:15:28)

- links: [abs](https://arxiv.org/abs/2104.01792) | [pdf](https://arxiv.org/pdf/2104.01792)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Understanding the context of complex and cluttered scenes is a challenging problem for semantic segmentation. However, it is difficult to model the context without prior and additional supervision because the scene's factors, such as the scale, shape, and appearance of objects, vary considerably in these scenes. To solve this, we propose to learn the structures of objects and the hierarchy among objects because context is based on these intrinsic properties. In this study, we design novel hierarchical, contextual, and multiscale pyramidal representations to capture the properties from an input image. Our key idea is the recursive segmentation in different hierarchical regions based on a predefined number of regions and the aggregation of the context in these regions. The aggregated contexts are used to predict the contextual relationship between the regions and partition the regions in the following hierarchical level. Finally, by constructing the pyramid representations from the recursively aggregated context, multiscale and hierarchical properties are attained. In the experiments, we confirmed that our proposed method achieves state-of-the-art performance in PASCAL Context.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">arxivに論文を公開しました！<br>階層的に文脈を収集する方法を提案しました<br>abs: <a href="https://t.co/GnKO4yVO2Y">https://t.co/GnKO4yVO2Y</a><br>project page: <a href="https://t.co/pMWtUXnIGy">https://t.co/pMWtUXnIGy</a> <a href="https://t.co/89YjHCKfUC">pic.twitter.com/89YjHCKfUC</a></p>&mdash; hiroaki aizawa (@aizw_h) <a href="https://twitter.com/aizw_h/status/1379254532513832968?ref_src=twsrc%5Etfw">April 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ca" dir="ltr">Hierarchical Pyramid Representations for Semantic Segmentation<br>pdf: <a href="https://t.co/d9yQAXbrPm">https://t.co/d9yQAXbrPm</a><br>abs: <a href="https://t.co/c1oPrie1BC">https://t.co/c1oPrie1BC</a> <a href="https://t.co/whdIBbiSZt">pic.twitter.com/whdIBbiSZt</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1379252950250483718?ref_src=twsrc%5Etfw">April 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. Timers and Such: A Practical Benchmark for Spoken Language Understanding  with Numbers

Loren Lugosch, Piyush Papreja, Mirco Ravanelli, Abdelwahab Heba, Titouan Parcollet

- retweets: 126, favorites: 60 (04/07/2021 10:15:29)

- links: [abs](https://arxiv.org/abs/2104.01604) | [pdf](https://arxiv.org/pdf/2104.01604)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [eess.AS](https://arxiv.org/list/eess.AS/recent)

This paper introduces Timers and Such, a new open source dataset of spoken English commands for common voice control use cases involving numbers. We describe the gap in existing spoken language understanding datasets that Timers and Such fills, the design and creation of the dataset, and experiments with a number of ASR-based and end-to-end baseline models, the code for which has been made available as part of the SpeechBrain toolkit.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Timers and Such! New speech dataset for timers, alarms, unit conversion, and math.<br><br>Use it to test your speech models or to train your own offline voice assistant.<br><br>Paper: <a href="https://t.co/1Itx4KKVyo">https://t.co/1Itx4KKVyo</a><br>Code: <a href="https://t.co/WxJctrnXXI">https://t.co/WxJctrnXXI</a><br>Pretrained model <a href="https://twitter.com/huggingface?ref_src=twsrc%5Etfw">@huggingface</a>: <a href="https://t.co/arrKjGLPJY">https://t.co/arrKjGLPJY</a></p>&mdash; Loren Lugosch (@lorenlugosch) <a href="https://twitter.com/lorenlugosch/status/1379403768194797568?ref_src=twsrc%5Etfw">April 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. Efficient Transformers in Reinforcement Learning using Actor-Learner  Distillation

Emilio Parisotto, Ruslan Salakhutdinov

- retweets: 81, favorites: 45 (04/07/2021 10:15:29)

- links: [abs](https://arxiv.org/abs/2104.01655) | [pdf](https://arxiv.org/pdf/2104.01655)
- [cs.LG](https://arxiv.org/list/cs.LG/recent)

Many real-world applications such as robotics provide hard constraints on power and compute that limit the viable model complexity of Reinforcement Learning (RL) agents. Similarly, in many distributed RL settings, acting is done on un-accelerated hardware such as CPUs, which likewise restricts model size to prevent intractable experiment run times. These "actor-latency" constrained settings present a major obstruction to the scaling up of model complexity that has recently been extremely successful in supervised learning. To be able to utilize large model capacity while still operating within the limits imposed by the system during acting, we develop an "Actor-Learner Distillation" (ALD) procedure that leverages a continual form of distillation that transfers learning progress from a large capacity learner model to a small capacity actor model. As a case study, we develop this procedure in the context of partially-observable environments, where transformer models have had large improvements over LSTMs recently, at the cost of significantly higher computational complexity. With transformer models as the learner and LSTMs as the actor, we demonstrate in several challenging memory environments that using Actor-Learner Distillation recovers the clear sample-efficiency gains of the transformer learner model while maintaining the fast inference and reduced total training time of the LSTM actor model.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Efficient Transformers in Reinforcement Learning using Actor-Learner Distillation<br>pdf: <a href="https://t.co/ncLWJgbf6d">https://t.co/ncLWJgbf6d</a><br>abs: <a href="https://t.co/ozkZ6WgNRl">https://t.co/ozkZ6WgNRl</a> <a href="https://t.co/N2bJv4kd6j">pic.twitter.com/N2bJv4kd6j</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1379275609428348930?ref_src=twsrc%5Etfw">April 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 10. SPGISpeech: 5,000 hours of transcribed financial audio for fully  formatted end-to-end speech recognition

Patrick K. O'Neill, Vitaly Lavrukhin, Somshubra Majumdar, Vahid Noroozi, Yuekai Zhang, Oleksii Kuchaiev, Jagadeesh Balam, Yuliya Dovzhenko, Keenan Freyberg, Michael D. Shulman, Boris Ginsburg, Shinji Watanabe, Georg Kucsko

- retweets: 81, favorites: 29 (04/07/2021 10:15:29)

- links: [abs](https://arxiv.org/abs/2104.02014) | [pdf](https://arxiv.org/pdf/2104.02014)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [eess.AS](https://arxiv.org/list/eess.AS/recent)

In the English speech-to-text (STT) machine learning task, acoustic models are conventionally trained on uncased Latin characters, and any necessary orthography (such as capitalization, punctuation, and denormalization of non-standard words) is imputed by separate post-processing models. This adds complexity and limits performance, as many formatting tasks benefit from semantic information present in the acoustic signal but absent in transcription. Here we propose a new STT task: end-to-end neural transcription with fully formatted text for target labels. We present baseline Conformer-based models trained on a corpus of 5,000 hours of professionally transcribed earnings calls, achieving a CER of 1.7. As a contribution to the STT research community, we release the corpus free for non-commercial use at https://datasets.kensho.com/datasets/scribe.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">SPGISpeech: 5,000 hours of transcribed financial audio for fully formatted end-to-end speech recognition<br>pdf: <a href="https://t.co/wcecmEyIpf">https://t.co/wcecmEyIpf</a><br>abs: <a href="https://t.co/Im6u55i8dx">https://t.co/Im6u55i8dx</a><br>project page: <a href="https://t.co/r8giPxG440">https://t.co/r8giPxG440</a> <a href="https://t.co/sUtGFAoKYg">pic.twitter.com/sUtGFAoKYg</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1379256763321614338?ref_src=twsrc%5Etfw">April 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 11. New Benchmarks for Learning on Non-Homophilous Graphs

Derek Lim, Xiuyu Li, Felix Hohne, Ser-Nam Lim

- retweets: 72, favorites: 16 (04/07/2021 10:15:29)

- links: [abs](https://arxiv.org/abs/2104.01404) | [pdf](https://arxiv.org/pdf/2104.01404)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.SI](https://arxiv.org/list/cs.SI/recent)

Much data with graph structures satisfy the principle of homophily, meaning that connected nodes tend to be similar with respect to a specific attribute. As such, ubiquitous datasets for graph machine learning tasks have generally been highly homophilous, rewarding methods that leverage homophily as an inductive bias. Recent work has pointed out this particular focus, as new non-homophilous datasets have been introduced and graph representation learning models better suited for low-homophily settings have been developed. However, these datasets are small and poorly suited to truly testing the effectiveness of new methods in non-homophilous settings. We present a series of improved graph datasets with node label relationships that do not satisfy the homophily principle. Along with this, we introduce a new measure of the presence or absence of homophily that is better suited than existing measures in different regimes. We benchmark a range of simple methods and graph neural networks across our proposed datasets, drawing new insights for further research. Data and codes can be found at https://github.com/CUAI/Non-Homophily-Benchmarks.




# 12. Tukey Depths and Hamilton-Jacobi Differential Equations

Martin Molina-Fructuoso, Ryan Murray

- retweets: 33, favorites: 46 (04/07/2021 10:15:29)

- links: [abs](https://arxiv.org/abs/2104.01648) | [pdf](https://arxiv.org/pdf/2104.01648)
- [math.ST](https://arxiv.org/list/math.ST/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [math.AP](https://arxiv.org/list/math.AP/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

The widespread application of modern machine learning has increased the need for robust statistical algorithms. This work studies one such fundamental statistical measure known as the Tukey depth. We study the problem in the continuum (population) limit. In particular, we derive the associated necessary conditions, which take the form of a first-order partial differential equation. We discuss the classical interpretation of this necessary condition as the viscosity solution of a Hamilton-Jacobi equation, but with a non-classical Hamiltonian with discontinuous dependence on the gradient at zero. We prove that this equation possesses a unique viscosity solution and that this solution always bounds the Tukey depth from below. In certain cases, we prove that the Tukey depth is equal to the viscosity solution, and we give some illustrations of standard numerical methods from the optimal control community which deal directly with the partial differential equation. We conclude by outlining several promising research directions both in terms of new numerical algorithms and theoretical challenges.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">an unexpected connection: <a href="https://t.co/KcFqqFRQ2D">https://t.co/KcFqqFRQ2D</a> shows that the Tukey depth (related to the Tukey median, a generalisation of the median to d &gt; 1 which retains good statistical + robustness properties) of a distribution can be characterised in terms of a Hamilton-Jacobi PDE!</p>&mdash; Sam Power (@sam_power_825) <a href="https://twitter.com/sam_power_825/status/1379327701274800133?ref_src=twsrc%5Etfw">April 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 13. Deep Learning of Conjugate Mappings

Jason J. Bramburger, Steven L. Brunton, J. Nathan Kutz

- retweets: 8, favorites: 44 (04/07/2021 10:15:29)

- links: [abs](https://arxiv.org/abs/2104.01874) | [pdf](https://arxiv.org/pdf/2104.01874)
- [math.DS](https://arxiv.org/list/math.DS/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Despite many of the most common chaotic dynamical systems being continuous in time, it is through discrete time mappings that much of the understanding of chaos is formed. Henri Poincar\'e first made this connection by tracking consecutive iterations of the continuous flow with a lower-dimensional, transverse subspace. The mapping that iterates the dynamics through consecutive intersections of the flow with the subspace is now referred to as a Poincar\'e map, and it is the primary method available for interpreting and classifying chaotic dynamics. Unfortunately, in all but the simplest systems, an explicit form for such a mapping remains outstanding. This work proposes a method for obtaining explicit Poincar\'e mappings by using deep learning to construct an invertible coordinate transformation into a conjugate representation where the dynamics are governed by a relatively simple chaotic mapping. The invertible change of variable is based on an autoencoder, which allows for dimensionality reduction, and has the advantage of classifying chaotic systems using the equivalence relation of topological conjugacies. Indeed, the enforcement of topological conjugacies is the critical neural network regularization for learning the coordinate and dynamics pairing. We provide expository applications of the method to low-dimensional systems such as the R\"ossler and Lorenz systems, while also demonstrating the utility of the method on infinite-dimensional systems, such as the Kuramoto--Sivashinsky equation.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Excited to share some new work with <a href="https://twitter.com/eigensteve?ref_src=twsrc%5Etfw">@eigensteve</a> and J.N. Kutz. We leverage deep learning to discover low-dimensional Poincare mappings that govern chaotic systems. We even applied it to the Kuramoto-Sivashinsky PDE!<a href="https://t.co/SQOLRtM7rc">https://t.co/SQOLRtM7rc</a></p>&mdash; Jason J. Bramburger (@jbramburger7) <a href="https://twitter.com/jbramburger7/status/1379445515956355077?ref_src=twsrc%5Etfw">April 6, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



