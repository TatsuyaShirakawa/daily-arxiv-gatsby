---
title: Hot Papers 2020-09-10
date: 2020-09-11T07:34:53.Z
template: "post"
draft: false
slug: "hot-papers-2020-09-10"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-09-10"
socialImage: "/media/flying-marine.jpg"

---

# 1. Fiber Bundle Codes: Breaking the $N^{1/2} \operatorname{polylog}(N)$  Barrier for Quantum LDPC Codes

Matthew B. Hastings, Jeongwan Haah, Ryan O'Donnell

- retweets: 24, favorites: 150 (09/11/2020 07:34:53)

- links: [abs](https://arxiv.org/abs/2009.03921) | [pdf](https://arxiv.org/pdf/2009.03921)
- [quant-ph](https://arxiv.org/list/quant-ph/recent) | [cs.IT](https://arxiv.org/list/cs.IT/recent) | [math.CO](https://arxiv.org/list/math.CO/recent)

We present a quantum LDPC code family that has distance $\Omega(N^{3/5}/\operatorname{polylog}(N))$ and $\tilde\Theta(N^{3/5})$ logical qubits. This is the first quantum LDPC code construction which achieves distance greater than $N^{1/2} \operatorname{polylog}(N)$. The construction is based on generalizing the homological product of codes to a fiber bundle.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">New record for quantum LDPC codes:<a href="https://t.co/7Oc2KANPYd">https://t.co/7Oc2KANPYd</a></p>&mdash; Isaac Kim (@Isaac__kim) <a href="https://twitter.com/Isaac__kim/status/1303860697273479168?ref_src=twsrc%5Etfw">September 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">LDPC codes with distance linear in n are not hard to find (see Fig 2.4 in Gallager 1963). Surprisingly, no quantum LDPC codes with distance &gt; n^c with c&gt;1/2 were known. Until this: <a href="https://t.co/ElHOjJEaK5">https://t.co/ElHOjJEaK5</a><br><br>Fantastic result by Hastings, Haah and O&#39;Donnell</p>&mdash; Nicolas Delfosse (@nic_delfosse) <a href="https://twitter.com/nic_delfosse/status/1303890662035566592?ref_src=twsrc%5Etfw">September 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Assessing Game Balance with AlphaZero: Exploring Alternative Rule Sets  in Chess

Nenad Tomašev, Ulrich Paquet, Demis Hassabis, Vladimir Kramnik

- retweets: 23, favorites: 120 (09/11/2020 07:34:53)

- links: [abs](https://arxiv.org/abs/2009.04374) | [pdf](https://arxiv.org/pdf/2009.04374)
- [cs.AI](https://arxiv.org/list/cs.AI/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

It is non-trivial to design engaging and balanced sets of game rules. Modern chess has evolved over centuries, but without a similar recourse to history, the consequences of rule changes to game dynamics are difficult to predict. AlphaZero provides an alternative in silico means of game balance assessment. It is a system that can learn near-optimal strategies for any rule set from scratch, without any human supervision, by continually learning from its own experience. In this study we use AlphaZero to creatively explore and design new chess variants. There is growing interest in chess variants like Fischer Random Chess, because of classical chess's voluminous opening theory, the high percentage of draws in professional play, and the non-negligible number of games that end while both players are still in their home preparation. We compare nine other variants that involve atomic changes to the rules of chess. The changes allow for novel strategic and tactical patterns to emerge, while keeping the games close to the original. By learning near-optimal strategies for each variant with AlphaZero, we determine what games between strong human players might look like if these variants were adopted. Qualitatively, several variants are very dynamic. An analytic comparison show that pieces are valued differently between variants, and that some variants are more decisive than classical chess. Our findings demonstrate the rich possibilities that lie beyond the rules of modern chess.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">How would chess look like if there was no stalemate?<br>If you could capture your own pieces?<br>If pawns could move sideways?<br> <br>Much less different than one would think! <br><br>Deepmind taught Alphazero to play with these rules and published a fascinating paper! <a href="https://t.co/hG4skYat2c">https://t.co/hG4skYat2c</a> <a href="https://t.co/vw8SKqEGyL">pic.twitter.com/vw8SKqEGyL</a></p>&mdash; Peter Heine Nielsen (@PHChess) <a href="https://twitter.com/PHChess/status/1303990503998881792?ref_src=twsrc%5Etfw">September 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. The Unbalanced Gromov Wasserstein Distance: Conic Formulation and  Relaxation

Thibault Séjourné, François-Xavier Vialard, Gabriel Peyré

- retweets: 8, favorites: 59 (09/11/2020 07:34:54)

- links: [abs](https://arxiv.org/abs/2009.04266) | [pdf](https://arxiv.org/pdf/2009.04266)
- [math.OC](https://arxiv.org/list/math.OC/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Comparing metric measure spaces (i.e. a metric space endowed with a probability distribution) is at the heart of many machine learning problems. This includes for instance predicting properties of molecules in quantum chemistry or generating graphs with varying connectivity. The most popular distance between such metric measure spaces is the Gromov-Wasserstein (GW) distance, which is the solution of a quadratic assignment problem. This distance has been successfully applied to supervised learning and generative modeling, for applications as diverse as quantum chemistry or natural language processing. The GW distance is however limited to the comparison of metric measure spaces endowed with a \emph{probability} distribution. This strong limitation is problematic for many applications in ML where there is no a priori natural normalization on the total mass of the data. Furthermore, imposing an exact conservation of mass across spaces is not robust to outliers and often leads to irregular matching. To alleviate these issues, we introduce two Unbalanced Gromov-Wasserstein formulations: a distance and a more computationally tractable upper-bounding relaxation. They both allow the comparison of metric spaces equipped with arbitrary positive measures up to isometries.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">In &quot;The Unbalanced Gromov Wasserstein Distance:<br>Conic Formulation and Relaxation&quot; with Thibault and F-X, we explain ways to extend the GW distance to arbitrary positive measures. <a href="https://t.co/S2LJRaTHQH">https://t.co/S2LJRaTHQH</a> <a href="https://t.co/51UZUWer47">pic.twitter.com/51UZUWer47</a></p>&mdash; Gabriel Peyré (@gabrielpeyre) <a href="https://twitter.com/gabrielpeyre/status/1304005460152987648?ref_src=twsrc%5Etfw">September 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. not-so-BigGAN: Generating High-Fidelity Images on a Small Compute Budget

Seungwook Han, Akash Srivastava, Cole Hurwitz, Prasanna Sattigeri, David D. Cox

- retweets: 5, favorites: 46 (09/11/2020 07:34:54)

- links: [abs](https://arxiv.org/abs/2009.04433) | [pdf](https://arxiv.org/pdf/2009.04433)
- [eess.IV](https://arxiv.org/list/eess.IV/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

BigGAN is the state-of-the-art in high-resolution image generation, successfully leveraging advancements in scalable computing and theoretical understanding of generative adversarial methods to set new records in conditional image generation. A major part of BigGAN's success is due to its use of large mini-batch sizes during training in high dimensions. While effective, this technique requires an incredible amount of compute resources and/or time (256 TPU-v3 Cores), putting the model out of reach for the larger research community. In this paper, we present not-so-BigGAN, a simple and scalable framework for training deep generative models on high-dimensional natural images. Instead of modelling the image in pixel space like in BigGAN, not-so-BigGAN uses wavelet transformations to bypass the curse of dimensionality, reducing the overall compute requirement significantly. Through extensive empirical evaluation, we demonstrate that for a fixed compute budget, not-so-BigGAN converges several times faster than BigGAN, reaching competitive image quality with an order of magnitude lower compute budget (4 Telsa-V100 GPUs).

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">not-so-BigGAN: Generating High-Fidelity Images on a Small Compute Budget <a href="https://t.co/qT4ayP8EVz">https://t.co/qT4ayP8EVz</a> not-so-BigGANはWavelet変換により画像の大域情報のみを残す符号化器と，残りの情報を復元しiWTによりアップサンプルする復号器からなる．隠れ空間でGANをする．VQVAEにも近い．V100*4で学習可能． <a href="https://t.co/GQGIRkBgn7">pic.twitter.com/GQGIRkBgn7</a></p>&mdash; ワクワクさん (@mosko_mule) <a href="https://twitter.com/mosko_mule/status/1303958910542336001?ref_src=twsrc%5Etfw">September 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Unconstrained Text Detection in Manga: a New Dataset and Baseline

Julián Del Gobbo, Rosana Matuk Herrera

- retweets: 14, favorites: 36 (09/11/2020 07:34:54)

- links: [abs](https://arxiv.org/abs/2009.04042) | [pdf](https://arxiv.org/pdf/2009.04042)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

The detection and recognition of unconstrained text is an open problem in research. Text in comic books has unusual styles that raise many challenges for text detection. This work aims to binarize text in a comic genre with highly sophisticated text styles: Japanese manga. To overcome the lack of a manga dataset with text annotations at a pixel level, we create our own. To improve the evaluation and search of an optimal model, in addition to standard metrics in binarization, we implement other special metrics. Using these resources, we designed and evaluated a deep network model, outperforming current methods for text binarization in manga in most metrics.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Unconstrained Text Detection in Manga: a New Dataset and Baseline<br>pdf: <a href="https://t.co/gqBOGi7M52">https://t.co/gqBOGi7M52</a><br>abs: <a href="https://t.co/HQXaHVb3bX">https://t.co/HQXaHVb3bX</a> <a href="https://t.co/JhBHQP7eHg">pic.twitter.com/JhBHQP7eHg</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1303882359381647367?ref_src=twsrc%5Etfw">September 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



