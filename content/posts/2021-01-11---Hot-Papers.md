---
title: Hot Papers 2021-01-11
date: 2021-01-12T13:59:29.Z
template: "post"
draft: false
slug: "hot-papers-2021-01-11"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2021-01-11"
socialImage: "/media/flying-marine.jpg"

---

# 1. A Tale of Fairness Revisited: Beyond Adversarial Learning for Deep  Neural Network Fairness

Becky Mashaido, Winston Moh Tangongho

- retweets: 1155, favorites: 15 (01/12/2021 13:59:29)

- links: [abs](https://arxiv.org/abs/2101.02831) | [pdf](https://arxiv.org/pdf/2101.02831)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent)

Motivated by the need for fair algorithmic decision making in the age of automation and artificially-intelligent technology, this technical report provides a theoretical insight into adversarial training for fairness in deep learning. We build upon previous work in adversarial fairness, show the persistent tradeoff between fair predictions and model performance, and explore further mechanisms that help in offsetting this tradeoff.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">A Tale of Fairness Revisited: Beyond Adversarial Learning for Deep Neural Network Fairness. <a href="https://twitter.com/hashtag/DeepLearning?src=hash&amp;ref_src=twsrc%5Etfw">#DeepLearning</a> <a href="https://twitter.com/hashtag/BigData?src=hash&amp;ref_src=twsrc%5Etfw">#BigData</a> <a href="https://twitter.com/hashtag/Analytics?src=hash&amp;ref_src=twsrc%5Etfw">#Analytics</a> <a href="https://twitter.com/hashtag/Python?src=hash&amp;ref_src=twsrc%5Etfw">#Python</a> <a href="https://twitter.com/hashtag/RStats?src=hash&amp;ref_src=twsrc%5Etfw">#RStats</a> <a href="https://twitter.com/hashtag/DevCommunity?src=hash&amp;ref_src=twsrc%5Etfw">#DevCommunity</a> <a href="https://twitter.com/hashtag/Serverless?src=hash&amp;ref_src=twsrc%5Etfw">#Serverless</a> <a href="https://twitter.com/hashtag/Linux?src=hash&amp;ref_src=twsrc%5Etfw">#Linux</a> <a href="https://twitter.com/hashtag/Programming?src=hash&amp;ref_src=twsrc%5Etfw">#Programming</a> <a href="https://twitter.com/hashtag/IoT?src=hash&amp;ref_src=twsrc%5Etfw">#IoT</a> <a href="https://twitter.com/hashtag/womenwhocode?src=hash&amp;ref_src=twsrc%5Etfw">#womenwhocode</a> <a href="https://twitter.com/hashtag/100DaysOfCode?src=hash&amp;ref_src=twsrc%5Etfw">#100DaysOfCode</a> <a href="https://twitter.com/hashtag/DataScience?src=hash&amp;ref_src=twsrc%5Etfw">#DataScience</a> <a href="https://twitter.com/hashtag/MachineLearning?src=hash&amp;ref_src=twsrc%5Etfw">#MachineLearning</a> <a href="https://twitter.com/hashtag/AI?src=hash&amp;ref_src=twsrc%5Etfw">#AI</a><a href="https://t.co/X2hRsqWe8G">https://t.co/X2hRsqWe8G</a> <a href="https://t.co/B0CLFmQbZj">pic.twitter.com/B0CLFmQbZj</a></p>&mdash; Marcus Borba (@marcusborba) <a href="https://twitter.com/marcusborba/status/1348630248896819201?ref_src=twsrc%5Etfw">January 11, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. SE(3)-Equivariant Graph Neural Networks for Data-Efficient and Accurate  Interatomic Potentials

Simon Batzner, Tess E. Smidt, Lixin Sun, Jonathan P. Mailoa, Mordechai Kornbluth, Nicola Molinari, Boris Kozinsky

- retweets: 971, favorites: 115 (01/12/2021 13:59:30)

- links: [abs](https://arxiv.org/abs/2101.03164) | [pdf](https://arxiv.org/pdf/2101.03164)
- [physics.comp-ph](https://arxiv.org/list/physics.comp-ph/recent) | [cond-mat.mtrl-sci](https://arxiv.org/list/cond-mat.mtrl-sci/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

This work presents Neural Equivariant Interatomic Potentials (NequIP), a SE(3)-equivariant neural network approach for learning interatomic potentials from ab-initio calculations for molecular dynamics simulations. While most contemporary symmetry-aware models use invariant convolutions and only act on scalars, NequIP employs SE(3)-equivariant convolutions for interactions of geometric tensors, resulting in a more information-rich and faithful representation of atomic environments. The method achieves state-of-the-art accuracy on a challenging set of diverse molecules and materials while exhibiting remarkable data efficiency. NequIP outperforms existing models with up to three orders of magnitude fewer training data, challenging the widely held belief that deep neural networks require massive training sets. The high data efficiency of the method allows for the construction of accurate potentials using high-order quantum chemical level of theory as reference and enables high-fidelity molecular dynamics simulations over long time scales.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">We&#39;re excited to introduce NequIP, an equivariant Machine Learning Interatomic Potential that not only obtains SOTA on MD-17, but also outperforms existing potentials with up to 1000x fewer data! w/ <a href="https://twitter.com/tesssmidt?ref_src=twsrc%5Etfw">@tesssmidt</a> <a href="https://twitter.com/Materials_Intel?ref_src=twsrc%5Etfw">@Materials_Intel</a> <a href="https://twitter.com/bkoz37?ref_src=twsrc%5Etfw">@bkoz37</a> <a href="https://twitter.com/hashtag/compchem?src=hash&amp;ref_src=twsrc%5Etfw">#compchem</a>👇🧵 1/N <a href="https://t.co/5njHPLCcyD">https://t.co/5njHPLCcyD</a> <a href="https://t.co/mnUbxqYgCc">pic.twitter.com/mnUbxqYgCc</a></p>&mdash; Simon Batzner (@simonbatzner) <a href="https://twitter.com/simonbatzner/status/1348642158308425732?ref_src=twsrc%5Etfw">January 11, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. VisualVoice: Audio-Visual Speech Separation with Cross-Modal Consistency

Ruohan Gao, Kristen Grauman

- retweets: 241, favorites: 104 (01/12/2021 13:59:30)

- links: [abs](https://arxiv.org/abs/2101.03149) | [pdf](https://arxiv.org/pdf/2101.03149)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.SD](https://arxiv.org/list/cs.SD/recent) | [eess.IV](https://arxiv.org/list/eess.IV/recent)

We introduce a new approach for audio-visual speech separation. Given a video, the goal is to extract the speech associated with a face in spite of simultaneous background sounds and/or other human speakers. Whereas existing methods focus on learning the alignment between the speaker's lip movements and the sounds they generate, we propose to leverage the speaker's face appearance as an additional prior to isolate the corresponding vocal qualities they are likely to produce. Our approach jointly learns audio-visual speech separation and cross-modal speaker embeddings from unlabeled video. It yields state-of-the-art results on five benchmark datasets for audio-visual speech separation and enhancement, and generalizes well to challenging real-world videos of diverse scenarios. Our video results and code: http://vision.cs.utexas.edu/projects/VisualVoice/.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">VisualVoice: Audio-Visual Speech Separation with Cross-Modal Consistency<br>pdf: <a href="https://t.co/5qZYJY9Bnt">https://t.co/5qZYJY9Bnt</a><br>abs: <a href="https://t.co/JZJGfkAsLf">https://t.co/JZJGfkAsLf</a><br>project page: <a href="https://t.co/3FgTlmKaSd">https://t.co/3FgTlmKaSd</a> <a href="https://t.co/oDuMDzeLLr">pic.twitter.com/oDuMDzeLLr</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1348461817299935233?ref_src=twsrc%5Etfw">January 11, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. The Distracting Control Suite -- A Challenging Benchmark for  Reinforcement Learning from Pixels

Austin Stone, Oscar Ramirez, Kurt Konolige, Rico Jonschkowski

- retweets: 145, favorites: 36 (01/12/2021 13:59:30)

- links: [abs](https://arxiv.org/abs/2101.02722) | [pdf](https://arxiv.org/pdf/2101.02722)
- [cs.RO](https://arxiv.org/list/cs.RO/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Robots have to face challenging perceptual settings, including changes in viewpoint, lighting, and background. Current simulated reinforcement learning (RL) benchmarks such as DM Control provide visual input without such complexity, which limits the transfer of well-performing methods to the real world. In this paper, we extend DM Control with three kinds of visual distractions (variations in background, color, and camera pose) to produce a new challenging benchmark for vision-based control, and we analyze state of the art RL algorithms in these settings. Our experiments show that current RL methods for vision-based control perform poorly under distractions, and that their performance decreases with increasing distraction complexity, showing that new methods are needed to cope with the visual complexities of the real world. We also find that combinations of multiple distraction types are more difficult than a mere combination of their individual effects.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">The Distracting Control Suite – A Challenging Benchmark<br>for Reinforcement Learning from Pixels<br>pdf: <a href="https://t.co/EfAFtqL6xo">https://t.co/EfAFtqL6xo</a><br>abs: <a href="https://t.co/17VgeDj3li">https://t.co/17VgeDj3li</a><br>github: <a href="https://t.co/QvKqSxuv1Q">https://t.co/QvKqSxuv1Q</a> <a href="https://t.co/KP3DY46J9O">pic.twitter.com/KP3DY46J9O</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1348456522716737536?ref_src=twsrc%5Etfw">January 11, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial  Network for Video Generation

Yaohui Wang, Francois Bremond, Antitza Dantcheva

- retweets: 74, favorites: 48 (01/12/2021 13:59:30)

- links: [abs](https://arxiv.org/abs/2101.03049) | [pdf](https://arxiv.org/pdf/2101.03049)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

In this work, we introduce an unconditional video generative model, InMoDeGAN, targeted to (a) generate high quality videos, as well as to (b) allow for interpretation of the latent space. For the latter, we place emphasis on interpreting and manipulating motion. Towards this, we decompose motion into semantic sub-spaces, which allow for control of generated samples. We design the architecture of InMoDeGAN-generator in accordance to proposed Linear Motion Decomposition, which carries the assumption that motion can be represented by a dictionary, with related vectors forming an orthogonal basis in the latent space. Each vector in the basis represents a semantic sub-space. In addition, a Temporal Pyramid Discriminator analyzes videos at different temporal resolutions. Extensive quantitative and qualitative analysis shows that our model systematically and significantly outperforms state-of-the-art methods on the VoxCeleb2-mini and BAIR-robot datasets w.r.t. video quality related to (a). Towards (b) we present experimental results, confirming that decomposed sub-spaces are interpretable and moreover, generated motion is controllable.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation<br>pdf: <a href="https://t.co/DqE4zZ3E1t">https://t.co/DqE4zZ3E1t</a><br>abs: <a href="https://t.co/o2wdsDQrGU">https://t.co/o2wdsDQrGU</a><br>project page: <a href="https://t.co/7RZ5IWKJa8">https://t.co/7RZ5IWKJa8</a> <a href="https://t.co/IegvXTlRvx">pic.twitter.com/IegvXTlRvx</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1348452847344619523?ref_src=twsrc%5Etfw">January 11, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. One-Class Classification: A Survey

Pramuditha Perera, Poojan Oza, Vishal M. Patel

- retweets: 25, favorites: 37 (01/12/2021 13:59:30)

- links: [abs](https://arxiv.org/abs/2101.03064) | [pdf](https://arxiv.org/pdf/2101.03064)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

One-Class Classification (OCC) is a special case of multi-class classification, where data observed during training is from a single positive class. The goal of OCC is to learn a representation and/or a classifier that enables recognition of positively labeled queries during inference. This topic has received considerable amount of interest in the computer vision, machine learning and biometrics communities in recent years. In this article, we provide a survey of classical statistical and recent deep learning-based OCC methods for visual recognition. We discuss the merits and drawbacks of existing OCC approaches and identify promising avenues for research in this field. In addition, we present a discussion of commonly used datasets and evaluation metrics for OCC.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">1クラスのみの分類問題を解かせることで、次元圧縮された特徴ベクトルの空間などを取得し、その特徴空間の分布を用いて異常検出を行う様々な手法を紹介しているサーベイ論文。<br>One Class Classificationの手法の歴史が学べるので、時間のあるときに読んでみよう。<a href="https://t.co/hMqIaYDBTQ">https://t.co/hMqIaYDBTQ</a> <a href="https://t.co/gPH0l04Y8d">pic.twitter.com/gPH0l04Y8d</a></p>&mdash; 福田敦史 / Aillis CTO (@fukumimi014) <a href="https://twitter.com/fukumimi014/status/1348640465952804865?ref_src=twsrc%5Etfw">January 11, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. A Novel Regression Loss for Non-Parametric Uncertainty Optimization

Joachim Sicking, Maram Akila, Maximilian Pintz, Tim Wirtz, Asja Fischer, Stefan Wrobel

- retweets: 42, favorites: 11 (01/12/2021 13:59:31)

- links: [abs](https://arxiv.org/abs/2101.02726) | [pdf](https://arxiv.org/pdf/2101.02726)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Quantification of uncertainty is one of the most promising approaches to establish safe machine learning. Despite its importance, it is far from being generally solved, especially for neural networks. One of the most commonly used approaches so far is Monte Carlo dropout, which is computationally cheap and easy to apply in practice. However, it can underestimate the uncertainty. We propose a new objective, referred to as second-moment loss (SML), to address this issue. While the full network is encouraged to model the mean, the dropout networks are explicitly used to optimize the model variance. We intensively study the performance of the new objective on various UCI regression datasets. Comparing to the state-of-the-art of deep ensembles, SML leads to comparable prediction accuracies and uncertainty estimates while only requiring a single model. Under distribution shift, we observe moderate improvements. As a side result, we introduce an intuitive Wasserstein distance-based uncertainty measure that is non-saturating and thus allows to resolve quality differences between any two uncertainty estimates.



