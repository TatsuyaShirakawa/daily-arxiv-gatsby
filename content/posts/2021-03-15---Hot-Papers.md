---
title: Hot Papers 2021-03-15
date: 2021-03-16T09:09:31.Z
template: "post"
draft: false
slug: "hot-papers-2021-03-15"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2021-03-15"
socialImage: "/media/flying-marine.jpg"

---

# 1. HumanGAN: A Generative Model of Humans Images

Kripasindhu Sarkar, Lingjie Liu, Vladislav Golyanik, Christian Theobalt

- retweets: 729, favorites: 186 (03/16/2021 09:09:31)

- links: [abs](https://arxiv.org/abs/2103.06902) | [pdf](https://arxiv.org/pdf/2103.06902)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Generative adversarial networks achieve great performance in photorealistic image synthesis in various domains, including human images. However, they usually employ latent vectors that encode the sampled outputs globally. This does not allow convenient control of semantically-relevant individual parts of the image, and is not able to draw samples that only differ in partial aspects, such as clothing style. We address these limitations and present a generative model for images of dressed humans offering control over pose, local body part appearance and garment style. This is the first method to solve various aspects of human image generation such as global appearance sampling, pose transfer, parts and garment transfer, and parts sampling jointly in a unified framework. As our model encodes part-based latent appearance vectors in a normalized pose-independent space and warps them to different poses, it preserves body and clothing appearance under varying posture. Experiments show that our flexible and general generative method outperforms task-specific baselines for pose-conditioned image generation, pose transfer and part sampling in terms of realism and output resolution.

<blockquote class="twitter-tweet"><p lang="fr" dir="ltr">HumanGAN: A Generative Model of Humans Images<br>pdf: <a href="https://t.co/azARdJ9qlA">https://t.co/azARdJ9qlA</a><br>abs: <a href="https://t.co/PEtACUcLWY">https://t.co/PEtACUcLWY</a> <a href="https://t.co/jsectwAJEg">pic.twitter.com/jsectwAJEg</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1371263602016055299?ref_src=twsrc%5Etfw">March 15, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="in" dir="ltr">HumanGAN <a href="https://t.co/T7dkM6jFuE">https://t.co/T7dkM6jFuE</a> (2019)<br>vs.<br>HumanGAN <a href="https://t.co/oXfNBcwPRh">https://t.co/oXfNBcwPRh</a> (2021) <a href="https://t.co/e4FEQ4XbUR">https://t.co/e4FEQ4XbUR</a></p>&mdash; Shinnosuke Takamichi (高道 慎之介) (@forthshinji) <a href="https://twitter.com/forthshinji/status/1371286323806433283?ref_src=twsrc%5Etfw">March 15, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Probabilistic two-stage detection

Xingyi Zhou, Vladlen Koltun, Philipp Krähenbühl

- retweets: 311, favorites: 107 (03/16/2021 09:09:32)

- links: [abs](https://arxiv.org/abs/2103.07461) | [pdf](https://arxiv.org/pdf/2103.07461)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

We develop a probabilistic interpretation of two-stage object detection. We show that this probabilistic interpretation motivates a number of common empirical training practices. It also suggests changes to two-stage detection pipelines. Specifically, the first stage should infer proper object-vs-background likelihoods, which should then inform the overall score of the detector. A standard region proposal network (RPN) cannot infer this likelihood sufficiently well, but many one-stage detectors can. We show how to build a probabilistic two-stage detector from any state-of-the-art one-stage detector. The resulting detectors are faster and more accurate than both their one- and two-stage precursors. Our detector achieves 56.4 mAP on COCO test-dev with single-scale testing, outperforming all published results. Using a lightweight backbone, our detector achieves 49.2 mAP on COCO at 33 fps on a Titan Xp, outperforming the popular YOLOv4 model.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Probabilistic two-stage detection<br>pdf: <a href="https://t.co/AJ4m1cm95g">https://t.co/AJ4m1cm95g</a><br>abs: <a href="https://t.co/HERcj5bNFw">https://t.co/HERcj5bNFw</a><br>github: <a href="https://t.co/U8poNvhe78">https://t.co/U8poNvhe78</a> <a href="https://t.co/udAiVtGpw7">pic.twitter.com/udAiVtGpw7</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1371279275442241537?ref_src=twsrc%5Etfw">March 15, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">Probabilistic two-stage detection<a href="https://t.co/Y5LnYLYYoA">https://t.co/Y5LnYLYYoA</a><a href="https://t.co/iuOC2D1uoS">https://t.co/iuOC2D1uoS</a><br>去年のやつが公開されてた <a href="https://t.co/zNDi2vpQmh">pic.twitter.com/zNDi2vpQmh</a></p>&mdash; phalanx (@ZFPhalanx) <a href="https://twitter.com/ZFPhalanx/status/1371298264549486599?ref_src=twsrc%5Etfw">March 15, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Modern Dimension Reduction

Philip D. Waggoner

- retweets: 252, favorites: 63 (03/16/2021 09:09:32)

- links: [abs](https://arxiv.org/abs/2103.06885) | [pdf](https://arxiv.org/pdf/2103.06885)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.CY](https://arxiv.org/list/cs.CY/recent) | [stat.AP](https://arxiv.org/list/stat.AP/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Data are not only ubiquitous in society, but are increasingly complex both in size and dimensionality. Dimension reduction offers researchers and scholars the ability to make such complex, high dimensional data spaces simpler and more manageable. This Element offers readers a suite of modern unsupervised dimension reduction techniques along with hundreds of lines of R code, to efficiently represent the original high dimensional data space in a simplified, lower dimensional subspace. Launching from the earliest dimension reduction technique principal components analysis and using real social science data, I introduce and walk readers through application of the following techniques: locally linear embedding, t-distributed stochastic neighbor embedding (t-SNE), uniform manifold approximation and projection, self-organizing maps, and deep autoencoders. The result is a well-stocked toolbox of unsupervised algorithms for tackling the complexities of high dimensional data so common in modern society. All code is publicly accessible on Github.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Looks like  a really cool intro to unsupervised methods, with a focus on the social sciences! &quot;Modern Dimension Reduction&quot; by Philip D. Waggoner: <a href="https://t.co/UqDLrijJXl">https://t.co/UqDLrijJXl</a>; and there&#39;s code (<a href="https://t.co/SYBp8mCX7O">https://t.co/SYBp8mCX7O</a>)! <a href="https://t.co/pYYCn8RXlr">pic.twitter.com/pYYCn8RXlr</a></p>&mdash; Adam Lauretig (@lauretig) <a href="https://twitter.com/lauretig/status/1371268555535220736?ref_src=twsrc%5Etfw">March 15, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. VDSM: Unsupervised Video Disentanglement with State-Space Modeling and  Deep Mixtures of Experts

Matthew J. Vowels, Necati Cihan Camgoz, Richard Bowden

- retweets: 182, favorites: 112 (03/16/2021 09:09:32)

- links: [abs](https://arxiv.org/abs/2103.07292) | [pdf](https://arxiv.org/pdf/2103.07292)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Disentangled representations support a range of downstream tasks including causal reasoning, generative modeling, and fair machine learning. Unfortunately, disentanglement has been shown to be impossible without the incorporation of supervision or inductive bias. Given that supervision is often expensive or infeasible to acquire, we choose to incorporate structural inductive bias and present an unsupervised, deep State-Space-Model for Video Disentanglement (VDSM). The model disentangles latent time-varying and dynamic factors via the incorporation of hierarchical structure with a dynamic prior and a Mixture of Experts decoder. VDSM learns separate disentangled representations for the identity of the object or person in the video, and for the action being performed. We evaluate VDSM across a range of qualitative and quantitative tasks including identity and dynamics transfer, sequence generation, Fr\'echet Inception Distance, and factor classification. VDSM provides state-of-the-art performance and exceeds adversarial methods, even when the methods use additional supervision.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">VDSM: Unsupervised Video Disentanglement with State-Space Modeling and Deep Mixtures of Experts<br>pdf: <a href="https://t.co/vWOgbT8BXC">https://t.co/vWOgbT8BXC</a><br>abs: <a href="https://t.co/6nZyRIgWfi">https://t.co/6nZyRIgWfi</a><br>github: <a href="https://t.co/5rIOG6XWhK">https://t.co/5rIOG6XWhK</a> <a href="https://t.co/emdsFW2Dcr">pic.twitter.com/emdsFW2Dcr</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1371277621863059457?ref_src=twsrc%5Etfw">March 15, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Large Batch Simulation for Deep Reinforcement Learning

Brennan Shacklett, Erik Wijmans, Aleksei Petrenko, Manolis Savva, Dhruv Batra, Vladlen Koltun, Kayvon Fatahalian

- retweets: 146, favorites: 31 (03/16/2021 09:09:32)

- links: [abs](https://arxiv.org/abs/2103.07013) | [pdf](https://arxiv.org/pdf/2103.07013)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.GR](https://arxiv.org/list/cs.GR/recent)

We accelerate deep reinforcement learning-based training in visually complex 3D environments by two orders of magnitude over prior work, realizing end-to-end training speeds of over 19,000 frames of experience per second on a single GPU and up to 72,000 frames per second on a single eight-GPU machine. The key idea of our approach is to design a 3D renderer and embodied navigation simulator around the principle of "batch simulation": accepting and executing large batches of requests simultaneously. Beyond exposing large amounts of work at once, batch simulation allows implementations to amortize in-memory storage of scene assets, rendering work, data loading, and synchronization costs across many simulation requests, dramatically improving the number of simulated agents per GPU and overall simulation throughput. To balance DNN inference and training costs with faster simulation, we also build a computationally efficient policy DNN that maintains high task performance, and modify training algorithms to maintain sample efficiency when training with large mini-batches. By combining batch simulation and DNN performance optimizations, we demonstrate that PointGoal navigation agents can be trained in complex 3D environments on a single GPU in 1.5 days to 97% of the accuracy of agents trained on a prior state-of-the-art system using a 64-GPU cluster over three days. We provide open-source reference implementations of our batch 3D renderer and simulator to facilitate incorporation of these ideas into RL systems.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Large Batch Simulation for Deep Reinforcement Learning<br>pdf: <a href="https://t.co/km7Es8HxIt">https://t.co/km7Es8HxIt</a><br>abs: <a href="https://t.co/tEwdQBnI8J">https://t.co/tEwdQBnI8J</a><br>github: <a href="https://t.co/5CRJvEbqJy">https://t.co/5CRJvEbqJy</a> <a href="https://t.co/hqbHkHwQ2a">pic.twitter.com/hqbHkHwQ2a</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1371264972806713345?ref_src=twsrc%5Etfw">March 15, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. Neural Reprojection Error: Merging Feature Learning and Camera Pose  Estimation

Hugo Germain, Vincent Lepetit, Guillaume Bourmaud

- retweets: 120, favorites: 37 (03/16/2021 09:09:32)

- links: [abs](https://arxiv.org/abs/2103.07153) | [pdf](https://arxiv.org/pdf/2103.07153)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Absolute camera pose estimation is usually addressed by sequentially solving two distinct subproblems: First a feature matching problem that seeks to establish putative 2D-3D correspondences, and then a Perspective-n-Point problem that minimizes, with respect to the camera pose, the sum of so-called Reprojection Errors (RE). We argue that generating putative 2D-3D correspondences 1) leads to an important loss of information that needs to be compensated as far as possible, within RE, through the choice of a robust loss and the tuning of its hyperparameters and 2) may lead to an RE that conveys erroneous data to the pose estimator. In this paper, we introduce the Neural Reprojection Error (NRE) as a substitute for RE. NRE allows to rethink the camera pose estimation problem by merging it with the feature learning problem, hence leveraging richer information than 2D-3D correspondences and eliminating the need for choosing a robust loss and its hyperparameters. Thus NRE can be used as training loss to learn image descriptors tailored for pose estimation. We also propose a coarse-to-fine optimization method able to very efficiently minimize a sum of NRE terms with respect to the camera pose. We experimentally demonstrate that NRE is a good substitute for RE as it significantly improves both the robustness and the accuracy of the camera pose estimate while being computationally and memory highly efficient. From a broader point of view, we believe this new way of merging deep learning and 3D geometry may be useful in other computer vision applications.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Neural Reprojection Error: Merging Feature Learning and Camera Pose Estimation<br><br>Hugo Germain, Vincent Lepetit, Guillaume Bourmaud<a href="https://t.co/GHl0pLtLRk">https://t.co/GHl0pLtLRk</a><br>Idea: require dense descriptor similarly match &quot;reprojection&quot; probability. I.e. small blob where 3D point lies in 2D image <a href="https://t.co/42RN9EVkwz">pic.twitter.com/42RN9EVkwz</a></p>&mdash; Dmytro Mishkin (@ducha_aiki) <a href="https://twitter.com/ducha_aiki/status/1371480070758490118?ref_src=twsrc%5Etfw">March 15, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. Latent Space Explorations of Singing Voice Synthesis using DDSP

Juan Alonso, Cumhur Erkut

- retweets: 70, favorites: 76 (03/16/2021 09:09:32)

- links: [abs](https://arxiv.org/abs/2103.07197) | [pdf](https://arxiv.org/pdf/2103.07197)
- [cs.SD](https://arxiv.org/list/cs.SD/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.MM](https://arxiv.org/list/cs.MM/recent) | [eess.AS](https://arxiv.org/list/eess.AS/recent)

Machine learning based singing voice models require large datasets and lengthy training times. In this work we present a lightweight architecture, based on the Differentiable Digital Signal Processing (DDSP) library, that is able to output song-like utterances conditioned only on pitch and amplitude, after twelve hours of training using small datasets of unprocessed audio. The results are promising, as both the melody and the singer's voice are recognizable. In addition, we present two zero-configuration tools to train new models and experiment with them. Currently we are exploring the latent space representation, which is included in the DDSP library, but not in the original DDSP examples. Our results indicate that the latent space improves both the identification of the singer as well as the comprehension of the lyrics. Our code is available at https://github.com/juanalonso/DDSP-singing-experiments with links to the zero-configuration notebooks, and our sound examples are at https://juanalonso.github.io/DDSP-singing-experiments/ .

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Latent Space Explorations of Singing Voice Synthesis using DDSP<br>pdf: <a href="https://t.co/mJCyCSc3Dm">https://t.co/mJCyCSc3Dm</a><br>abs: <a href="https://t.co/7T1igxWAAk">https://t.co/7T1igxWAAk</a><br>github: <a href="https://t.co/5yYxpaykGx">https://t.co/5yYxpaykGx</a> <a href="https://t.co/roZEFENUko">pic.twitter.com/roZEFENUko</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1371272149491724289?ref_src=twsrc%5Etfw">March 15, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. Vision Transformer for COVID-19 CXR Diagnosis using Chest X-ray Feature  Corpus

Sangjoon Park, Gwanghyun Kim, Yujin Oh, Joon Beom Seo, Sang Min Lee, Jin Hwan Kim, Sungjun Moon, Jae-Kwang Lim, Jong Chul Ye

- retweets: 72, favorites: 51 (03/16/2021 09:09:33)

- links: [abs](https://arxiv.org/abs/2103.07055) | [pdf](https://arxiv.org/pdf/2103.07055)
- [eess.IV](https://arxiv.org/list/eess.IV/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Under the global COVID-19 crisis, developing robust diagnosis algorithm for COVID-19 using CXR is hampered by the lack of the well-curated COVID-19 data set, although CXR data with other disease are abundant. This situation is suitable for vision transformer architecture that can exploit the abundant unlabeled data using pre-training. However, the direct use of existing vision transformer that uses the corpus generated by the ResNet is not optimal for correct feature embedding. To mitigate this problem, we propose a novel vision Transformer by using the low-level CXR feature corpus that are obtained to extract the abnormal CXR features. Specifically, the backbone network is trained using large public datasets to obtain the abnormal features in routine diagnosis such as consolidation, glass-grass opacity (GGO), etc. Then, the embedded features from the backbone network are used as corpus for vision transformer training. We examine our model on various external test datasets acquired from totally different institutions to assess the generalization ability. Our experiments demonstrate that our method achieved the state-of-art performance and has better generalization capability, which are crucial for a widespread deployment.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Vision Transformer for COVID-19 CXR Diagnosis using Chest X-ray Feature Corpus<br>pdf: <a href="https://t.co/PO6626eHdV">https://t.co/PO6626eHdV</a><br>abs: <a href="https://t.co/vPAb4PcaLi">https://t.co/vPAb4PcaLi</a> <a href="https://t.co/cTPd135yMP">pic.twitter.com/cTPd135yMP</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1371261069893713922?ref_src=twsrc%5Etfw">March 15, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. Searching by Generating: Flexible and Efficient One-Shot NAS with  Architecture Generator

Sian-Yao Huang, Wei-Ta Chu

- retweets: 81, favorites: 14 (03/16/2021 09:09:33)

- links: [abs](https://arxiv.org/abs/2103.07289) | [pdf](https://arxiv.org/pdf/2103.07289)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

In one-shot NAS, sub-networks need to be searched from the supernet to meet different hardware constraints. However, the search cost is high and $N$ times of searches are needed for $N$ different constraints. In this work, we propose a novel search strategy called architecture generator to search sub-networks by generating them, so that the search process can be much more efficient and flexible. With the trained architecture generator, given target hardware constraints as the input, $N$ good architectures can be generated for $N$ constraints by just one forward pass without re-searching and supernet retraining. Moreover, we propose a novel single-path supernet, called unified supernet, to further improve search efficiency and reduce GPU memory consumption of the architecture generator. With the architecture generator and the unified supernet, we propose a flexible and efficient one-shot NAS framework, called Searching by Generating NAS (SGNAS). With the pre-trained supernt, the search time of SGNAS for $N$ different hardware constraints is only 5 GPU hours, which is $4N$ times faster than previous SOTA single-path methods. After training from scratch, the top1-accuracy of SGNAS on ImageNet is 77.1%, which is comparable with the SOTAs. The code is available at: https://github.com/eric8607242/SGNAS.




# 10. Preregistering NLP Research

Emiel van Miltenburg, Chris van der Lee, Emiel Krahmer

- retweets: 34, favorites: 46 (03/16/2021 09:09:33)

- links: [abs](https://arxiv.org/abs/2103.06944) | [pdf](https://arxiv.org/pdf/2103.06944)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

Preregistration refers to the practice of specifying what you are going to do, and what you expect to find in your study, before carrying out the study. This practice is increasingly common in medicine and psychology, but is rarely discussed in NLP. This paper discusses preregistration in more detail, explores how NLP researchers could preregister their work, and presents several preregistration questions for different kinds of studies. Finally, we argue in favour of registered reports, which could provide firmer grounds for slow science in NLP research. The goal of this paper is to elicit a discussion in the NLP community, which we hope to synthesise into a general NLP preregistration form in future research.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Our paper on preregistering <a href="https://twitter.com/hashtag/NLProc?src=hash&amp;ref_src=twsrc%5Etfw">#NLProc</a> research is now on ArXiv: <a href="https://t.co/dZ75GjbXa7">https://t.co/dZ75GjbXa7</a><br><br>I usually don&#39;t share work in progress, because then multiple versions of the paper will be floating around, but for this paper I&#39;d really appreciate feedback before it appears at NAACL.</p>&mdash; Emiel van Miltenburg (@evanmiltenburg) <a href="https://twitter.com/evanmiltenburg/status/1371349856569151489?ref_src=twsrc%5Etfw">March 15, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



