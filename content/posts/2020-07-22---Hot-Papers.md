---
title: Hot Papers 2020-07-22
date: 2020-07-23T10:41:31.Z
template: "post"
draft: false
slug: "hot-papers-2020-07-22"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-07-22"
socialImage: "/media/42-line-bible.jpg"

---

# 1. Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse  Coding

David Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland Brendel, Matthias Bethge, Dylan Paiton

- retweets: 100, favorites: 347 (07/23/2020 10:41:31)

- links: [abs](https://arxiv.org/abs/2007.10930) | [pdf](https://arxiv.org/pdf/2007.10930)
- [stat.ML](https://arxiv.org/list/stat.ML/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

We construct an unsupervised learning model that achieves nonlinear disentanglement of underlying factors of variation in naturalistic videos. Previous work suggests that representations can be disentangled if all but a few factors in the environment stay constant at any point in time. As a result, algorithms proposed for this problem have only been tested on carefully constructed datasets with this exact property, leaving it unclear whether they will transfer to natural scenes. Here we provide evidence that objects in segmented natural movies undergo transitions that are typically small in magnitude with occasional large jumps, which is characteristic of a temporally sparse distribution. We leverage this finding and present SlowVAE, a model for unsupervised representation learning that uses a sparse prior on temporally adjacent observations to disentangle generative factors without any assumptions on the number of changing factors. We provide a proof of identifiability and show that the model reliably learns disentangled representations on several established benchmark datasets, often surpassing the current state-of-the-art. We additionally demonstrate transferability towards video datasets with natural dynamics, Natural Sprites and KITTI Masks, which we contribute as benchmarks for guiding disentanglement research towards more natural data domains.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Unsupervised learning of disentangled representations is possible!<br><br>Our approach relies on videos (e.g. below) and assumes that the factors of variation change sparsely across time.<br>Paper: <a href="https://t.co/whtyc2EvIj">https://t.co/whtyc2EvIj</a><br>Code: <a href="https://t.co/I15x7GcIC9">https://t.co/I15x7GcIC9</a><br>[1/6] <a href="https://t.co/bQSQVucFko">pic.twitter.com/bQSQVucFko</a></p>&mdash; Bethge Lab (@bethgelab) <a href="https://twitter.com/bethgelab/status/1285944029084037121?ref_src=twsrc%5Etfw">July 22, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Randomized Automatic Differentiation

Deniz Oktay, Nick McGreivy, Joshua Aduol, Alex Beatson, Ryan P. Adams

- retweets: 50, favorites: 370 (07/23/2020 10:41:31)

- links: [abs](https://arxiv.org/abs/2007.10412) | [pdf](https://arxiv.org/pdf/2007.10412)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

The successes of deep learning, variational inference, and many other fields have been aided by specialized implementations of reverse-mode automatic differentiation (AD) to compute gradients of mega-dimensional objectives. The AD techniques underlying these tools were designed to compute exact gradients to numerical precision, but modern machine learning models are almost always trained with stochastic gradient descent. Why spend computation and memory on exact (minibatch) gradients only to use them for stochastic optimization? We develop a general framework and approach for randomized automatic differentiation (RAD), which allows unbiased gradient estimates to be computed with reduced memory in return for variance. We examine limitations of the general approach, and argue that we must leverage problem specific structure to realize benefits. We develop RAD techniques for a variety of simple neural network architectures, and show that for a fixed memory budget, RAD converges in fewer iterations than using a small batch size for feedforward networks, and in a similar number for recurrent networks. We also show that RAD can be applied to scientific computing, and use it to develop a low-memory stochastic gradient method for optimizing the control parameters of a linear reaction-diffusion PDE representing a fission reactor.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Why spend computation and memory on exact gradients only to use them for stochastic optimization?<br><br>Introducing: Randomized Automatic Differentiation (RAD)<a href="https://t.co/MQmb21m7wS">https://t.co/MQmb21m7wS</a><br><br>w/ <a href="https://twitter.com/NMcgreivy?ref_src=twsrc%5Etfw">@NMcgreivy</a> <a href="https://twitter.com/jaduol1?ref_src=twsrc%5Etfw">@jaduol1</a> <a href="https://twitter.com/AlexBeatson?ref_src=twsrc%5Etfw">@AlexBeatson</a> <a href="https://twitter.com/ryan_p_adams?ref_src=twsrc%5Etfw">@ryan_p_adams</a></p>&mdash; Deniz Oktay (@denizzokt) <a href="https://twitter.com/denizzokt/status/1285744028793929729?ref_src=twsrc%5Etfw">July 22, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Privacy Preserving Visual SLAM

Mikiya Shibuya, Shinya Sumikura, Ken Sakurada

- retweets: 82, favorites: 332 (07/23/2020 10:41:31)

- links: [abs](https://arxiv.org/abs/2007.10361) | [pdf](https://arxiv.org/pdf/2007.10361)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

This study proposes a privacy-preserving Visual SLAM framework for estimating camera poses and performing bundle adjustment with mixed line and point clouds in real time. Previous studies have proposed localization methods to estimate a camera pose using a line-cloud map for a single image or a reconstructed point cloud. These methods offer a scene privacy protection against the inversion attacks by converting a point cloud to a line cloud, which reconstruct the scene images from the point cloud. However, they are not directly applicable to a video sequence because they do not address computational efficiency. This is a critical issue to solve for estimating camera poses and performing bundle adjustment with mixed line and point clouds in real time. Moreover, there has been no study on a method to optimize a line-cloud map of a server with a point cloud reconstructed from a client video because any observation points on the image coordinates are not available to prevent the inversion attacks, namely the reversibility of the 3D lines. The experimental results with synthetic and real data show that our Visual SLAM framework achieves the intended privacy-preserving formation and real-time performance using a line-cloud map.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">私たちの論文 &quot;Privacy Preserving Visual SLAM&quot; がコンピュータビジョンの主要国際会議であるECCV2020に採択されました．AR/MRなどにおいてシーンのプライバシーを保護しながら複数ユーザー間でマップを共有することができます．<br>Paper: <a href="https://t.co/baV6TKnpiV">https://t.co/baV6TKnpiV</a><br>Project: <a href="https://t.co/M9UlhYfOyv">https://t.co/M9UlhYfOyv</a> <a href="https://t.co/afX6eC4Wov">pic.twitter.com/afX6eC4Wov</a></p>&mdash; Ken Sakurada (@sakuDken) <a href="https://twitter.com/sakuDken/status/1285764028002779136?ref_src=twsrc%5Etfw">July 22, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Privacy Preserving Visual SLAM<br>pdf: <a href="https://t.co/w3Fs8TTT53">https://t.co/w3Fs8TTT53</a><br>abs: <a href="https://t.co/5nrA16ahPz">https://t.co/5nrA16ahPz</a><br>project page: <a href="https://t.co/4krCZMEKPX">https://t.co/4krCZMEKPX</a> <a href="https://t.co/gUUUeIFms8">pic.twitter.com/gUUUeIFms8</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1285738338901798912?ref_src=twsrc%5Etfw">July 22, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Our paper &quot;Privacy Preserving Visual SLAM&quot; has been accepted at <a href="https://twitter.com/hashtag/ECCV2020?src=hash&amp;ref_src=twsrc%5Etfw">#ECCV2020</a>. The framework &quot;LC-VSLAM” enables real-time tracking/mapping with a line-cloud map in practical applications such as AR and MR.<br>Paper: <a href="https://t.co/baV6TKnpiV">https://t.co/baV6TKnpiV</a><br>Project: <a href="https://t.co/vTTEI0Kcht">https://t.co/vTTEI0Kcht</a> <a href="https://t.co/e9GaUAY9aU">pic.twitter.com/e9GaUAY9aU</a></p>&mdash; Ken Sakurada (@sakuDken) <a href="https://twitter.com/sakuDken/status/1285764767898955776?ref_src=twsrc%5Etfw">July 22, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Shape and Viewpoint without Keypoints

Shubham Goel, Angjoo Kanazawa, Jitendra Malik

- retweets: 40, favorites: 213 (07/23/2020 10:41:32)

- links: [abs](https://arxiv.org/abs/2007.10982) | [pdf](https://arxiv.org/pdf/2007.10982)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [eess.IV](https://arxiv.org/list/eess.IV/recent)

We present a learning framework that learns to recover the 3D shape, pose and texture from a single image, trained on an image collection without any ground truth 3D shape, multi-view, camera viewpoints or keypoint supervision. We approach this highly under-constrained problem in a "analysis by synthesis" framework where the goal is to predict the likely shape, texture and camera viewpoint that could produce the image with various learned category-specific priors. Our particular contribution in this paper is a representation of the distribution over cameras, which we call "camera-multiplex". Instead of picking a point estimate, we maintain a set of camera hypotheses that are optimized during training to best explain the image given the current shape and texture. We call our approach Unsupervised Category-Specific Mesh Reconstruction (U-CMR), and present qualitative and quantitative results on CUB, Pascal 3D and new web-scraped datasets. We obtain state-of-the-art camera prediction results and show that we can learn to predict diverse shapes and textures across objects using an image collection without any keypoint annotations or 3D ground truth. Project page: https://shubham-goel.github.io/ucmr

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">New work in <a href="https://twitter.com/hashtag/ECCV2020?src=hash&amp;ref_src=twsrc%5Etfw">#ECCV2020</a> learning to reconstruct a morphable shape, texture, and viewpoint from an image collection without 3D ground truth *and* 2D keypoints, allowing us to explore new categories like shoes! <br>Congrats to <a href="https://twitter.com/goelshbhm?ref_src=twsrc%5Etfw">@goelshbhm</a>! <a href="https://t.co/9fX482cgtp">https://t.co/9fX482cgtp</a><a href="https://t.co/bIL3RNJweC">https://t.co/bIL3RNJweC</a> <a href="https://t.co/jA6TJDYCPk">pic.twitter.com/jA6TJDYCPk</a></p>&mdash; Angjoo Kanazawa (@akanazawa) <a href="https://twitter.com/akanazawa/status/1286041155570810880?ref_src=twsrc%5Etfw">July 22, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Shape and Viewpoint without Keypoints<br>pdf: <a href="https://t.co/y9hKADuqGz">https://t.co/y9hKADuqGz</a><br>abs: <a href="https://t.co/7xCmzCDo6M">https://t.co/7xCmzCDo6M</a><br>project page: <a href="https://t.co/9M2z6ktTDS">https://t.co/9M2z6ktTDS</a> <a href="https://t.co/M4hRimaxZG">pic.twitter.com/M4hRimaxZG</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1285743798618927105?ref_src=twsrc%5Etfw">July 22, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. PointContrast: Unsupervised Pre-training for 3D Point Cloud  Understanding

Saining Xie, Jiatao Gu, Demi Guo, Charles R. Qi, Leonidas J. Guibas, Or Litany

- retweets: 34, favorites: 167 (07/23/2020 10:41:33)

- links: [abs](https://arxiv.org/abs/2007.10985) | [pdf](https://arxiv.org/pdf/2007.10985)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Arguably one of the top success stories of deep learning is transfer learning. The finding that pre-training a network on a rich source set (eg., ImageNet) can help boost performance once fine-tuned on a usually much smaller target set, has been instrumental to many applications in language and vision. Yet, very little is known about its usefulness in 3D point cloud understanding. We see this as an opportunity considering the effort required for annotating data in 3D. In this work, we aim at facilitating research on 3D representation learning. Different from previous works, we focus on high-level scene understanding tasks. To this end, we select a suite of diverse datasets and tasks to measure the effect of unsupervised pre-training on a large source set of 3D scenes. Our findings are extremely encouraging: using a unified triplet of architecture, source dataset, and contrastive loss for pre-training, we achieve improvement over recent best results in segmentation and detection across 6 different benchmarks for indoor and outdoor, real and synthetic datasets -- demonstrating that the learned representation can generalize across domains. Furthermore, the improvement was similar to supervised pre-training, suggesting that future efforts should favor scaling data collection over more detailed annotation. We hope these findings will encourage more research on unsupervised pretext task design for 3D deep learning.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Almost every deep learning model for 3D recognition has been *trained from scratch*. In our <a href="https://twitter.com/hashtag/ECCV2020?src=hash&amp;ref_src=twsrc%5Etfw">#ECCV2020</a> spotlight paper, we propose 👉PointContrast👈, an unsupervised pre-training framework that boosts performance on 6 different 3D point cloud benchmarks.  <a href="https://t.co/3tayph1lmh">https://t.co/3tayph1lmh</a> <a href="https://t.co/7FN7eMWD23">pic.twitter.com/7FN7eMWD23</a></p>&mdash; Saining Xie (@SainingX) <a href="https://twitter.com/SainingX/status/1285786883209879552?ref_src=twsrc%5Etfw">July 22, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. Neural Mesh Flow: 3D Manifold Mesh Generationvia Diffeomorphic Flows

Kunal Gupta, Manmohan Chandraker

- retweets: 28, favorites: 130 (07/23/2020 10:41:33)

- links: [abs](https://arxiv.org/abs/2007.10973) | [pdf](https://arxiv.org/pdf/2007.10973)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Meshes are important representations of physical 3D entities in the virtual world. Applications like rendering, simulations and 3D printing require meshes to be manifold so that they can interact with the world like the real objects they represent. Prior methods generate meshes with great geometric accuracy but poor manifoldness. In this work, we propose Neural Mesh Flow (NMF) to generate two-manifold meshes for genus-0 shapes. Specifically, NMF is a shape auto-encoder consisting of several Neural Ordinary Differential Equation (NODE)[1]blocks that learn accurate mesh geometry by progressively deforming a spherical mesh. Training NMF is simpler compared to state-of-the-art methods since it does not require any explicit mesh-based regularization. Our experiments demonstrate that NMF facilitates several applications such as single-view mesh reconstruction, global shape parameterization, texture mapping, shape deformation and correspondence. Importantly, we demonstrate that manifold meshes generated using NMF are better-suited for physically-based rendering and simulation. Code and data will be released.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Neural Mesh Flow: 3D Manifold Mesh Generation via Diffeomorphic Flows<br>pdf: <a href="https://t.co/H1RSfgdNZU">https://t.co/H1RSfgdNZU</a><br>abs: <a href="https://t.co/IzMpra5ngL">https://t.co/IzMpra5ngL</a><br>project page: <a href="https://t.co/QUiDikfGhB">https://t.co/QUiDikfGhB</a> <a href="https://t.co/kAjTPM57WA">pic.twitter.com/kAjTPM57WA</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1285755188008157184?ref_src=twsrc%5Etfw">July 22, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Excited to announce our new work!<br>“Neural Mesh Flow: 3D Manifold Mesh Generation via Diffeomorphic Flows”<br><br>Generate physically realizable meshes that can be 3D printed, or used in physics simulation<br><br>Paper: <a href="https://t.co/l4EgfaRrgC">https://t.co/l4EgfaRrgC</a><br>Project Page: <a href="https://t.co/9GzjR1MdAx">https://t.co/9GzjR1MdAx</a> <a href="https://t.co/iOyOFzdwZ0">pic.twitter.com/iOyOFzdwZ0</a></p>&mdash; Kunal Gupta (@KunalMGupta) <a href="https://twitter.com/KunalMGupta/status/1285772807553482752?ref_src=twsrc%5Etfw">July 22, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. Generative Hierarchical Features from Synthesizing Images

Yinghao Xu, Yujun Shen, Jiapeng Zhu, Ceyuan Yang, Bolei Zhou

- retweets: 20, favorites: 82 (07/23/2020 10:41:33)

- links: [abs](https://arxiv.org/abs/2007.10379) | [pdf](https://arxiv.org/pdf/2007.10379)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Generative Adversarial Networks (GANs) have recently advanced image synthesis by learning the underlying distribution of observed data in an unsupervised manner. However, how the features trained from solving the task of image synthesis are applicable to visual tasks remains seldom explored. In this work, we show that learning to synthesize images is able to bring remarkable hierarchical visual features that are generalizable across a wide range of visual tasks. Specifically, we consider the pre-trained StyleGAN generator as a learned loss function and utilize its layer-wise disentangled representation to train a novel hierarchical encoder. As a result, the visual feature produced by our encoder, termed as Generative Hierarchical Feature (GH-Feat), has compelling discriminative and disentangled properties, facilitating a range of both discriminative and generative tasks. Extensive experiments on face verification, landmark detection, layout prediction, transfer learning, style mixing, and image editing show the appealing performance of the GH-Feat learned from synthesizing images, outperforming existing unsupervised feature learning methods.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Generative Hierarchical Features from Synthesizing Images<br>pdf: <a href="https://t.co/nR2Fnty2uU">https://t.co/nR2Fnty2uU</a><br>abs: <a href="https://t.co/7muyoJVEFS">https://t.co/7muyoJVEFS</a> <a href="https://t.co/IaYouIwk1H">pic.twitter.com/IaYouIwk1H</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1285750882479607820?ref_src=twsrc%5Etfw">July 22, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. Learning Monocular Visual Odometry via Self-Supervised Long-Term  Modeling

Yuliang Zou, Pan Ji, Quoc-Huy Tran, Jia-Bin Huang, Manmohan Chandraker

- retweets: 23, favorites: 72 (07/23/2020 10:41:34)

- links: [abs](https://arxiv.org/abs/2007.10983) | [pdf](https://arxiv.org/pdf/2007.10983)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Monocular visual odometry (VO) suffers severely from error accumulation during frame-to-frame pose estimation. In this paper, we present a self-supervised learning method for VO with special consideration for consistency over longer sequences. To this end, we model the long-term dependency in pose prediction using a pose network that features a two-layer convolutional LSTM module. We train the networks with purely self-supervised losses, including a cycle consistency loss that mimics the loop closure module in geometric VO. Inspired by prior geometric systems, we allow the networks to see beyond a small temporal window during training, through a novel a loss that incorporates temporally distant (e.g., O(100)) frames. Given GPU memory constraints, we propose a stage-wise training mechanism, where the first stage operates in a local time window and the second stage refines the poses with a "global" loss given the first stage features. We demonstrate competitive results on several standard VO datasets, including KITTI and TUM RGB-D.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Learning Monocular Visual Odometry via Self-Supervised Long-Term Modeling <a href="https://t.co/rL2mQTrblC">https://t.co/rL2mQTrblC</a> <a href="https://twitter.com/hashtag/computervision?src=hash&amp;ref_src=twsrc%5Etfw">#computervision</a> <a href="https://t.co/CTsABVYlpR">pic.twitter.com/CTsABVYlpR</a></p>&mdash; Tomasz Malisiewicz (@quantombone) <a href="https://twitter.com/quantombone/status/1285944929303265280?ref_src=twsrc%5Etfw">July 22, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. Points2Surf: Learning Implicit Surfaces from Point Cloud Patches

Philipp Erler, Paul Guerrero, Stefan Ohrhallinger, Michael Wimmer, Niloy J. Mitra

- retweets: 17, favorites: 76 (07/23/2020 10:41:34)

- links: [abs](https://arxiv.org/abs/2007.10453) | [pdf](https://arxiv.org/pdf/2007.10453)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

A key step in any scanning-based asset creation workflow is to convert unordered point clouds to a surface. Classical methods (e.g., Poisson reconstruction) start to degrade in the presence of noisy and partial scans. Hence, deep learning based methods have recently been proposed to produce complete surfaces, even from partial scans. However, such data-driven methods struggle to generalize to new shapes with large geometric and topological variations. We present Points2Surf, a novel patch-based learning framework that produces accurate surfaces directly from raw scans without normals. Learning a prior over a combination of detailed local patches and coarse global information improves generalization performance and reconstruction accuracy. Our extensive comparison on both synthetic and real data demonstrates a clear advantage of our method over state-of-the-art alternatives on previously unseen classes (on average, Points2Surf brings down reconstruction error by 30\% over SPR and by 270\%+ over deep learning based SotA methods) at the cost of longer computation times and a slight increase in small-scale topological noise in some cases. Our source code, pre-trained model, and dataset are available on: https://github.com/ErlerPhilipp/points2surf

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Points2Surf Learning Implicit Surfaces from Point Clouds<br>pdf: <a href="https://t.co/4V34uSwqBI">https://t.co/4V34uSwqBI</a><br>abs: <a href="https://t.co/oSRZ5KKwMi">https://t.co/oSRZ5KKwMi</a><br>github: <a href="https://t.co/7WzlirOoIQ">https://t.co/7WzlirOoIQ</a> <a href="https://t.co/9Ih3VfMvmi">pic.twitter.com/9Ih3VfMvmi</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1285739052088664064?ref_src=twsrc%5Etfw">July 22, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 10. Ideas for Improving the Field of Machine Learning: Summarizing  Discussion from the NeurIPS 2019 Retrospectives Workshop

Shagun Sodhani, Mayoore S. Jaiswal, Lauren Baker, Koustuv Sinha, Carl Shneider, Peter Henderson, Joel Lehman, Ryan Lowe

- retweets: 21, favorites: 70 (07/23/2020 10:41:34)

- links: [abs](https://arxiv.org/abs/2007.10546) | [pdf](https://arxiv.org/pdf/2007.10546)
- [cs.CY](https://arxiv.org/list/cs.CY/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

This report documents ideas for improving the field of machine learning, which arose from discussions at the ML Retrospectives workshop at NeurIPS 2019. The goal of the report is to disseminate these ideas more broadly, and in turn encourage continuing discussion about how the field could improve along these axes. We focus on topics that were most discussed at the workshop: incentives for encouraging alternate forms of scholarship, re-structuring the review process, participation from academia and industry, and how we might better train computer scientists as scientists. Videos from the workshop can be accessed at https://slideslive.com/neurips/west-114-115-retrospectives-a-venue-for-selfreflection-in-ml-research

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Ideas for Improving the Field of Machine Learning: Summarizing Discussion from the NeurIPS 2019 Workshop: <a href="https://t.co/2VDqulVCnt">https://t.co/2VDqulVCnt</a><br><br>“One of most visible symptoms of the misalignment between the goal of the ML field and the incentives of researchers is paper obfuscation”</p>&mdash; Denny Britz (@dennybritz) <a href="https://twitter.com/dennybritz/status/1285966842775982082?ref_src=twsrc%5Etfw">July 22, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 11. What is important about the No Free Lunch theorems?

David H. Wolpert

- retweets: 8, favorites: 54 (07/23/2020 10:41:34)

- links: [abs](https://arxiv.org/abs/2007.10928) | [pdf](https://arxiv.org/pdf/2007.10928)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.NE](https://arxiv.org/list/cs.NE/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

The No Free Lunch theorems prove that under a uniform distribution over induction problems (search problems or learning problems), all induction algorithms perform equally. As I discuss in this chapter, the importance of the theorems arises by using them to analyze scenarios involving {non-uniform} distributions, and to compare different algorithms, without any assumption about the distribution over problems at all. In particular, the theorems prove that {anti}-cross-validation (choosing among a set of candidate algorithms based on which has {worst} out-of-sample behavior) performs as well as cross-validation, unless one makes an assumption -- which has never been formalized -- about how the distribution over induction problems, on the one hand, is related to the set of algorithms one is choosing among using (anti-)cross validation, on the other. In addition, they establish strong caveats concerning the significance of the many results in the literature which establish the strength of a particular algorithm without assuming a particular distribution. They also motivate a ``dictionary'' between supervised learning and improve blackbox optimization, which allows one to ``translate'' techniques from supervised learning into the domain of blackbox optimization, thereby strengthening blackbox optimization algorithms. In addition to these topics, I also briefly discuss their implications for philosophy of science.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Great review of the No Free Lunch theorems: <a href="https://t.co/AgtRr0COlW">https://t.co/AgtRr0COlW</a><br>An important point is that selecting hypotheses using cross-validation does not free you from the need for inductive bias, because CV is no better than any other selection algorithm under a uniform prior.</p>&mdash; Sam Gershman (@gershbrain) <a href="https://twitter.com/gershbrain/status/1285929591278907393?ref_src=twsrc%5Etfw">July 22, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



