---
title: Hot Papers 2021-04-05
date: 2021-04-06T09:19:02.Z
template: "post"
draft: false
slug: "hot-papers-2021-04-05"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2021-04-05"
socialImage: "/media/flying-marine.jpg"

---

# 1. Towards General Purpose Vision Systems

Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi, Derek Hoiem

- retweets: 5217, favorites: 177 (04/06/2021 09:19:02)

- links: [abs](https://arxiv.org/abs/2104.00743) | [pdf](https://arxiv.org/pdf/2104.00743)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

A special purpose learning system assumes knowledge of admissible tasks at design time. Adapting such a system to unforeseen tasks requires architecture manipulation such as adding an output head for each new task or dataset. In this work, we propose a task-agnostic vision-language system that accepts an image and a natural language task description and outputs bounding boxes, confidences, and text. The system supports a wide range of vision tasks such as classification, localization, question answering, captioning, and more. We evaluate the system's ability to learn multiple skills simultaneously, to perform tasks with novel skill-concept combinations, and to learn new skills efficiently and without forgetting.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Towards General Purpose Vision Systems<br>pdf: <a href="https://t.co/lYmA9BIa3n">https://t.co/lYmA9BIa3n</a><br>abs: <a href="https://t.co/KjXW1aQGBB">https://t.co/KjXW1aQGBB</a><br>project page: <a href="https://t.co/U37GTpAxeI">https://t.co/U37GTpAxeI</a> <a href="https://t.co/PhyPZsAniH">pic.twitter.com/PhyPZsAniH</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1378886190242402305?ref_src=twsrc%5Etfw">April 5, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Towards General Purpose Vision Systems<br><br>Proposes the first task-agnostic vision-language model for classification, grounding, QA, captioning etc that involve image, text and pointing (via bounding boxes) modalities.<br><br>abs: <a href="https://t.co/eHhZ2r806e">https://t.co/eHhZ2r806e</a><br>site: <a href="https://t.co/0vnktLqz6r">https://t.co/0vnktLqz6r</a> <a href="https://t.co/miNjTzgnxX">pic.twitter.com/miNjTzgnxX</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1378873135928287232?ref_src=twsrc%5Etfw">April 5, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. LatentCLR: A Contrastive Learning Approach for Unsupervised Discovery of  Interpretable Directions

Oğuz Kaan Yüksel, Enis Simsar, Ezgi Gülperi Er, Pinar Yanardag

- retweets: 388, favorites: 178 (04/06/2021 09:19:02)

- links: [abs](https://arxiv.org/abs/2104.00820) | [pdf](https://arxiv.org/pdf/2104.00820)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent)

Recent research has shown great potential for finding interpretable directions in the latent spaces of pre-trained Generative Adversarial Networks (GANs). These directions provide controllable generation and support a wide range of semantic editing operations such as zoom or rotation. The discovery of such directions is often performed in a supervised or semi-supervised fashion and requires manual annotations, limiting their applications in practice. In comparison, unsupervised discovery enables finding subtle directions a priori hard to recognize. In this work, we propose a contrastive-learning-based approach for discovering semantic directions in the latent space of pretrained GANs in a self-supervised manner. Our approach finds semantically meaningful dimensions compatible with state-of-the-art methods.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">LatentCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions<br>pdf: <a href="https://t.co/KpNAPuIfT7">https://t.co/KpNAPuIfT7</a><br>abs: <a href="https://t.co/YGKw1xxv2l">https://t.co/YGKw1xxv2l</a> <a href="https://t.co/jLkX1IUDw8">pic.twitter.com/jLkX1IUDw8</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1378877738103230464?ref_src=twsrc%5Etfw">April 5, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Happy to share our latest research where latent space manipulation meets contrastive learning: &quot;LatentCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions&quot; <a href="https://t.co/JhdgyuBeij">https://t.co/JhdgyuBeij</a> <a href="https://t.co/slRXNL23zj">pic.twitter.com/slRXNL23zj</a></p>&mdash; Pinar Yanardag (@PINguAR) <a href="https://twitter.com/PINguAR/status/1378965110962667520?ref_src=twsrc%5Etfw">April 5, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Decomposing 3D Scenes into Objects via Unsupervised Volume Segmentation

Karl Stelzner, Kristian Kersting, Adam R. Kosiorek

- retweets: 342, favorites: 106 (04/06/2021 09:19:02)

- links: [abs](https://arxiv.org/abs/2104.01148) | [pdf](https://arxiv.org/pdf/2104.01148)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

We present ObSuRF, a method which turns a single image of a scene into a 3D model represented as a set of Neural Radiance Fields (NeRFs), with each NeRF corresponding to a different object. A single forward pass of an encoder network outputs a set of latent vectors describing the objects in the scene. These vectors are used independently to condition a NeRF decoder, defining the geometry and appearance of each object. We make learning more computationally efficient by deriving a novel loss, which allows training NeRFs on RGB-D inputs without explicit ray marching. After confirming that the model performs equal or better than state of the art on three 2D image segmentation benchmarks, we apply it to two multi-object 3D datasets: A multiview version of CLEVR, and a novel dataset in which scenes are populated by ShapeNet models. We find that after training ObSuRF on RGB-D views of training scenes, it is capable of not only recovering the 3D geometry of a scene depicted in a single input image, but also to segment it into objects, despite receiving no supervision in that regard.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Decomposing 3D Scenes into Objects via Unsupervised Volume Segmentation<br>pdf: <a href="https://t.co/oDPUl6LWlG">https://t.co/oDPUl6LWlG</a><br>abs: <a href="https://t.co/OmqOukzsNV">https://t.co/OmqOukzsNV</a><br>project page: <a href="https://t.co/4AzFnng7Yo">https://t.co/4AzFnng7Yo</a> <a href="https://t.co/kBsnJtnPzQ">pic.twitter.com/kBsnJtnPzQ</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1378876152144617473?ref_src=twsrc%5Etfw">April 5, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Multiple Heads are Better than One: Few-shot Font Generation with  Multiple Localized Experts

Song Park, Sanghyuk Chun, Junbum Cha, Bado Lee, Hyunjung Shim

- retweets: 262, favorites: 73 (04/06/2021 09:19:02)

- links: [abs](https://arxiv.org/abs/2104.00887) | [pdf](https://arxiv.org/pdf/2104.00887)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

A few-shot font generation (FFG) method has to satisfy two objectives: the generated images should preserve the underlying global structure of the target character and present the diverse local reference style. Existing FFG methods aim to disentangle content and style either by extracting a universal representation style or extracting multiple component-wise style representations. However, previous methods either fail to capture diverse local styles or cannot be generalized to a character with unseen components, e.g., unseen language systems. To mitigate the issues, we propose a novel FFG method, named Multiple Localized Experts Few-shot Font Generation Network (MX-Font). MX-Font extracts multiple style features not explicitly conditioned on component labels, but automatically by multiple experts to represent different local concepts, e.g., left-side sub-glyph. Owing to the multiple experts, MX-Font can capture diverse local concepts and show the generalizability to unseen languages. During training, we utilize component labels as weak supervision to guide each expert to be specialized for different local concepts. We formulate the component assign problem to each expert as the graph matching problem, and solve it by the Hungarian algorithm. We also employ the independence loss and the content-style adversarial loss to impose the content-style disentanglement. In our experiments, MX-Font outperforms previous state-of-the-art FFG methods in the Chinese generation and cross-lingual, e.g., Chinese to Korean, generation. Source code is available at https://github.com/clovaai/mxfont.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Multiple Heads are Better than One: Few-shot Font Generation with Multiple Localized Experts<br>pdf: <a href="https://t.co/nPMMewh0z4">https://t.co/nPMMewh0z4</a><br>abs: <a href="https://t.co/zCzjP8Mync">https://t.co/zCzjP8Mync</a> <a href="https://t.co/4RZcn82aT8">pic.twitter.com/4RZcn82aT8</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1378912561379311617?ref_src=twsrc%5Etfw">April 5, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Information-constrained optimization: can adaptive processing of  gradients help?

Jayadev Acharya, Clément L. Canonne, Prathamesh Mayekar, Himanshu Tyagi

- retweets: 212, favorites: 93 (04/06/2021 09:19:02)

- links: [abs](https://arxiv.org/abs/2104.00979) | [pdf](https://arxiv.org/pdf/2104.00979)
- [math.OC](https://arxiv.org/list/math.OC/recent) | [cs.DS](https://arxiv.org/list/cs.DS/recent) | [cs.IT](https://arxiv.org/list/cs.IT/recent)

We revisit first-order optimization under local information constraints such as local privacy, gradient quantization, and computational constraints limiting access to a few coordinates of the gradient. In this setting, the optimization algorithm is not allowed to directly access the complete output of the gradient oracle, but only gets limited information about it subject to the local information constraints.   We study the role of adaptivity in processing the gradient output to obtain this limited information from it.We consider optimization for both convex and strongly convex functions and obtain tight or nearly tight lower bounds for the convergence rate, when adaptive gradient processing is allowed. Prior work was restricted to convex functions and allowed only nonadaptive processing of gradients. For both of these function classes and for the three information constraints mentioned above, our lower bound implies that adaptive processing of gradients cannot outperform nonadaptive processing in most regimes of interest. We complement these results by exhibiting a natural optimization problem under information constraints for which adaptive processing of gradient strictly outperforms nonadaptive processing.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Very happy about this paper on &quot;info-constrained optimisation,&quot; w/ <a href="https://twitter.com/AcharyaJayadev?ref_src=twsrc%5Etfw">@AcharyaJayadev</a>, <a href="https://twitter.com/prathameshM220?ref_src=twsrc%5Etfw">@prathameshM220</a>, and <a href="https://twitter.com/hstyagi?ref_src=twsrc%5Etfw">@hstyagi</a>: <a href="https://t.co/A9Okn2dvNH">https://t.co/A9Okn2dvNH</a><br>You want to minimise a function, but the first-order oracle can only provide you w/ some limited info about the gradient. E.g., 5 bits.<br><br> 1/3 <a href="https://t.co/qqQ0Dsk0d8">pic.twitter.com/qqQ0Dsk0d8</a></p>&mdash; Clément Canonne (@ccanonne_) <a href="https://twitter.com/ccanonne_/status/1378874168565923844?ref_src=twsrc%5Etfw">April 5, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. The Spatially-Correlative Loss for Various Image Translation Tasks

Chuanxia Zheng, Tat-Jen Cham, Jianfei Cai

- retweets: 196, favorites: 44 (04/06/2021 09:19:02)

- links: [abs](https://arxiv.org/abs/2104.00854) | [pdf](https://arxiv.org/pdf/2104.00854)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

We propose a novel spatially-correlative loss that is simple, efficient and yet effective for preserving scene structure consistency while supporting large appearance changes during unpaired image-to-image (I2I) translation. Previous methods attempt this by using pixel-level cycle-consistency or feature-level matching losses, but the domain-specific nature of these losses hinder translation across large domain gaps. To address this, we exploit the spatial patterns of self-similarity as a means of defining scene structure. Our spatially-correlative loss is geared towards only capturing spatial relationships within an image rather than domain appearance. We also introduce a new self-supervised learning method to explicitly learn spatially-correlative maps for each specific translation task. We show distinct improvement over baseline models in all three modes of unpaired I2I translation: single-modal, multi-modal, and even single-image translation. This new loss can easily be integrated into existing network architectures and thus allows wide applicability.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">The Spatially-Correlative Loss for Various Image Translation Tasks<br>pdf: <a href="https://t.co/Xt0WpdAJyX">https://t.co/Xt0WpdAJyX</a><br>abs: <a href="https://t.co/ip3oPcXq4B">https://t.co/ip3oPcXq4B</a> <a href="https://t.co/BgIqUstDl3">pic.twitter.com/BgIqUstDl3</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1378875440408973312?ref_src=twsrc%5Etfw">April 5, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. Language-based Video Editing via Multi-Modal Multi-Level Transformer

Tsu-Jui Fu, Xin Eric Wang, Scott T. Grafton, Miguel P. Eckstein, William Yang Wang

- retweets: 134, favorites: 61 (04/06/2021 09:19:03)

- links: [abs](https://arxiv.org/abs/2104.01122) | [pdf](https://arxiv.org/pdf/2104.01122)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Video editing tools are widely used nowadays for digital design. Although the demand for these tools is high, the prior knowledge required makes it difficult for novices to get started. Systems that could follow natural language instructions to perform automatic editing would significantly improve accessibility. This paper introduces the language-based video editing (LBVE) task, which allows the model to edit, guided by text instruction, a source video into a target video. LBVE contains two features: 1) the scenario of the source video is preserved instead of generating a completely different video; 2) the semantic is presented differently in the target video, and all changes are controlled by the given instruction. We propose a Multi-Modal Multi-Level Transformer (M$^3$L-Transformer) to carry out LBVE. The M$^3$L-Transformer dynamically learns the correspondence between video perception and language semantic at different levels, which benefits both the video understanding and video frame synthesis. We build three new datasets for evaluation, including two diagnostic and one from natural videos with human-labeled text. Extensive experimental results show that M$^3$L-Transformer is effective for video editing and that LBVE can lead to a new field toward vision-and-language research.

<blockquote class="twitter-tweet"><p lang="it" dir="ltr">Language-based Video Editing via Multi-Modal Multi-Level Transformer<br>pdf: <a href="https://t.co/APK6dVUCyO">https://t.co/APK6dVUCyO</a><br>abs: <a href="https://t.co/IGCqPC2zWH">https://t.co/IGCqPC2zWH</a> <a href="https://t.co/j9XOJQZCOb">pic.twitter.com/j9XOJQZCOb</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1378872267057299457?ref_src=twsrc%5Etfw">April 5, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference

Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, Matthijs Douze

- retweets: 82, favorites: 48 (04/06/2021 09:19:03)

- links: [abs](https://arxiv.org/abs/2104.01136) | [pdf](https://arxiv.org/pdf/2104.01136)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We re-evaluated principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers. As a result, we propose LeVIT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80\% ImageNet top-1 accuracy, LeViT is 3.3 times faster than EfficientNet on the CPU.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">LeViT: a Vision Transformer in ConvNet&#39;s Clothing for Faster Inference<br>pdf: <a href="https://t.co/dUNZwJBaw8">https://t.co/dUNZwJBaw8</a><br>abs: <a href="https://t.co/7EfboUXLpa">https://t.co/7EfboUXLpa</a><br><br>&quot;LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff&quot; <a href="https://t.co/9D2xiy6zfs">pic.twitter.com/9D2xiy6zfs</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1378872821124890627?ref_src=twsrc%5Etfw">April 5, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. TFill: Image Completion via a Transformer-Based Architecture

Chuanxia Zheng, Tat-Jen Cham, Jianfei Cai

- retweets: 72, favorites: 37 (04/06/2021 09:19:03)

- links: [abs](https://arxiv.org/abs/2104.00845) | [pdf](https://arxiv.org/pdf/2104.00845)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Bridging distant context interactions is important for high quality image completion with large masks. Previous methods attempting this via deep or large receptive field (RF) convolutions cannot escape from the dominance of nearby interactions, which may be inferior. In this paper, we propose treating image completion as a directionless sequence-to-sequence prediction task, and deploy a transformer to directly capture long-range dependence in the encoder in a first phase. Crucially, we employ a restrictive CNN with small and non-overlapping RF for token representation, which allows the transformer to explicitly model the long-range context relations with equal importance in all layers, without implicitly confounding neighboring tokens when larger RFs are used. In a second phase, to improve appearance consistency between visible and generated regions, a novel attention-aware layer (AAL) is introduced to better exploit distantly related features and also avoid the insular effect of standard attention. Overall, extensive experiments demonstrate superior performance compared to state-of-the-art methods on several datasets.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">TFill: Image Completion via a Transformer-Based Architecture<br>pdf: <a href="https://t.co/vyuxTgKHnn">https://t.co/vyuxTgKHnn</a><br>abs: <a href="https://t.co/yzrw0ancGd">https://t.co/yzrw0ancGd</a> <a href="https://t.co/Jd4SJ5XIr5">pic.twitter.com/Jd4SJ5XIr5</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1378874156893229059?ref_src=twsrc%5Etfw">April 5, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 10. Assem-VC: Realistic Voice Conversion by Assembling Modern Speech  Synthesis Techniques

Kang-wook Kim, Seung-won Park, Myun-chul Joe

- retweets: 38, favorites: 42 (04/06/2021 09:19:03)

- links: [abs](https://arxiv.org/abs/2104.00931) | [pdf](https://arxiv.org/pdf/2104.00931)
- [eess.AS](https://arxiv.org/list/eess.AS/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.SD](https://arxiv.org/list/cs.SD/recent)

In this paper, we pose the current state-of-the-art voice conversion (VC) systems as two-encoder-one-decoder models. After comparing these models, we combine the best features and propose Assem-VC, a new state-of-the-art any-to-many non-parallel VC system. This paper also introduces the GTA finetuning in VC, which significantly improves the quality and the speaker similarity of the outputs. Assem-VC outperforms the previous state-of-the-art approaches in both the naturalness and the speaker similarity on the VCTK dataset. As an objective result, the degree of speaker disentanglement of features such as phonetic posteriorgrams (PPG) is also explored. Our investigation indicates that many-to-many VC results are no longer distinct from human speech and similar quality can be achieved with any-to-many models. Audio samples are available at https://mindslab-ai.github.io/assem-vc/

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Assem-VC: Realistic Voice Conversion by Assembling Modern Speech Synthesis Techniques<br>pdf: <a href="https://t.co/HjZs7Mfe9F">https://t.co/HjZs7Mfe9F</a><br>abs: <a href="https://t.co/zM9BBRMtB5">https://t.co/zM9BBRMtB5</a><br>project page: <a href="https://t.co/1NrQmOSvRg">https://t.co/1NrQmOSvRg</a> <a href="https://t.co/NtczwN60uH">pic.twitter.com/NtczwN60uH</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1378885125849632771?ref_src=twsrc%5Etfw">April 5, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 11. Towards High Fidelity Face Relighting with Realistic Shadows

Andrew Hou, Ze Zhang, Michel Sarkis, Ning Bi, Yiying Tong, Xiaoming Liu

- retweets: 20, favorites: 41 (04/06/2021 09:19:03)

- links: [abs](https://arxiv.org/abs/2104.00825) | [pdf](https://arxiv.org/pdf/2104.00825)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Existing face relighting methods often struggle with two problems: maintaining the local facial details of the subject and accurately removing and synthesizing shadows in the relit image, especially hard shadows. We propose a novel deep face relighting method that addresses both problems. Our method learns to predict the ratio (quotient) image between a source image and the target image with the desired lighting, allowing us to relight the image while maintaining the local facial details. During training, our model also learns to accurately modify shadows by using estimated shadow masks to emphasize on the high-contrast shadow borders. Furthermore, we introduce a method to use the shadow mask to estimate the ambient light intensity in an image, and are thus able to leverage multiple datasets during training with different global lighting intensities. With quantitative and qualitative evaluations on the Multi-PIE and FFHQ datasets, we demonstrate that our proposed method faithfully maintains the local facial details of the subject and can accurately handle hard shadows while achieving state-of-the-art face relighting performance.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Towards High Fidelity Face Relighting with Realistic Shadows<br>pdf: <a href="https://t.co/8JdhkFvfRh">https://t.co/8JdhkFvfRh</a><br>abs: <a href="https://t.co/wj0fWszLJ5">https://t.co/wj0fWszLJ5</a> <a href="https://t.co/wpqMyfb9xx">pic.twitter.com/wpqMyfb9xx</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1378914495175741441?ref_src=twsrc%5Etfw">April 5, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



