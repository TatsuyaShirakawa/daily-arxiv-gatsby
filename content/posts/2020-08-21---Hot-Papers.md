---
title: Hot Papers 2020-08-21
date: 2020-08-22T22:53:07.Z
template: "post"
draft: false
slug: "hot-papers-2020-08-21"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-08-21"
socialImage: "/media/flying-marine.jpg"

---

# 1. Large Associative Memory Problem in Neurobiology and Machine Learning

Dmitry Krotov, John Hopfield

- retweets: 163, favorites: 549 (08/22/2020 22:53:07)

- links: [abs](https://arxiv.org/abs/2008.06996) | [pdf](https://arxiv.org/pdf/2008.06996)
- [q-bio.NC](https://arxiv.org/list/q-bio.NC/recent) | [cond-mat.dis-nn](https://arxiv.org/list/cond-mat.dis-nn/recent) | [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Dense Associative Memories or modern Hopfield networks permit storage and reliable retrieval of an exponentially large (in the dimension of feature space) number of memories. At the same time, their naive implementation is non-biological, since it seemingly requires the existence of many-body synaptic junctions between the neurons. We show that these models are effective descriptions of a more microscopic (written in terms of biological degrees of freedom) theory that has additional (hidden) neurons and only requires two-body interactions between them. For this reason our proposed microscopic theory is a valid model of large associative memory with a degree of biological plausibility. The dynamics of our network and its reduced dimensional equivalent both minimize energy (Lyapunov) functions. When certain dynamical variables (hidden neurons) are integrated out from our microscopic theory, one can recover many of the models that were previously discussed in the literature, e.g. the model presented in ''Hopfield Networks is All You Need'' paper. We also provide an alternative derivation of the energy function and the update rule proposed in the aforementioned paper and clarify the relationships between various models of this class.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Dense Associative Memories, aka modern Hopfield networks, have a huge memory storage capacity. But are they biologically realistic? In our new paper with <a href="https://twitter.com/DimaKrotov?ref_src=twsrc%5Etfw">@DimaKrotov</a> we argue that they can be written in terms of biological variables. <a href="https://t.co/w58EAQ54xm">https://t.co/w58EAQ54xm</a> <a href="https://t.co/dGp4npMRo2">pic.twitter.com/dGp4npMRo2</a></p>&mdash; John Hopfield (@HopfieldJohn) <a href="https://twitter.com/HopfieldJohn/status/1295741296133517313?ref_src=twsrc%5Etfw">August 18, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">New microscopic theory of Dense Associative Memory aka modern Hopfield network can be reduced to the model proposed in “Hopfield Networks is All You Need’’ paper, which is equivalent to self-attention mechanism of Transformers. Work with <a href="https://twitter.com/HopfieldJohn?ref_src=twsrc%5Etfw">@HopfieldJohn</a> <a href="https://t.co/jYAxIjvgKf">https://t.co/jYAxIjvgKf</a> <a href="https://t.co/212qRx9JrQ">pic.twitter.com/212qRx9JrQ</a></p>&mdash; Dmitry Krotov (@DimaKrotov) <a href="https://twitter.com/DimaKrotov/status/1295740398871117825?ref_src=twsrc%5Etfw">August 18, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Large Associative Memory Problem in Neurobiology and Machine Learning<br><br>Biological plausible explanation of “Hopfield Networks is All You Need” by Krotov and Hopfield<br><br>Paper: <a href="https://t.co/8k8as68S4E">https://t.co/8k8as68S4E</a><br><br>Discussion: <a href="https://t.co/DoEgQppEA3">https://t.co/DoEgQppEA3</a> <a href="https://t.co/tDWf4QgYpE">pic.twitter.com/tDWf4QgYpE</a></p>&mdash; hardmaru (@hardmaru) <a href="https://twitter.com/hardmaru/status/1295999451224391680?ref_src=twsrc%5Etfw">August 19, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Language Models as Knowledge Bases: On Entity Representations, Storage  Capacity, and Paraphrased Queries

Benjamin Heinzerling, Kentaro Inui

- retweets: 42, favorites: 213 (08/22/2020 22:53:08)

- links: [abs](https://arxiv.org/abs/2008.09036) | [pdf](https://arxiv.org/pdf/2008.09036)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

Pretrained language models have been suggested as a possible alternative or complement to structured knowledge bases. However, this emerging LM-as-KB paradigm has so far only been considered in a very limited setting, which only allows handling 21k entities whose single-token name is found in common LM vocabularies. Furthermore, the main benefit of this paradigm, namely querying the KB using a variety of natural language paraphrases, is underexplored so far. Here, we formulate two basic requirements for treating LMs as KBs: (i) the ability to store a large number facts involving a large number of entities and (ii) the ability to query stored facts. We explore three entity representations that allow LMs to represent millions of entities and present a detailed case study on paraphrased querying of world knowledge in LMs, thereby providing a proof-of-concept that language models can indeed serve as knowledge bases.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Can language models be used for knowledge bases?<br><br>This paper aims to provide a proof of concept with some discussions and potential requirements to achieve this.<a href="https://t.co/Tj6KonH2dB">https://t.co/Tj6KonH2dB</a> <a href="https://t.co/XpYJrkMsGI">pic.twitter.com/XpYJrkMsGI</a></p>&mdash; elvis (@omarsar0) <a href="https://twitter.com/omarsar0/status/1296736567235674112?ref_src=twsrc%5Etfw">August 21, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. PRINCIPIA: a Decentralized Peer-Review Ecosystem

Andrea Mambrini, Andrea Baronchelli, Michele Starnini, Daniele Marinazzo, Manlio De Domenico

- retweets: 52, favorites: 143 (08/22/2020 22:53:08)

- links: [abs](https://arxiv.org/abs/2008.09011) | [pdf](https://arxiv.org/pdf/2008.09011)
- [cs.DL](https://arxiv.org/list/cs.DL/recent) | [nlin.AO](https://arxiv.org/list/nlin.AO/recent) | [physics.soc-ph](https://arxiv.org/list/physics.soc-ph/recent)

Peer review is a cornerstone of modern scientific endeavor. However, there is growing consensus that several limitations of the current peer review system, from lack of incentives to reviewers to lack of transparency, risks to undermine its benefits. Here, we introduce the PRINCIPIA (http://www.principia.network/) framework for peer-review of scientific outputs (e.g., papers, grant proposals or patents). The framework allows key players of the scientific ecosystem -- including existing publishing groups -- to create and manage peer-reviewed journals, by building a free market for reviews and publications. PRINCIPIA's referees are transparently rewarded according to their efforts and the quality of their reviews. PRINCIPIA also naturally allows to recognize the prestige of users and journals, with an intrinsic reputation system that does not depend on third-parties. PRINCIPIA re-balances the power between researchers and publishers, stimulates valuable assessments from referees, favors a fair competition between journals, and reduces the costs to access research output and to publish.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Did you ever think, at least for 1s, that there is something wrong with the current peer-review and publishing system of research? If the answer is YES, then you might want to read this paper: <a href="https://t.co/MX1xEYqCdU">https://t.co/MX1xEYqCdU</a><br><br>Spoiler: <a href="https://twitter.com/hashtag/PRINCIPIA?src=hash&amp;ref_src=twsrc%5Etfw">#PRINCIPIA</a>, a peer-review market<br>Thread 1/n <a href="https://t.co/cLv4wOfXj8">pic.twitter.com/cLv4wOfXj8</a></p>&mdash; Manlio De Domenico (@manlius84) <a href="https://twitter.com/manlius84/status/1296725665216176130?ref_src=twsrc%5Etfw">August 21, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Neural Networks and Quantum Field Theory

James Halverson, Anindita Maiti, Keegan Stoner

- retweets: 40, favorites: 137 (08/22/2020 22:53:08)

- links: [abs](https://arxiv.org/abs/2008.08601) | [pdf](https://arxiv.org/pdf/2008.08601)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [hep-th](https://arxiv.org/list/hep-th/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

We propose a theoretical understanding of neural networks in terms of Wilsonian effective field theory. The correspondence relies on the fact that many asymptotic neural networks are drawn from Gaussian processes, the analog of non-interacting field theories. Moving away from the asymptotic limit yields a non-Gaussian process and corresponds to turning on particle interactions, allowing for the computation of correlation functions of neural network outputs with Feynman diagrams. Minimal non-Gaussian process likelihoods are determined by the most relevant non-Gaussian terms, according to the flow in their coefficients induced by the Wilsonian renormalization group. This yields a direct connection between overparameterization and simplicity of neural network likelihoods. Whether the coefficients are constants or functions may be understood in terms of GP limit symmetries, as expected from 't Hooft's technical naturalness. General theoretical calculations are matched to neural network experiments in the simplest class of models allowing the correspondence. Our formalism is valid for any of the many architectures that becomes a GP in an asymptotic limit, a property preserved under certain types of training.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">Neural Networks and Quantum Field Theory<a href="https://t.co/sGsiFakZfk">https://t.co/sGsiFakZfk</a><br>ニューラルネットワークをWilson流の有効場理論として理解しようという論文。ニューラルネットワークは漸近的にGauss過程で表せるということに着目し、Gauss過程⇔自由場理論、ニューラルネットワーク⇔有効場理論と対応付ける。</p>&mdash; (A,H,D) (@AHD21) <a href="https://twitter.com/AHD21/status/1296656089195245568?ref_src=twsrc%5Etfw">August 21, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I&#39;m very excited about the direction of research by <a href="https://twitter.com/jhhalverson?ref_src=twsrc%5Etfw">@jhhalverson</a> et al, revealing a correspondence between deep neural networks and Wilsonian Effective Field Theory:<a href="https://t.co/mYM08Vyqed">https://t.co/mYM08Vyqed</a></p>&mdash; Ryan Reece (@RyanDavidReece) <a href="https://twitter.com/RyanDavidReece/status/1296900879245664256?ref_src=twsrc%5Etfw">August 21, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Meta-Sim2: Unsupervised Learning of Scene Structure for Synthetic Data  Generation

Jeevan Devaranjan, Amlan Kar, Sanja Fidler

- retweets: 21, favorites: 134 (08/22/2020 22:53:08)

- links: [abs](https://arxiv.org/abs/2008.09092) | [pdf](https://arxiv.org/pdf/2008.09092)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.GR](https://arxiv.org/list/cs.GR/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [eess.IV](https://arxiv.org/list/eess.IV/recent)

Procedural models are being widely used to synthesize scenes for graphics, gaming, and to create (labeled) synthetic datasets for ML. In order to produce realistic and diverse scenes, a number of parameters governing the procedural models have to be carefully tuned by experts. These parameters control both the structure of scenes being generated (e.g. how many cars in the scene), as well as parameters which place objects in valid configurations. Meta-Sim aimed at automatically tuning parameters given a target collection of real images in an unsupervised way. In Meta-Sim2, we aim to learn the scene structure in addition to parameters, which is a challenging problem due to its discrete nature. Meta-Sim2 proceeds by learning to sequentially sample rule expansions from a given probabilistic scene grammar. Due to the discrete nature of the problem, we use Reinforcement Learning to train our model, and design a feature space divergence between our synthesized and target images that is key to successful training. Experiments on a real driving dataset show that, without any supervision, we can successfully learn to generate data that captures discrete structural statistics of objects, such as their frequency, in real images. We also show that this leads to downstream improvement in the performance of an object detector trained on our generated dataset as opposed to other baseline simulation methods. Project page: https://nv-tlabs.github.io/meta-sim-structure/.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Meta-Sim2: Unsupervised Learning of Scene Structure for Synthetic Data Generation<br>pdf: <a href="https://t.co/TODGyy210M">https://t.co/TODGyy210M</a><br>abs: <a href="https://t.co/LNMZOhTcBs">https://t.co/LNMZOhTcBs</a><br>project page: <a href="https://t.co/PP3yEdXr4w">https://t.co/PP3yEdXr4w</a> <a href="https://t.co/78DVzbUu6p">pic.twitter.com/78DVzbUu6p</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1296620281730138112?ref_src=twsrc%5Etfw">August 21, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. The effect of data encoding on the expressive power of variational  quantum machine learning models

Maria Schuld, Ryan Sweke, Johannes Jakob Meyer

- retweets: 24, favorites: 126 (08/22/2020 22:53:08)

- links: [abs](https://arxiv.org/abs/2008.08605) | [pdf](https://arxiv.org/pdf/2008.08605)
- [quant-ph](https://arxiv.org/list/quant-ph/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Quantum computers can be used for supervised learning by treating parametrised quantum circuits as models that map data inputs to predictions. While a lot of work has been done to investigate practical implications of this approach, many important theoretical properties of these models remain unknown. Here we investigate how the strategy with which data is encoded into the model influences the expressive power of parametrised quantum circuits as function approximators. We show that one can naturally write a quantum model as a partial Fourier series in the data, where the accessible frequencies are determined by the nature of the data encoding gates in the circuit. By repeating simple data encoding gates multiple times, quantum models can access increasingly rich frequency spectra. We show that there exist quantum models which can realise all possible sets of Fourier coefficients, and therefore, if the accessible frequency spectrum is asymptotically rich enough, such models are universal function approximators.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Very happy to share a new preprint from Maria Schuld, <a href="https://twitter.com/rndm_wlks?ref_src=twsrc%5Etfw">@rndm_wlks</a> and me: <a href="https://t.co/u6hC4GKmXY">https://t.co/u6hC4GKmXY</a><br>We show that quantum learning models will always output a Fourier series where the encoding gates specify the frequencies and the rest of the circuit specifies the weights.<br><br>See 👇 <a href="https://t.co/N9msqKgjWN">pic.twitter.com/N9msqKgjWN</a></p>&mdash; Johannes Jakob Meyer (@jj_xyz) <a href="https://twitter.com/jj_xyz/status/1296721098839601153?ref_src=twsrc%5Etfw">August 21, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">The expressive power of variational quantum machine<br>learning models - by <a href="https://twitter.com/XanaduAI?ref_src=twsrc%5Etfw">@XanaduAI</a>&#39;s Maria Schuld and <a href="https://twitter.com/rndm_wlks?ref_src=twsrc%5Etfw">@rndm_wlks</a>, <a href="https://twitter.com/jj_xyz?ref_src=twsrc%5Etfw">@jj_xyz</a>! Out now! :)<a href="https://t.co/Qn7PVXomSe">https://t.co/Qn7PVXomSe</a></p>&mdash; Christian Weedbrook (@_cweedbrook) <a href="https://twitter.com/_cweedbrook/status/1296632120639655937?ref_src=twsrc%5Etfw">August 21, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. A Survey on Text Simplification

Punardeep Sikka, Manmeet Singh, Allen Pink, Vijay Mago

- retweets: 34, favorites: 45 (08/22/2020 22:53:09)

- links: [abs](https://arxiv.org/abs/2008.08612) | [pdf](https://arxiv.org/pdf/2008.08612)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

Text Simplification (TS) aims to reduce the linguistic complexity of content to make it easier to understand. Research in TS has been of keen interest, especially as approaches to TS have shifted from manual, hand-crafted rules to automated simplification. This survey seeks to provide a comprehensive overview of TS, including a brief description of earlier approaches used, discussion of various aspects of simplification (lexical, semantic and syntactic), and latest techniques being utilized in the field. We note that the research in the field has clearly shifted towards utilizing deep learning techniques to perform TS, with a specific focus on developing solutions to combat the lack of data available for simplification. We also include a discussion of datasets and evaluations metrics commonly used, along with discussion of related fields within Natural Language Processing (NLP), like semantic similarity.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">A Survey on Text Simplification. <a href="https://twitter.com/hashtag/NLP?src=hash&amp;ref_src=twsrc%5Etfw">#NLP</a> <a href="https://twitter.com/hashtag/DataScience?src=hash&amp;ref_src=twsrc%5Etfw">#DataScience</a> <a href="https://twitter.com/hashtag/DeepLearning?src=hash&amp;ref_src=twsrc%5Etfw">#DeepLearning</a> <a href="https://twitter.com/hashtag/DataMining?src=hash&amp;ref_src=twsrc%5Etfw">#DataMining</a> <a href="https://twitter.com/hashtag/BigData?src=hash&amp;ref_src=twsrc%5Etfw">#BigData</a> <a href="https://twitter.com/hashtag/Analytics?src=hash&amp;ref_src=twsrc%5Etfw">#Analytics</a> <a href="https://twitter.com/hashtag/Python?src=hash&amp;ref_src=twsrc%5Etfw">#Python</a> <a href="https://twitter.com/hashtag/RStats?src=hash&amp;ref_src=twsrc%5Etfw">#RStats</a> <a href="https://twitter.com/hashtag/TensorFlow?src=hash&amp;ref_src=twsrc%5Etfw">#TensorFlow</a> <a href="https://twitter.com/hashtag/IoT?src=hash&amp;ref_src=twsrc%5Etfw">#IoT</a> <a href="https://twitter.com/hashtag/Java?src=hash&amp;ref_src=twsrc%5Etfw">#Java</a> <a href="https://twitter.com/hashtag/JavaScript?src=hash&amp;ref_src=twsrc%5Etfw">#JavaScript</a> <a href="https://twitter.com/hashtag/ReactJS?src=hash&amp;ref_src=twsrc%5Etfw">#ReactJS</a> <a href="https://twitter.com/hashtag/GoLang?src=hash&amp;ref_src=twsrc%5Etfw">#GoLang</a> <a href="https://twitter.com/hashtag/Serverless?src=hash&amp;ref_src=twsrc%5Etfw">#Serverless</a> <a href="https://twitter.com/hashtag/Linux?src=hash&amp;ref_src=twsrc%5Etfw">#Linux</a> <a href="https://twitter.com/hashtag/Cloud?src=hash&amp;ref_src=twsrc%5Etfw">#Cloud</a> <a href="https://twitter.com/hashtag/AI?src=hash&amp;ref_src=twsrc%5Etfw">#AI</a> <a href="https://twitter.com/hashtag/Programmer?src=hash&amp;ref_src=twsrc%5Etfw">#Programmer</a> <a href="https://twitter.com/hashtag/MachineLearning?src=hash&amp;ref_src=twsrc%5Etfw">#MachineLearning</a> <a href="https://twitter.com/hashtag/ArtificialIntelligence?src=hash&amp;ref_src=twsrc%5Etfw">#ArtificialIntelligence</a> <a href="https://twitter.com/hashtag/NLProc?src=hash&amp;ref_src=twsrc%5Etfw">#NLProc</a><a href="https://t.co/GiS1xy8hck">https://t.co/GiS1xy8hck</a> <a href="https://t.co/SSmwlpFWg4">pic.twitter.com/SSmwlpFWg4</a></p>&mdash; Marcus Borba (@marcusborba) <a href="https://twitter.com/marcusborba/status/1296916770004119552?ref_src=twsrc%5Etfw">August 21, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">A survey paper on text simplification.<br><br>&quot;Text Simplification (TS) aims to reduce the linguistic complexity of content to make it easier to understand.&quot;<br><br>This line of research has tremendous potential to build more accessible educational content online. <a href="https://t.co/Ls7vBOL2CP">https://t.co/Ls7vBOL2CP</a> <a href="https://t.co/VCIHrF6rKg">pic.twitter.com/VCIHrF6rKg</a></p>&mdash; elvis (@omarsar0) <a href="https://twitter.com/omarsar0/status/1296738413480226817?ref_src=twsrc%5Etfw">August 21, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



