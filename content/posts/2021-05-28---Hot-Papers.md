---
title: Hot Papers 2021-05-28
date: 2021-05-29T06:43:32.Z
template: "post"
draft: false
slug: "hot-papers-2021-05-28"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2021-05-28"
socialImage: "/media/flying-marine.jpg"

---

# 1. CogView: Mastering Text-to-Image Generation via Transformers

Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, Jie Tang

- retweets: 3826, favorites: 131 (05/29/2021 06:43:32)

- links: [abs](https://arxiv.org/abs/2105.13290) | [pdf](https://arxiv.org/pdf/2105.13290)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Text-to-Image generation in the general domain has long been an open problem, which requires both generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView (zero-shot) achieves a new state-of-the-art FID on blurred MS COCO, outperforms previous GAN-based models and a recent similar work DALL-E.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Interesting web demo for this state-of-the-art, text-to-image generative model (<a href="https://t.co/kIJUI617x8">https://t.co/kIJUI617x8</a>)<br><br>However, when I typed in the phrase &quot;Hong Kong&quot; (È¶ôÊ∏Ø), the system failed because &quot;the input text has illegal content.&quot; ü§î<br><br>Their web demo: <a href="https://t.co/XBERgMTI73">https://t.co/XBERgMTI73</a> <a href="https://t.co/1jO9BjEUsH">https://t.co/1jO9BjEUsH</a> <a href="https://t.co/Sdvpx9EJwP">pic.twitter.com/Sdvpx9EJwP</a></p>&mdash; hardmaru (@hardmaru) <a href="https://twitter.com/hardmaru/status/1398270316636381188?ref_src=twsrc%5Etfw">May 28, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">CogView: Mastering Text-to-Image Generation via Transformers<br>pdf: <a href="https://t.co/SOIqFhHleW">https://t.co/SOIqFhHleW</a><br>abs: <a href="https://t.co/nmhinD6kXK">https://t.co/nmhinD6kXK</a><br>demo: <a href="https://t.co/QULIeN628P">https://t.co/QULIeN628P</a><br><br>a 4-billion-parameter Transformer with VQ-VAE tokenizer, achieves a new SOTA FID on blurred MS COCO <a href="https://t.co/hvXheo5cYP">pic.twitter.com/hvXheo5cYP</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1398079706180763649?ref_src=twsrc%5Etfw">May 28, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. PyTouch: A Machine Learning Library for Touch Processing

Mike Lambeta, Huazhe Xu, Jingwei Xu, Po-Wei Chou, Shaoxiong Wang, Trevor Darrell, Roberto Calandra

- retweets: 2613, favorites: 307 (05/29/2021 06:43:32)

- links: [abs](https://arxiv.org/abs/2105.12791) | [pdf](https://arxiv.org/pdf/2105.12791)
- [cs.RO](https://arxiv.org/list/cs.RO/recent) | [cs.HC](https://arxiv.org/list/cs.HC/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

With the increased availability of rich tactile sensors, there is an equally proportional need for open-source and integrated software capable of efficiently and effectively processing raw touch measurements into high-level signals that can be used for control and decision-making. In this paper, we present PyTouch -- the first machine learning library dedicated to the processing of touch sensing signals. PyTouch, is designed to be modular, easy-to-use and provides state-of-the-art touch processing capabilities as a service with the goal of unifying the tactile sensing community by providing a library for building scalable, proven, and performance-validated modules over which applications and research can be built upon. We evaluate PyTouch on real-world data from several tactile sensors on touch processing tasks such as touch detection, slip and object pose estimations. PyTouch is open-sourced at https://github.com/facebookresearch/pytouch .

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">PyTouch: A Machine Learning Library for Touch Processing<br>pdf: <a href="https://t.co/La8xopY5Ce">https://t.co/La8xopY5Ce</a><br>abs: <a href="https://t.co/ltdASLt91N">https://t.co/ltdASLt91N</a><br>github (to be released): <a href="https://t.co/CNzlXGcigR">https://t.co/CNzlXGcigR</a><br><br>ML library dedicated to the processing of touch sensing signals <a href="https://t.co/Ry5uGhsHlQ">pic.twitter.com/Ry5uGhsHlQ</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1398092048960479236?ref_src=twsrc%5Etfw">May 28, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Stylizing 3D Scene via Implicit Representation and HyperNetwork

Pei-Ze Chiang, Meng-Shiun Tsai, Hung-Yu Tseng, Wei-sheng Lai, Wei-Chen Chiu

- retweets: 519, favorites: 126 (05/29/2021 06:43:32)

- links: [abs](https://arxiv.org/abs/2105.13016) | [pdf](https://arxiv.org/pdf/2105.13016)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

In this work, we aim to address the 3D scene stylization problem - generating stylized images of the scene at arbitrary novel view angles. A straightforward solution is to combine existing novel view synthesis and image/video style transfer approaches, which often leads to blurry results or inconsistent appearance. Inspired by the high quality results of the neural radiance fields (NeRF) method, we propose a joint framework to directly render novel views with the desired style. Our framework consists of two components: an implicit representation of the 3D scene with the neural radiance field model, and a hypernetwork to transfer the style information into the scene representation. In particular, our implicit representation model disentangles the scene into the geometry and appearance branches, and the hypernetwork learns to predict the parameters of the appearance branch from the reference style image. To alleviate the training difficulties and memory burden, we propose a two-stage training procedure and a patch sub-sampling approach to optimize the style and content losses with the neural radiance field model. After optimization, our model is able to render consistent novel views at arbitrary view angles with arbitrary style. Both quantitative evaluation and human subject study have demonstrated that the proposed method generates faithful stylization results with consistent appearance across different views.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Stylizing 3D Scene via Implicit Representation and HyperNetwork<br>pdf: <a href="https://t.co/lpp503Yfwc">https://t.co/lpp503Yfwc</a><br>abs: <a href="https://t.co/brXkdUViM9">https://t.co/brXkdUViM9</a><br>project page: <a href="https://t.co/ecYGirmpv4">https://t.co/ecYGirmpv4</a> <a href="https://t.co/A6PHgdI5Eb">pic.twitter.com/A6PHgdI5Eb</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1398078882776072194?ref_src=twsrc%5Etfw">May 28, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Stylizing 3D Scene via Implicit Representation and HyperNetwork <a href="https://t.co/F3bflWbgfl">https://t.co/F3bflWbgfl</a><br><br>Interesting use of hypernetworks and NeRF! <a href="https://twitter.com/drsrinathsridha?ref_src=twsrc%5Etfw">@drsrinathsridha</a> check this out<a href="https://twitter.com/hashtag/3d?src=hash&amp;ref_src=twsrc%5Etfw">#3d</a> <a href="https://twitter.com/hashtag/computervision?src=hash&amp;ref_src=twsrc%5Etfw">#computervision</a> <a href="https://t.co/7zOz8JAxC6">pic.twitter.com/7zOz8JAxC6</a></p>&mdash; Tomasz Malisiewicz (@quantombone) <a href="https://twitter.com/quantombone/status/1398307525548675075?ref_src=twsrc%5Etfw">May 28, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Drawing Multiple Augmentation Samples Per Image During Training  Efficiently Decreases Test Error

Stanislav Fort, Andrew Brock, Razvan Pascanu, Soham De, Samuel L. Smith

- retweets: 450, favorites: 161 (05/29/2021 06:43:33)

- links: [abs](https://arxiv.org/abs/2105.13343) | [pdf](https://arxiv.org/pdf/2105.13343)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent)

In computer vision, it is standard practice to draw a single sample from the data augmentation procedure for each unique image in the mini-batch, however it is not clear whether this choice is optimal for generalization. In this work, we provide a detailed empirical evaluation of how the number of augmentation samples per unique image influences performance on held out data. Remarkably, we find that drawing multiple samples per image consistently enhances the test accuracy achieved for both small and large batch training, despite reducing the number of unique training examples in each mini-batch. This benefit arises even when different augmentation multiplicities perform the same number of parameter updates and gradient evaluations. Our results suggest that, although the variance in the gradient estimate arising from subsampling the dataset has an implicit regularization benefit, the variance which arises from the data augmentation process harms test accuracy. By applying augmentation multiplicity to the recently proposed NFNet model family, we achieve a new ImageNet state of the art of 86.8$\%$ top-1 w/o extra data.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Drawing Multiple Augmentation Samples Per Image<br>During Training Efficiently Decreases Test Error<br><br>By applying augmentation multiplicity to the recently proposed NFNet model family, they achieved a new ImageNet SotA of 86.8% top-1 w/o extra data.<a href="https://t.co/UoEzw922lt">https://t.co/UoEzw922lt</a> <a href="https://t.co/XM7FGCg5yF">pic.twitter.com/XM7FGCg5yF</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1398079485447008259?ref_src=twsrc%5Etfw">May 28, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Drawing Multiple Augmentation Samples Per Image<br>During Training Efficiently Decreases Test Error<br>pdf: <a href="https://t.co/zwS8DT9Km0">https://t.co/zwS8DT9Km0</a><br>abs: <a href="https://t.co/8xxFUi2nay">https://t.co/8xxFUi2nay</a><br><br>ImageNet SOTA of 86.8% top-1 accuracy after just 34 epochs of training with an NFNet-F5 using the SAM optimizer <a href="https://t.co/AhDVnLSef4">pic.twitter.com/AhDVnLSef4</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1398129234040791040?ref_src=twsrc%5Etfw">May 28, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. On the Universality of Graph Neural Networks on Large Random Graphs

Nicolas Keriven, Alberto Bietti, Samuel Vaiter

- retweets: 100, favorites: 36 (05/29/2021 06:43:33)

- links: [abs](https://arxiv.org/abs/2105.13099) | [pdf](https://arxiv.org/pdf/2105.13099)
- [stat.ML](https://arxiv.org/list/stat.ML/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

We study the approximation power of Graph Neural Networks (GNNs) on latent position random graphs. In the large graph limit, GNNs are known to converge to certain "continuous" models known as c-GNNs, which directly enables a study of their approximation power on random graph models. In the absence of input node features however, just as GNNs are limited by the Weisfeiler-Lehman isomorphism test, c-GNNs will be severely limited on simple random graph models. For instance, they will fail to distinguish the communities of a well-separated Stochastic Block Model (SBM) with constant degree function. Thus, we consider recently proposed architectures that augment GNNs with unique node identifiers, sometimes referred to as Graph Wavelets Neural Networks (GWNNs). We study the convergence of GWNNs to their continuous counterpart (c-GWNNs) in the large random graph limit, under new conditions on the node identifiers. We then show that c-GWNNs are strictly more powerful than c-GNNs in the continuous limit, and prove their universality on several random graph models of interest, including most SBMs and a large class of random geometric graphs. Our results cover both permutation-invariant and permutation-equivariant architectures.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">New preprint!ü§ì &quot;On the Universality of GNNs on Large Random Graphs&quot; w/ <a href="https://twitter.com/albertobietti?ref_src=twsrc%5Etfw">@albertobietti</a> <a href="https://twitter.com/vaiter?ref_src=twsrc%5Etfw">@vaiter</a> <br><br>What can GNNs compute in the continuous limit? Are recent architectures more powerful than normal message-passing ones?<a href="https://t.co/YUBaGU3dMy">https://t.co/YUBaGU3dMy</a><br><br>(1/6) <a href="https://t.co/zbTMzsCncW">pic.twitter.com/zbTMzsCncW</a></p>&mdash; Nicolas Keriven (@n_keriven) <a href="https://twitter.com/n_keriven/status/1398186300545585153?ref_src=twsrc%5Etfw">May 28, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. Uncertainty-Aware Self-Supervised Target-Mass Grasping of Granular Foods

Kuniyuki Takahashi, Wilson Ko, Avinash Ummadisingu, Shin-ichi Maeda

- retweets: 70, favorites: 28 (05/29/2021 06:43:33)

- links: [abs](https://arxiv.org/abs/2105.12946) | [pdf](https://arxiv.org/pdf/2105.12946)
- [cs.RO](https://arxiv.org/list/cs.RO/recent)

Food packing industry workers typically pick a target amount of food by hand from a food tray and place them in containers. Since menus are diverse and change frequently, robots must adapt and learn to handle new foods in a short time-span. Learning to grasp a specific amount of granular food requires a large training dataset, which is challenging to collect reasonably quickly. In this study, we propose ways to reduce the necessary amount of training data by augmenting a deep neural network with models that estimate its uncertainty through self-supervised learning. To further reduce human effort, we devise a data collection system that automatically generates labels. We build on the idea that we can grasp sufficiently well if there is at least one low-uncertainty (high-confidence) grasp point among the various grasp point candidates. We evaluate the methods we propose in this work on a variety of granular foods -- coffee beans, rice, oatmeal and peanuts -- each of which has a different size, shape and material properties such as volumetric mass density or friction. For these foods, we show significantly improved grasp accuracy of user-specified target masses using smaller datasets by incorporating uncertainty.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">ICRA2021„ÅßÁô∫Ë°®‰∫àÂÆö„ÅÆÈ£üÂìÅ„ÅÆÂÆöÈáèÊääÊåÅ„ÅÆË´ñÊñáÔºåÂãïÁîªÔºåËß£Ë™¨Ë®ò‰∫ã„ÇíÂÖ¨Èñã„Åó„Åæ„Åó„ÅüÔºéÂ∞ëÈáè„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åó„ÅãÂèñÂæó„Åß„Åç„Å™„ÅÑÂ†¥ÂêàÔºåÊ∑±Â±§Â≠¶Áøí„ÅØ‰∏çÂÆâÂÆö„Åß„ÅôÔºéËá™Â∑±ÊïôÂ∏´„ÅÇ„ÇäÂ≠¶Áøí„Çí‰Ωø„Å£„Åü‰∏çÁ¢∫ÂÆüÊÄß„ÇíËÄÉÊÖÆ„Åô„Çã„Åì„Å®„ÅßÂÆâÂÆöÁöÑ„Å™ÁµêÊûú„ÇíÂá∫„ÅôÊñπÊ≥ï„ÇíÊèêÊ°à„Åó„Å¶„Åæ„ÅôÔºé<br>Ë´ñÊñáÔºö<a href="https://t.co/Gf1SUGR9GD">https://t.co/Gf1SUGR9GD</a><br>ÂãïÁîªÔºö<a href="https://t.co/SMic5HqcMk">https://t.co/SMic5HqcMk</a> <a href="https://t.co/Jrp8p7qtbk">https://t.co/Jrp8p7qtbk</a></p>&mdash; Kuniyuki Takahashi (@kuniyuki_taka) <a href="https://twitter.com/kuniyuki_taka/status/1398081549665374212?ref_src=twsrc%5Etfw">May 28, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. CoSQA: 20,000+ Web Queries for Code Search and Question Answering

Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong, Ke Xu, Daxin Jiang, Ming Zhou, Nan Duan

- retweets: 42, favorites: 27 (05/29/2021 06:43:33)

- links: [abs](https://arxiv.org/abs/2105.13239) | [pdf](https://arxiv.org/pdf/2105.13239)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.SE](https://arxiv.org/list/cs.SE/recent)

Finding codes given natural language query isb eneficial to the productivity of software developers. Future progress towards better semantic matching between query and code requires richer supervised training resources. To remedy this, we introduce the CoSQA dataset.It includes 20,604 labels for pairs of natural language queries and codes, each annotated by at least 3 human annotators. We further introduce a contrastive learning method dubbed CoCLR to enhance query-code matching, which works as a data augmenter to bring more artificially generated training instances. We show that evaluated on CodeXGLUE with the same CodeBERT model, training on CoSQA improves the accuracy of code question answering by 5.1%, and incorporating CoCLR brings a further improvement of 10.5%.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">CoSQA: 20,000+ Web Queries for Code Search and Question Answering<br>pdf: <a href="https://t.co/NiVRH9kxdo">https://t.co/NiVRH9kxdo</a><br>abs: <a href="https://t.co/bDb8WWgQ77">https://t.co/bDb8WWgQ77</a> <a href="https://t.co/8KYfC0panw">pic.twitter.com/8KYfC0panw</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1398089859525451781?ref_src=twsrc%5Etfw">May 28, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. Self-Supervised Bug Detection and Repair

Miltiadis Allamanis, Henry Jackson-Flux, Marc Brockschmidt

- retweets: 42, favorites: 25 (05/29/2021 06:43:33)

- links: [abs](https://arxiv.org/abs/2105.12787) | [pdf](https://arxiv.org/pdf/2105.12787)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.SE](https://arxiv.org/list/cs.SE/recent)

Machine learning-based program analyses have recently shown the promise of integrating formal and probabilistic reasoning towards aiding software development. However, in the absence of large annotated corpora, training these analyses is challenging. Towards addressing this, we present BugLab, an approach for self-supervised learning of bug detection and repair. BugLab co-trains two models: (1) a detector model that learns to detect and repair bugs in code, (2) a selector model that learns to create buggy code for the detector to use as training data. A Python implementation of BugLab improves by up to 30% upon baseline methods on a test dataset of 2374 real-life bugs and finds 19 previously unknown bugs in open-source software.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Self-Supervised Bug Detection and Repair<br>pdf: <a href="https://t.co/CWTMTauPQ6">https://t.co/CWTMTauPQ6</a><br>abs: <a href="https://t.co/IwQ0aULKp0">https://t.co/IwQ0aULKp0</a><br><br>improves by up to 30% upon baseline methods on a test dataset of 2374 real-life bugs and finds 19 previously unknown bugs in open-source software <a href="https://t.co/TPCaCrFoJu">pic.twitter.com/TPCaCrFoJu</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1398085101959979018?ref_src=twsrc%5Etfw">May 28, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. Corpus-Level Evaluation for Event QA: The IndiaPoliceEvents Corpus  Covering the 2002 Gujarat Violence

Andrew Halterman, Katherine A. Keith, Sheikh Muhammad Sarwar, Brendan O'Connor

- retweets: 44, favorites: 23 (05/29/2021 06:43:33)

- links: [abs](https://arxiv.org/abs/2105.12936) | [pdf](https://arxiv.org/pdf/2105.12936)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

Automated event extraction in social science applications often requires corpus-level evaluations: for example, aggregating text predictions across metadata and unbiased estimates of recall. We combine corpus-level evaluation requirements with a real-world, social science setting and introduce the IndiaPoliceEvents corpus--all 21,391 sentences from 1,257 English-language Times of India articles about events in the state of Gujarat during March 2002. Our trained annotators read and label every document for mentions of police activity events, allowing for unbiased recall evaluations. In contrast to other datasets with structured event representations, we gather annotations by posing natural questions, and evaluate off-the-shelf models for three different tasks: sentence classification, document ranking, and temporal aggregation of target events. We present baseline results from zero-shot BERT-based models fine-tuned on natural language inference and passage retrieval tasks. Our novel corpus-level evaluations and annotation approach can guide creation of similar social-science-oriented resources in the future.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Excited to announce <a href="https://twitter.com/ahalterman?ref_src=twsrc%5Etfw">@ahalterman</a> <a href="https://twitter.com/zzz2aaa?ref_src=twsrc%5Etfw">@zzz2aaa</a> <a href="https://twitter.com/brendan642?ref_src=twsrc%5Etfw">@brendan642</a>&#39;s and my paper &quot;Corpus-Level Evaluation for Event QA: The IndiaPoliceEvents Corpus Covering the 2002 Gujarat Violence&quot; is to be published in Findings of ACL 2021! <a href="https://t.co/aAR3KEJ7tE">https://t.co/aAR3KEJ7tE</a> 1/6 <a href="https://t.co/evKw2Qqiyf">pic.twitter.com/evKw2Qqiyf</a></p>&mdash; Katie Keith (@katakeith) <a href="https://twitter.com/katakeith/status/1398287383171194885?ref_src=twsrc%5Etfw">May 28, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 10. Beyond Algorithmic Bias: A Socio-Computational Interrogation of the  Google Search by Image Algorithm

Orestis Papakyriakopoulos, Arwa Michelle Mboya

- retweets: 30, favorites: 24 (05/29/2021 06:43:33)

- links: [abs](https://arxiv.org/abs/2105.12856) | [pdf](https://arxiv.org/pdf/2105.12856)
- [cs.CY](https://arxiv.org/list/cs.CY/recent)

We perform a socio-computational interrogation of the google search by image algorithm, a main component of the google search engine. We audit the algorithm by presenting it with more than 40 thousands faces of all ages and more than four races and collecting and analyzing the assigned labels with the appropriate statistical tools. We find that the algorithm reproduces white male patriarchal structures, often simplifying, stereotyping and discriminating females and non-white individuals, while providing more diverse and positive descriptions of white men. By drawing from Bourdieu's theory of cultural reproduction, we link these results to the attitudes of the algorithm's designers, owners, and the dataset the algorithm was trained on. We further underpin the problematic nature of the algorithm by using the ethnographic practice of studying-up: We show how the algorithm places individuals at the top of the tech industry within the socio-cultural reality that they shaped, many times creating biased representations of them. We claim that the use of social-theoretic frameworks such as the above are able to contribute to improved algorithmic accountability, algorithmic impact assessment and provide additional and more critical depth in algorithmic bias and auditing studies. Based on the analysis, we discuss the scientific and design implications and provide suggestions for alternative ways to design just socioalgorithmic systems.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">üåüproud to share the preprint co-authored with great <a href="https://twitter.com/RuMboya?ref_src=twsrc%5Etfw">@RuMboya</a>. We audited a part of the <a href="https://twitter.com/Google?ref_src=twsrc%5Etfw">@google</a> images search engine, and illustrate how the engine reproduces the problematic culture of white patriarchy. <a href="https://t.co/2FDCBfOlif">https://t.co/2FDCBfOlif</a> <a href="https://twitter.com/hashtag/bias?src=hash&amp;ref_src=twsrc%5Etfw">#bias</a> <a href="https://twitter.com/hashtag/aiethics?src=hash&amp;ref_src=twsrc%5Etfw">#aiethics</a></p>&mdash; Orestis Papakyriakopoulos (@SciOrestis) <a href="https://twitter.com/SciOrestis/status/1398287592408354817?ref_src=twsrc%5Etfw">May 28, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 11. A Computational Model of the Institutional Analysis and Development  Framework

Nieves Montes

- retweets: 36, favorites: 16 (05/29/2021 06:43:34)

- links: [abs](https://arxiv.org/abs/2105.13151) | [pdf](https://arxiv.org/pdf/2105.13151)
- [cs.AI](https://arxiv.org/list/cs.AI/recent) | [econ.TH](https://arxiv.org/list/econ.TH/recent)

The Institutional Analysis and Development (IAD) framework is a conceptual toolbox put forward by Elinor Ostrom and colleagues in an effort to identify and delineate the universal common variables that structure the immense variety of human interactions. The framework identifies rules as one of the core concepts to determine the structure of interactions, and acknowledges their potential to steer a community towards more beneficial and socially desirable outcomes. This work presents the first attempt to turn the IAD framework into a computational model to allow communities of agents to formally perform what-if analysis on a given rule configuration. To do so, we define the Action Situation Language -- or ASL -- whose syntax is hgighly tailored to the components of the IAD framework and that we use to write descriptions of social interactions. ASL is complemented by a game engine that generates its semantics as an extensive-form game. These models, then, can be analyzed with the standard tools of game theory to predict which outcomes are being most incentivized, and evaluated according to their socially relevant properties.



