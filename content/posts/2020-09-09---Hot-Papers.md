---
title: Hot Papers 2020-09-09
date: 2020-09-10T11:36:02.Z
template: "post"
draft: false
slug: "hot-papers-2020-09-09"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-09-09"
socialImage: "/media/flying-marine.jpg"

---

# 1. Generative Language Modeling for Automated Theorem Proving

Stanislas Polu, Ilya Sutskever

- retweets: 242, favorites: 1126 (09/10/2020 11:36:02)

- links: [abs](https://arxiv.org/abs/2009.03393) | [pdf](https://arxiv.org/pdf/2009.03393)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.CL](https://arxiv.org/list/cs.CL/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

We explore the application of transformer-based language models to automated theorem proving. This work is motivated by the possibility that a major limitation of automated theorem provers compared to humans -- the generation of original mathematical terms -- might be addressable via generation from language models. We present an automated prover and proof assistant, GPT-f, for the Metamath formalization language, and analyze its performance. GPT-f found new short proofs that were accepted into the main Metamath library, which is to our knowledge, the first time a deep-learning based system has contributed proofs that were adopted by a formal mathematics community.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Posted my first paper on arXivğŸ’¥ğŸ™Œ<br><br>GPT-f is a Transformer-based automated theorem prover. We show that Transformer + Search is suitable to formal reasoning and continuous self-improvement ğŸ¦¾<a href="https://t.co/VllDcCV3Kc">https://t.co/VllDcCV3Kc</a> <a href="https://t.co/5ttVX0MNBC">pic.twitter.com/5ttVX0MNBC</a></p>&mdash; Stanislas Polu (@spolu) <a href="https://twitter.com/spolu/status/1303578985276887042?ref_src=twsrc%5Etfw">September 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Generative Language Modeling for Automated<br>Theorem Proving<br><br>GPT-f, a Transformer LM pretrained on arXiv and trained through continuous self improvement for theorem proving, finds novel short proofs.  <a href="https://t.co/NKIY7WgVRm">https://t.co/NKIY7WgVRm</a> <a href="https://t.co/iYzqnHAddJ">pic.twitter.com/iYzqnHAddJ</a></p>&mdash; Aran Komatsuzaki (@arankomatsuzaki) <a href="https://twitter.com/arankomatsuzaki/status/1303498028154671104?ref_src=twsrc%5Etfw">September 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">GPT-fã¯Transformerã‚’ä½¿ã£ãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã§è‡ªå‹•å®šç†è¨¼æ˜ã‚’è¡Œã†ã€‚ã‚¦ã‚§ãƒ–åé›†ã•ã‚ŒãŸæ•°å­¦é–¢é€£ã®è«–æ–‡ãªã©ã§äº‹å‰å­¦ç¿’ã—ã€è¨¼æ˜ç›®æ¨™ã§æ¡ä»¶ä»˜ã—ãŸè¨¼æ˜çµŒè·¯ã®ç”Ÿæˆç¢ºç‡ã‚’ä½¿ã£ã¦æ¢ç´¢ã™ã‚‹ã€‚ç™ºè¦‹ã•ã‚ŒãŸä¸­ã§23å€‹ã®è¨¼æ˜ã¯ã“ã‚Œã¾ã§ã®è¨¼æ˜ã‚ˆã‚ŠçŸ­ãã‚ã‹ã‚Šã‚„ã™ãMetamathè¨¼æ˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«åŠ ãˆã‚‰ã‚ŒãŸ<a href="https://t.co/RiMM5QGQQB">https://t.co/RiMM5QGQQB</a></p>&mdash; Daisuke Okanohara (@hillbig) <a href="https://twitter.com/hillbig/status/1303836941356425217?ref_src=twsrc%5Etfw">September 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">OpenAIã®AIç ”ç©¶ï¼ˆGPT-fï¼‰<br><br>è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦è‡ªå‹•å®šç†è¨¼æ˜ã€‚GPT-fãŒã€Metamathãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«å—ã‘å…¥ã‚Œã‚‰ã‚ŒãŸæ–°ã—ã„çŸ­ã„è¨¼æ˜ã‚’ç™ºè¦‹ã€‚ã“ã‚Œã¯ã€ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãŒãŠãã‚‰ãåˆã‚ã¦æ­£å¼ãªæ•°å­¦ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«æ¡ç”¨ã•ã‚ŒãŸè¨¼æ˜ã‚’æä¾›ã—ãŸã€‚ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¯å½¢å¼æ¨è«–ã«ã‚‚é©ã—ã¦ã‚‹<a href="https://t.co/z2PMMgyoLJ">https://t.co/z2PMMgyoLJ</a></p>&mdash; å°çŒ«éŠã‚Šã‚‡ã†ï¼ˆãŸã‹ã«ã‚ƒã—ãƒ»ã‚Šã‚‡ã†ï¼‰ (@jaguring1) <a href="https://twitter.com/jaguring1/status/1303655947231875072?ref_src=twsrc%5Etfw">September 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Generative Language Modeling for Automated Theorem Proving<br>pdf: <a href="https://t.co/gmD9HK1R80">https://t.co/gmD9HK1R80</a><br>abs: <a href="https://t.co/qstcD7UYyZ">https://t.co/qstcD7UYyZ</a> <a href="https://t.co/nKKECgFhpT">pic.twitter.com/nKKECgFhpT</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1303495393368571906?ref_src=twsrc%5Etfw">September 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">OpenAIã®GPTç ”ç©¶ (pdf)<br>â‘ GPTï¼ˆ2018/6/11ï¼‰<a href="https://t.co/zJoazUBUez">https://t.co/zJoazUBUez</a>â€¦â€¦â€¦<br><br>â‘¡GPT-2ï¼ˆ2019/2/14ï¼‰<a href="https://t.co/gVN6kkxAHX">https://t.co/gVN6kkxAHX</a>â€¦â€¦â€¦<br><br>â‘¢GPT-3ï¼ˆ2020/5/28ï¼‰<a href="https://t.co/4qlMV7VDoO">https://t.co/4qlMV7VDoO</a>â€¦â€¦<br><br>â‘£image GPTï¼ˆ2020/6/17ï¼‰<a href="https://t.co/2LOWDz78LV">https://t.co/2LOWDz78LV</a><br><br>â‘¤GPT-fï¼ˆ2020/9/7ï¼‰<a href="https://t.co/sNWyPweIoJ">https://t.co/sNWyPweIoJ</a></p>&mdash; å°çŒ«éŠã‚Šã‚‡ã†ï¼ˆãŸã‹ã«ã‚ƒã—ãƒ»ã‚Šã‚‡ã†ï¼‰ (@jaguring1) <a href="https://twitter.com/jaguring1/status/1303665695712931840?ref_src=twsrc%5Etfw">September 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Compute is eating AI. <a href="https://twitter.com/OpenAI?ref_src=twsrc%5Etfw">@OpenAI</a> trains a state of the art theorem prover using a GPT architecture. I was waiting for this to happen, neat!<br><br>Though for real...wtf is this proof? Maybe the gains were just that you need an AI to understand this language haha!<a href="https://t.co/VXV4gWZcNK">https://t.co/VXV4gWZcNK</a> <a href="https://t.co/cNpqNiZmcc">pic.twitter.com/cNpqNiZmcc</a></p>&mdash; Connor Leahy (@NPCollapse) <a href="https://twitter.com/NPCollapse/status/1303620454251364352?ref_src=twsrc%5Etfw">September 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Randomness Concerns When Deploying Differential Privacy

Simson L. Garfinkel, Philip Leclerc

- retweets: 37, favorites: 136 (09/10/2020 11:36:04)

- links: [abs](https://arxiv.org/abs/2009.03777) | [pdf](https://arxiv.org/pdf/2009.03777)
- [cs.CR](https://arxiv.org/list/cs.CR/recent) | [cs.CY](https://arxiv.org/list/cs.CY/recent)

The U.S. Census Bureau is using differential privacy (DP) to protect confidential respondent data collected for the 2020 Decennial Census of Population & Housing. The Census Bureau's DP system is implemented in the Disclosure Avoidance System (DAS) and requires a source of random numbers. We estimate that the 2020 Census will require roughly 90TB of random bytes to protect the person and household tables. Although there are critical differences between cryptography and DP, they have similar requirements for randomness. We review the history of random number generation on deterministic computers, including von Neumann's "middle-square" method, Mersenne Twister (MT19937) (previously the default NumPy random number generator, which we conclude is unacceptable for use in production privacy-preserving systems), and the Linux /dev/urandom device. We also review hardware random number generator schemes, including the use of so-called "Lava Lamps" and the Intel Secure Key RDRAND instruction. We finally present our plan for generating random bits in the Amazon Web Services (AWS) environment using AES-CTR-DRBG seeded by mixing bits from /dev/urandom and the Intel Secure Key RDSEED instruction, a compromise of our desire to rely on a trusted hardware implementation, the unease of our external reviewers in trusting a hardware-only implementation, and the need to generate so many random bits.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">This paper contains the &quot;philosophy of differential privacy&quot; stuff I&#39;ve been wanting to get out, as well as a fun history of PRNGs and it even tells you what we&#39;re doing for the 2020 Census! <a href="https://t.co/VGUlIkAWBh">https://t.co/VGUlIkAWBh</a></p>&mdash; Simson Garfinkel (@xchatty) <a href="https://twitter.com/xchatty/status/1303502209330614272?ref_src=twsrc%5Etfw">September 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">(Thread)<br>Some highlights from this paper by Garfinkel (<a href="https://twitter.com/xchatty?ref_src=twsrc%5Etfw">@xchatty</a>) and Leclerc: <a href="https://t.co/vYgfyt8nkL">https://t.co/vYgfyt8nkL</a>.<br><br>First, you WON&#39;T BELIEVE the amount of randomness required! While the raw data for the 2020 US Census takes only &quot;a few tens of [GB],&quot; *90 TB* of random data will be used! 1/n <a href="https://t.co/nRp85gGQFb">pic.twitter.com/nRp85gGQFb</a></p>&mdash; Gautam Kamath (@thegautamkamath) <a href="https://twitter.com/thegautamkamath/status/1303748300097703936?ref_src=twsrc%5Etfw">September 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Robust Conversational AI with Grounded Text Generation

Jianfeng Gao, Baolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayandeh, Lars Liden, Heung-Yeung Shum

- retweets: 16, favorites: 50 (09/10/2020 11:36:04)

- links: [abs](https://arxiv.org/abs/2009.03457) | [pdf](https://arxiv.org/pdf/2009.03457)
- [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.CL](https://arxiv.org/list/cs.CL/recent)

This article presents a hybrid approach based on a Grounded Text Generation (GTG) model to building robust task bots at scale. GTG is a hybrid model which uses a large-scale Transformer neural network as its backbone, combined with symbol-manipulation modules for knowledge base inference and prior knowledge encoding, to generate responses grounded in dialog belief state and real-world knowledge for task completion. GTG is pre-trained on large amounts of raw text and human conversational data, and can be fine-tuned to complete a wide range of tasks.   The hybrid approach and its variants are being developed simultaneously by multiple research teams. The primary results reported on task-oriented dialog benchmarks are very promising, demonstrating the big potential of this approach. This article provides an overview of this progress and discusses related methods and technologies that can be incorporated for building robust conversational AI systems.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">A method to improve conversational AI systems through a hybrid approach for grounded text generation.<br><br>It combines Transformers and modules that encode prior knowledge &amp; knowledge base inference which helps to generate fluent &amp; on-topic responses.<a href="https://t.co/Sk7QsoJNgq">https://t.co/Sk7QsoJNgq</a> <a href="https://t.co/5CLWISoQpo">pic.twitter.com/5CLWISoQpo</a></p>&mdash; elvis (@omarsar0) <a href="https://twitter.com/omarsar0/status/1303641799706079232?ref_src=twsrc%5Etfw">September 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Robust Conversational AI with Grounded Text Generation<br>pdf: <a href="https://t.co/Okofj9kWcC">https://t.co/Okofj9kWcC</a><br>abs: <a href="https://t.co/AFYHal00FG">https://t.co/AFYHal00FG</a> <a href="https://t.co/06U29v10n3">pic.twitter.com/06U29v10n3</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1303497149586509824?ref_src=twsrc%5Etfw">September 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Deep Cyclic Generative Adversarial Residual Convolutional Networks for  Real Image Super-Resolution

Rao Muhammad Umer, Christian Micheloni

- retweets: 16, favorites: 36 (09/10/2020 11:36:04)

- links: [abs](https://arxiv.org/abs/2009.03693) | [pdf](https://arxiv.org/pdf/2009.03693)
- [eess.IV](https://arxiv.org/list/eess.IV/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent)

Recent deep learning based single image super-resolution (SISR) methods mostly train their models in a clean data domain where the low-resolution (LR) and the high-resolution (HR) images come from noise-free settings (same domain) due to the bicubic down-sampling assumption. However, such degradation process is not available in real-world settings. We consider a deep cyclic network structure to maintain the domain consistency between the LR and HR data distributions, which is inspired by the recent success of CycleGAN in the image-to-image translation applications. We propose the Super-Resolution Residual Cyclic Generative Adversarial Network (SRResCycGAN) by training with a generative adversarial network (GAN) framework for the LR to HR domain translation in an end-to-end manner. We demonstrate our proposed approach in the quantitative and qualitative experiments that generalize well to the real image super-resolution and it is easy to deploy for the mobile/embedded devices. In addition, our SR results on the AIM 2020 Real Image SR Challenge datasets demonstrate that the proposed SR approach achieves comparable results as the other state-of-art methods.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Deep Cyclic Generative Adversarial Residual Convolutional Networks for Real Image Super-Resolution<br>pdf: <a href="https://t.co/GhYBfgMctN">https://t.co/GhYBfgMctN</a><br>abs: <a href="https://t.co/BNXeKciPAn">https://t.co/BNXeKciPAn</a><br>github: <a href="https://t.co/rRYf5BbJ3m">https://t.co/rRYf5BbJ3m</a> <a href="https://t.co/CZkE70OPqr">pic.twitter.com/CZkE70OPqr</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1303499256679075841?ref_src=twsrc%5Etfw">September 9, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



