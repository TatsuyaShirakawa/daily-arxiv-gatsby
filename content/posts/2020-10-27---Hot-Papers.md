---
title: Hot Papers 2020-10-27
date: 2020-10-28T10:49:15.Z
template: "post"
draft: false
slug: "hot-papers-2020-10-27"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-10-27"
socialImage: "/media/flying-marine.jpg"

---

# 1. Simplifying Hamiltonian and Lagrangian Neural Networks via Explicit  Constraints

Marc Finzi, Ke Alexander Wang, Andrew Gordon Wilson

- retweets: 1482, favorites: 235 (10/28/2020 10:49:15)

- links: [abs](https://arxiv.org/abs/2010.13581) | [pdf](https://arxiv.org/pdf/2010.13581)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [math.DS](https://arxiv.org/list/math.DS/recent) | [physics.comp-ph](https://arxiv.org/list/physics.comp-ph/recent) | [physics.data-an](https://arxiv.org/list/physics.data-an/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Reasoning about the physical world requires models that are endowed with the right inductive biases to learn the underlying dynamics. Recent works improve generalization for predicting trajectories by learning the Hamiltonian or Lagrangian of a system rather than the differential equations directly. While these methods encode the constraints of the systems using generalized coordinates, we show that embedding the system into Cartesian coordinates and enforcing the constraints explicitly with Lagrange multipliers dramatically simplifies the learning problem. We introduce a series of challenging chaotic and extended-body systems, including systems with N-pendulums, spring coupling, magnetic fields, rigid rotors, and gyroscopes, to push the limits of current approaches. Our experiments show that Cartesian coordinates with explicit constraints lead to a 100x improvement in accuracy and data efficiency.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">We can greatly simplify Hamiltonian and Lagrangian neural nets by working in Cartesian coordinates with explicit constraints, leading to dramatic performance improvements! Our <a href="https://twitter.com/hashtag/NeurIPS2020?src=hash&amp;ref_src=twsrc%5Etfw">#NeurIPS2020</a> paper: <a href="https://t.co/G3geBSxlSU">https://t.co/G3geBSxlSU</a><br>with <a href="https://twitter.com/m_finzi?ref_src=twsrc%5Etfw">@m_finzi</a>, <a href="https://twitter.com/KAlexanderWang?ref_src=twsrc%5Etfw">@KAlexanderWang</a>. 1/5 <a href="https://t.co/5VG6XX9wUo">pic.twitter.com/5VG6XX9wUo</a></p>&mdash; Andrew Gordon Wilson (@andrewgwils) <a href="https://twitter.com/andrewgwils/status/1320900424820510720?ref_src=twsrc%5Etfw">October 27, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Attention is All You Need in Speech Separation

Cem Subakan, Mirco Ravanelli, Samuele Cornell, Mirko Bronzi, Jianyuan Zhong

- retweets: 1042, favorites: 162 (10/28/2020 10:49:16)

- links: [abs](https://arxiv.org/abs/2010.13154) | [pdf](https://arxiv.org/pdf/2010.13154)
- [eess.AS](https://arxiv.org/list/eess.AS/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.SD](https://arxiv.org/list/cs.SD/recent) | [eess.SP](https://arxiv.org/list/eess.SP/recent)

Recurrent Neural Networks (RNNs) have long been the dominant architecture in sequence-to-sequence learning. RNNs, however, are inherently sequential models that do not allow parallelization of their computations. Transformers are emerging as a natural alternative to standard RNNs, replacing recurrent computations with a multi-head attention mechanism. In this paper, we propose the `SepFormer', a novel RNN-free Transformer-based neural network for speech separation. The SepFormer learns short and long-term dependencies with a multi-scale approach that employs transformers. The proposed model matches or overtakes the state-of-the-art (SOTA) performance on the standard WSJ0-2/3mix datasets. It indeed achieves an SI-SNRi of 20.2 dB on WSJ0-2mix matching the SOTA, and an SI-SNRi of 17.6 dB on WSJ0-3mix, a SOTA result. The SepFormer inherits the parallelization advantages of Transformers and achieves a competitive performance even when downsampling the encoded representation by a factor of 8. It is thus significantly faster and it is less memory-demanding than the latest RNN-based systems.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Happy to announce our SepFormer! It is a novel RNN-free <a href="https://twitter.com/hashtag/Transformers?src=hash&amp;ref_src=twsrc%5Etfw">#Transformers</a> for <a href="https://twitter.com/hashtag/speech?src=hash&amp;ref_src=twsrc%5Etfw">#speech</a> separation. It matches or overtakes the SOTA on the WSJ0-2/3mix. It is faster and less memory-demanding than <a href="https://twitter.com/hashtag/RNN?src=hash&amp;ref_src=twsrc%5Etfw">#RNN</a> systems.<a href="https://t.co/QXXXaJenMC">https://t.co/QXXXaJenMC</a><a href="https://twitter.com/hashtag/DeepLearning?src=hash&amp;ref_src=twsrc%5Etfw">#DeepLearning</a> <a href="https://twitter.com/hashtag/SpeechBrain?src=hash&amp;ref_src=twsrc%5Etfw">#SpeechBrain</a> <a href="https://twitter.com/hashtag/AI?src=hash&amp;ref_src=twsrc%5Etfw">#AI</a> <a href="https://twitter.com/MILAMontreal?ref_src=twsrc%5Etfw">@MILAMontreal</a> <a href="https://t.co/JOHXbanLhb">pic.twitter.com/JOHXbanLhb</a></p>&mdash; Mirco Ravanelli (@mirco_ravanelli) <a href="https://twitter.com/mirco_ravanelli/status/1321099555555971072?ref_src=twsrc%5Etfw">October 27, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Attention is All You Need in Speech Separation<br>pdf: <a href="https://t.co/IduWOIECdp">https://t.co/IduWOIECdp</a><br>abs: <a href="https://t.co/9USbAXn3US">https://t.co/9USbAXn3US</a> <a href="https://t.co/WLIM8ghg1X">pic.twitter.com/WLIM8ghg1X</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1320902933144391680?ref_src=twsrc%5Etfw">October 27, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Exemplary Natural Images Explain CNN Activations Better than Feature  Visualizations

Judy Borowski, Roland S. Zimmermann, Judith Schepers, Robert Geirhos, Thomas S. A. Wallis, Matthias Bethge, Wieland Brendel

- retweets: 683, favorites: 108 (10/28/2020 10:49:16)

- links: [abs](https://arxiv.org/abs/2010.12606) | [pdf](https://arxiv.org/pdf/2010.12606)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.HC](https://arxiv.org/list/cs.HC/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Feature visualizations such as synthetic maximally activating images are a widely used explanation method to better understand the information processing of convolutional neural networks (CNNs). At the same time, there are concerns that these visualizations might not accurately represent CNNs' inner workings. Here, we measure how much extremely activating images help humans to predict CNN activations. Using a well-controlled psychophysical paradigm, we compare the informativeness of synthetic images (Olah et al., 2017) with a simple baseline visualization, namely exemplary natural images that also strongly activate a specific feature map. Given either synthetic or natural reference images, human participants choose which of two query images leads to strong positive activation. The experiment is designed to maximize participants' performance, and is the first to probe intermediate instead of final layer representations. We find that synthetic images indeed provide helpful information about feature map activations (82% accuracy; chance would be 50%). However, natural images-originally intended to be a baseline-outperform synthetic images by a wide margin (92% accuracy). Additionally, participants are faster and more confident for natural images, whereas subjective impressions about the interpretability of feature visualization are mixed. The higher informativeness of natural images holds across most layers, for both expert and lay participants as well as for hand- and randomly-picked feature visualizations. Even if only a single reference image is given, synthetic images provide less information than natural images (65% vs. 73%). In summary, popular synthetic images from feature visualizations are significantly less informative for assessing CNN activations than natural images. We argue that future visualization methods should improve over this simple baseline.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">We tested whether feature visualizations (from <a href="https://twitter.com/ch402?ref_src=twsrc%5Etfw">@ch402</a> et al.) really help humans understand CNNs. Our surprising finding: while they do help, they are outperformed by a very simple baseline - natural reference images from the dataset! <br>Paper @ <a href="https://t.co/xxFQtNXltX">https://t.co/xxFQtNXltX</a> (1/N) <a href="https://t.co/GMQEVMBK2O">pic.twitter.com/GMQEVMBK2O</a></p>&mdash; Bethge Lab (@bethgelab) <a href="https://twitter.com/bethgelab/status/1321018663705653248?ref_src=twsrc%5Etfw">October 27, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement  Learning

Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, Ofir Nachum

- retweets: 501, favorites: 223 (10/28/2020 10:49:16)

- links: [abs](https://arxiv.org/abs/2010.13611) | [pdf](https://arxiv.org/pdf/2010.13611)
- [cs.LG](https://arxiv.org/list/cs.LG/recent)

Reinforcement learning (RL) has achieved impressive performance in a variety of online settings in which an agent's ability to query the environment for transitions and rewards is effectively unlimited. However, in many practical applications, the situation is reversed: an agent may have access to large amounts of undirected offline experience data, while access to the online environment is severely limited. In this work, we focus on this offline setting. Our main insight is that, when presented with offline data composed of a variety of behaviors, an effective way to leverage this data is to extract a continuous space of recurring and temporally extended primitive behaviors before using these primitives for downstream task learning. Primitives extracted in this way serve two purposes: they delineate the behaviors that are supported by the data from those that are not, making them useful for avoiding distributional shift in offline RL; and they provide a degree of temporal abstraction, which reduces the effective horizon yielding better learning in theory, and improved offline RL in practice. In addition to benefiting offline policy optimization, we show that performing offline primitive learning in this way can also be leveraged for improving few-shot imitation learning as well as exploration and transfer in online RL on a variety of benchmark domains. Visualizations are available at https://sites.google.com/view/opal-iclr

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Hierarchy + offline RL are a natural combination. With OPAL, we extract primitives from offline data, then use them as actions for conservative Q-learning, leading to substantially better results!<br><br>w/ Anurag Ajay, Aviral Kumar, <a href="https://twitter.com/pulkitology?ref_src=twsrc%5Etfw">@pulkitology</a>, <a href="https://twitter.com/ofirnachum?ref_src=twsrc%5Etfw">@ofirnachum</a> <a href="https://t.co/ip39RyLfz7">https://t.co/ip39RyLfz7</a> <a href="https://t.co/7CK890OGJk">pic.twitter.com/7CK890OGJk</a></p>&mdash; Sergey Levine (@svlevine) <a href="https://twitter.com/svlevine/status/1321133085786656770?ref_src=twsrc%5Etfw">October 27, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">The future of offline RL is unsupervised learning on large datasets. In this work (<a href="https://t.co/lQBG9qEsyB">https://t.co/lQBG9qEsyB</a>) we show that using a simple autoencoding objective on undirected experience dataset can dramatically improve perf for offline/online RL, imitation/transfer learning etc. 1/ <a href="https://t.co/OTIvqioKVV">pic.twitter.com/OTIvqioKVV</a></p>&mdash; Ofir Nachum (@ofirnachum) <a href="https://twitter.com/ofirnachum/status/1321110818889478145?ref_src=twsrc%5Etfw">October 27, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Lightning-Fast Gravitational Wave Parameter Inference through Neural  Amortization

Arnaud Delaunoy, Antoine Wehenkel, Tanja Hinderer, Samaya Nissanke, Christoph Weniger, Andrew R. Williamson, Gilles Louppe

- retweets: 289, favorites: 83 (10/28/2020 10:49:17)

- links: [abs](https://arxiv.org/abs/2010.12931) | [pdf](https://arxiv.org/pdf/2010.12931)
- [astro-ph.IM](https://arxiv.org/list/astro-ph.IM/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [gr-qc](https://arxiv.org/list/gr-qc/recent)

Gravitational waves from compact binaries measured by the LIGO and Virgo detectors are routinely analyzed using Markov Chain Monte Carlo sampling algorithms. Because the evaluation of the likelihood function requires evaluating millions of waveform models that link between signal shapes and the source parameters, running Markov chains until convergence is typically expensive and requires days of computation. In this extended abstract, we provide a proof of concept that demonstrates how the latest advances in neural simulation-based inference can speed up the inference time by up to three orders of magnitude -- from days to minutes -- without impairing the performance. Our approach is based on a convolutional neural network modeling the likelihood-to-evidence ratio and entirely amortizes the computation of the posterior. We find that our model correctly estimates credible intervals for the parameters of simulated gravitational waves.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Happy to announce our latest work led by <a href="https://twitter.com/ArnaudDelaunoy?ref_src=twsrc%5Etfw">@ArnaudDelaunoy</a> for fast posterior inference on gravitational wave data using likelihood-to-evidence ratio estimation (AALR / SNRE), reducing inference time from days to minutes or less! <a href="https://t.co/SIbqvSjPOL">https://t.co/SIbqvSjPOL</a> 🔭🤖 <a href="https://t.co/IymZ6WWYGB">pic.twitter.com/IymZ6WWYGB</a></p>&mdash; Gilles Louppe (@glouppe) <a href="https://twitter.com/glouppe/status/1320994382078713858?ref_src=twsrc%5Etfw">October 27, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. TTS-by-TTS: TTS-driven Data Augmentation for Fast and High-Quality  Speech Synthesis

Min-Jae Hwang, Ryuichi Yamamoto, Eunwoo Song, Jae-Min Kim

- retweets: 182, favorites: 35 (10/28/2020 10:49:17)

- links: [abs](https://arxiv.org/abs/2010.13421) | [pdf](https://arxiv.org/pdf/2010.13421)
- [eess.AS](https://arxiv.org/list/eess.AS/recent) | [cs.SD](https://arxiv.org/list/cs.SD/recent)

In this paper, we propose a text-to-speech (TTS)-driven data augmentation method for improving the quality of a non-autoregressive (AR) TTS system. Recently proposed non-AR models, such as FastSpeech 2, have successfully achieved fast speech synthesis system. However, their quality is not satisfactory, especially when the amount of training data is insufficient. To address this problem, we propose an effective data augmentation method using a well-designed AR TTS system. In this method, large-scale synthetic corpora including text-waveform pairs with phoneme duration are generated by the AR TTS system and then used to train the target non-AR model. Perceptual listening test results showed that the proposed method significantly improved the quality of the non-AR TTS system. In particular, we augmented five hours of a training database to 179 hours of a synthetic one. Using these databases, our TTS system consisting of a FastSpeech 2 acoustic model with a Parallel WaveGAN vocoder achieved a mean opinion score of 3.74, which is 40% higher than that achieved by the conventional method.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">TTS-by-TTS: TTS-driven Data Augmentation for Fast and High-Quality Speech Synthesis<br>pdf: <a href="https://t.co/weZDvpEFdI">https://t.co/weZDvpEFdI</a><br>abs: <a href="https://t.co/wQNQdUqvIF">https://t.co/wQNQdUqvIF</a><br>project page: <a href="https://t.co/fvgMspndNQ">https://t.co/fvgMspndNQ</a> <a href="https://t.co/7ARHu8bFBs">pic.twitter.com/7ARHu8bFBs</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1320959365092904960?ref_src=twsrc%5Etfw">October 27, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. S2cGAN: Semi-Supervised Training of Conditional GANs with Fewer Labels

Arunava Chakraborty, Rahul Ragesh, Mahir Shah, Nipun Kwatra

- retweets: 78, favorites: 67 (10/28/2020 10:49:17)

- links: [abs](https://arxiv.org/abs/2010.12622) | [pdf](https://arxiv.org/pdf/2010.12622)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Generative adversarial networks (GANs) have been remarkably successful in learning complex high dimensional real word distributions and generating realistic samples. However, they provide limited control over the generation process. Conditional GANs (cGANs) provide a mechanism to control the generation process by conditioning the output on a user defined input. Although training GANs requires only unsupervised data, training cGANs requires labelled data which can be very expensive to obtain. We propose a framework for semi-supervised training of cGANs which utilizes sparse labels to learn the conditional mapping, and at the same time leverages a large amount of unsupervised data to learn the unconditional distribution. We demonstrate effectiveness of our method on multiple datasets and different conditional tasks.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">S2cGAN: Semi-Supervised Training of Conditional GANs with Fewer Labels<br>pdf: <a href="https://t.co/fwcAavRKZV">https://t.co/fwcAavRKZV</a><br>abs: <a href="https://t.co/XBXazSVGZw">https://t.co/XBXazSVGZw</a> <a href="https://t.co/MypkhMKWYM">pic.twitter.com/MypkhMKWYM</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1320932238398279681?ref_src=twsrc%5Etfw">October 27, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 8. The LMU Munich System for the WMT 2020 Unsupervised Machine Translation  Shared Task

Alexandra Chronopoulou, Dario Stojanovski, Viktor Hangya, Alexander Fraser

- retweets: 42, favorites: 57 (10/28/2020 10:49:17)

- links: [abs](https://arxiv.org/abs/2010.13192) | [pdf](https://arxiv.org/pdf/2010.13192)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

This paper describes the submission of LMU Munich to the WMT 2020 unsupervised shared task, in two language directions, German<->Upper Sorbian. Our core unsupervised neural machine translation (UNMT) system follows the strategy of Chronopoulou et al. (2020), using a monolingual pretrained language generation model (on German) and fine-tuning it on both German and Upper Sorbian, before initializing a UNMT model, which is trained with online backtranslation. Pseudo-parallel data obtained from an unsupervised statistical machine translation (USMT) system is used to fine-tune the UNMT model. We also apply BPE-Dropout to the low resource (Upper Sorbian) data to obtain a more robust system. We additionally experiment with residual adapters and find them useful in the Upper Sorbian->German direction. We explore sampling during backtranslation and curriculum learning to use SMT translations in a more principled way. Finally, we ensemble our best-performing systems and reach a BLEU score of 32.4 on German->Upper Sorbian and 35.2 on Upper Sorbian->German.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">We achieved 1st place in the <a href="https://twitter.com/hashtag/WMT2020?src=hash&amp;ref_src=twsrc%5Etfw">#WMT2020</a> Unsupervised MT task! Core elements: (twisted) MASS pretraining, online BT on monolingual data 📚+ translation loss on pseudo-parallel data📕📗from unsup. SMT(+BPE drop)<br><br>Joint work w. <a href="https://twitter.com/StDario1?ref_src=twsrc%5Etfw">@StDario1</a>, V.Hangya, A.Fraser  <br>📄<a href="https://t.co/zKVNmO3RDk">https://t.co/zKVNmO3RDk</a></p>&mdash; Alexandra Chronopoulou (@alexandraxron) <a href="https://twitter.com/alexandraxron/status/1321136925047201793?ref_src=twsrc%5Etfw">October 27, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. High Acceleration Reinforcement Learning for Real-World Juggling with  Binary Rewards

Kai Ploeger, Michael Lutter, Jan Peters

- retweets: 56, favorites: 40 (10/28/2020 10:49:17)

- links: [abs](https://arxiv.org/abs/2010.13483) | [pdf](https://arxiv.org/pdf/2010.13483)
- [cs.RO](https://arxiv.org/list/cs.RO/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Robots that can learn in the physical world will be important to en-able robots to escape their stiff and pre-programmed movements. For dynamic high-acceleration tasks, such as juggling, learning in the real-world is particularly challenging as one must push the limits of the robot and its actuation without harming the system, amplifying the necessity of sample efficiency and safety for robot learning algorithms. In contrast to prior work which mainly focuses on the learning algorithm, we propose a learning system, that directly incorporates these requirements in the design of the policy representation, initialization, and optimization. We demonstrate that this system enables the high-speed Barrett WAM manipulator to learn juggling two balls from 56 minutes of experience with a binary reward signal. The final policy juggles continuously for up to 33 minutes or about 4500 repeated catches. The videos documenting the learning process and the evaluation can be found at https://sites.google.com/view/jugglingbot

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">High Acceleration Reinforcement Learning for Real-World Juggling with Binary Rewards<a href="https://t.co/O5o5YJ66KJ">https://t.co/O5o5YJ66KJ</a><a href="https://t.co/iXgV3gC7Uh">https://t.co/iXgV3gC7Uh</a> <a href="https://t.co/Z0diCzVsHR">pic.twitter.com/Z0diCzVsHR</a></p>&mdash; sim2real (@sim2realAIorg) <a href="https://twitter.com/sim2realAIorg/status/1320917210890530816?ref_src=twsrc%5Etfw">October 27, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 10. Trajectory-wise Multiple Choice Learning for Dynamics Generalization in  Reinforcement Learning

Younggyo Seo, Kimin Lee, Ignasi Clavera, Thanard Kurutach, Jinwoo Shin, Pieter Abbeel

- retweets: 54, favorites: 35 (10/28/2020 10:49:17)

- links: [abs](https://arxiv.org/abs/2010.13303) | [pdf](https://arxiv.org/pdf/2010.13303)
- [cs.LG](https://arxiv.org/list/cs.LG/recent)

Model-based reinforcement learning (RL) has shown great potential in various control tasks in terms of both sample-efficiency and final performance. However, learning a generalizable dynamics model robust to changes in dynamics remains a challenge since the target transition dynamics follow a multi-modal distribution. In this paper, we present a new model-based RL algorithm, coined trajectory-wise multiple choice learning, that learns a multi-headed dynamics model for dynamics generalization. The main idea is updating the most accurate prediction head to specialize each head in certain environments with similar dynamics, i.e., clustering environments. Moreover, we incorporate context learning, which encodes dynamics-specific information from past experiences into the context latent vector, enabling the model to perform online adaptation to unseen environments. Finally, to utilize the specialized prediction heads more effectively, we propose an adaptive planning method, which selects the most accurate prediction head over a recent experience. Our method exhibits superior zero-shot generalization performance across a variety of control tasks, compared to state-of-the-art RL methods. Source code and videos are available at https://sites.google.com/view/trajectory-mcl.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Excited to share our <a href="https://twitter.com/hashtag/NeurIPS2020?src=hash&amp;ref_src=twsrc%5Etfw">#NeurIPS2020</a> paper that introduces a new model-based RL method to learn the multi-modal transition distribution in an unsupervised manner<br>🎓<a href="https://t.co/nhKgJJJ9JW">https://t.co/nhKgJJJ9JW</a><br>💻<a href="https://t.co/fRg25TSOdR">https://t.co/fRg25TSOdR</a><br>w/<a href="https://twitter.com/younggyoseo?ref_src=twsrc%5Etfw">@younggyoseo</a> <a href="https://twitter.com/clavera_i?ref_src=twsrc%5Etfw">@clavera_i</a> <a href="https://twitter.com/KurutachThanard?ref_src=twsrc%5Etfw">@KurutachThanard</a> <a href="https://twitter.com/jinwoos0417?ref_src=twsrc%5Etfw">@jinwoos0417</a> <a href="https://twitter.com/pabbeel?ref_src=twsrc%5Etfw">@pabbeel</a> <br>1/N</p>&mdash; Kimin (@kimin_le2) <a href="https://twitter.com/kimin_le2/status/1321139111063814144?ref_src=twsrc%5Etfw">October 27, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 11. FastFormers: Highly Efficient Transformer Models for Natural Language  Understanding

Young Jin Kim, Hany Hassan Awadalla

- retweets: 42, favorites: 33 (10/28/2020 10:49:17)

- links: [abs](https://arxiv.org/abs/2010.13382) | [pdf](https://arxiv.org/pdf/2010.13382)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

Transformer-based models are the state-of-the-art for Natural Language Understanding (NLU) applications. Models are getting bigger and better on various tasks. However, Transformer models remain computationally challenging since they are not efficient at inference-time compared to traditional approaches. In this paper, we present FastFormers, a set of recipes to achieve efficient inference-time performance for Transformer-based models on various NLU tasks. We show how carefully utilizing knowledge distillation, structured pruning and numerical optimization can lead to drastic improvements on inference efficiency. We provide effective recipes that can guide practitioners to choose the best settings for various NLU tasks and pretrained models. Applying the proposed recipes to the SuperGLUE benchmark, we achieve from 9.8x up to 233.9x speed-up compared to out-of-the-box models on CPU. On GPU, we also achieve up to 12.4x speed-up with the presented methods. We show that FastFormers can drastically reduce cost of serving 100 million requests from 4,223 USD to just 18 USD on an Azure F16s_v2 instance. This translates to a sustainable runtime by reducing energy consumption 6.9x - 125.8x according to the metrics used in the SustaiNLP 2020 shared task.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">FastFormers: Highly Efficient Transformer Models for Natural Language Understanding<br>pdf: <a href="https://t.co/HRCHSDOG9i">https://t.co/HRCHSDOG9i</a><br>abs: <a href="https://t.co/NI6w8qHEnQ">https://t.co/NI6w8qHEnQ</a><br>github: <a href="https://t.co/1vnyr3IBMQ">https://t.co/1vnyr3IBMQ</a> <a href="https://t.co/hXnynyolNO">pic.twitter.com/hXnynyolNO</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1320901814414417920?ref_src=twsrc%5Etfw">October 27, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 12. Graph Information Bottleneck

Tailin Wu, Hongyu Ren, Pan Li, Jure Leskovec

- retweets: 50, favorites: 19 (10/28/2020 10:49:18)

- links: [abs](https://arxiv.org/abs/2010.12811) | [pdf](https://arxiv.org/pdf/2010.12811)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Representation learning of graph-structured data is challenging because both graph structure and node features carry important information. Graph Neural Networks (GNNs) provide an expressive way to fuse information from network structure and node features. However, GNNs are prone to adversarial attacks. Here we introduce Graph Information Bottleneck (GIB), an information-theoretic principle that optimally balances expressiveness and robustness of the learned representation of graph-structured data. Inheriting from the general Information Bottleneck (IB), GIB aims to learn the minimal sufficient representation for a given task by maximizing the mutual information between the representation and the target, and simultaneously constraining the mutual information between the representation and the input data. Different from the general IB, GIB regularizes the structural as well as the feature information. We design two sampling algorithms for structural regularization and instantiate the GIB principle with two new models: GIB-Cat and GIB-Bern, and demonstrate the benefits by evaluating the resilience to adversarial attacks. We show that our proposed models are more robust than state-of-the-art graph defense models. GIB-based models empirically achieve up to 31% improvement with adversarial perturbation of the graph structure as well as node features.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">New <a href="https://twitter.com/hashtag/NeurIPS2020?src=hash&amp;ref_src=twsrc%5Etfw">#NeurIPS2020</a> paper on Graph Neural Nets <a href="https://twitter.com/hashtag/GNN?src=hash&amp;ref_src=twsrc%5Etfw">#GNN</a>, Representation Learning, Robustness!<br>&quot;Graph Information Bottleneck&quot; w/ @tailintalent, Pan Li, <a href="https://twitter.com/jure?ref_src=twsrc%5Etfw">@jure</a> <a href="https://twitter.com/StanfordAILab?ref_src=twsrc%5Etfw">@StanfordAILab</a> <a href="https://twitter.com/PurdueCS?ref_src=twsrc%5Etfw">@PurdueCS</a> <br>Website: <a href="https://t.co/sTYXObicuw">https://t.co/sTYXObicuw</a><br>Paper: <a href="https://t.co/1IWqYXEHWe">https://t.co/1IWqYXEHWe</a><br>Code: <a href="https://t.co/I0P05x8jz5">https://t.co/I0P05x8jz5</a><br>(1/n) <a href="https://t.co/N22ZDwj1Qv">pic.twitter.com/N22ZDwj1Qv</a></p>&mdash; Hongyu Ren (@ren_hongyu) <a href="https://twitter.com/ren_hongyu/status/1321133207257952256?ref_src=twsrc%5Etfw">October 27, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 13. XLVIN: eXecuted Latent Value Iteration Nets

Andreea Deac, Petar Veličković, Ognjen Milinković, Pierre-Luc Bacon, Jian Tang, Mladen Nikolić

- retweets: 20, favorites: 41 (10/28/2020 10:49:18)

- links: [abs](https://arxiv.org/abs/2010.13146) | [pdf](https://arxiv.org/pdf/2010.13146)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Value Iteration Networks (VINs) have emerged as a popular method to incorporate planning algorithms within deep reinforcement learning, enabling performance improvements on tasks requiring long-range reasoning and understanding of environment dynamics. This came with several limitations, however: the model is not incentivised in any way to perform meaningful planning computations, the underlying state space is assumed to be discrete, and the Markov decision process (MDP) is assumed fixed and known. We propose eXecuted Latent Value Iteration Networks (XLVINs), which combine recent developments across contrastive self-supervised learning, graph representation learning and neural algorithmic reasoning to alleviate all of the above limitations, successfully deploying VIN-style models on generic environments. XLVINs match the performance of VIN-like models when the underlying MDP is discrete, fixed and known, and provides significant improvements to model-free baselines across three general MDP setups.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Excited to share the XLVIN agent!<br><br>Using GNNs for implicit planning and algorithmically aligning to Value Iteration, we extend Value Iteration Nets to pixel-based/continuous MDPs (eg Atari-Freeway, CartPole, Acrobot), outperforming model-free baselines.<a href="https://t.co/Kkv5xXPvZr">https://t.co/Kkv5xXPvZr</a> <a href="https://t.co/4KVVenVrXr">pic.twitter.com/4KVVenVrXr</a></p>&mdash; Andreea Deac (@andreeadeac22) <a href="https://twitter.com/andreeadeac22/status/1321112911578845187?ref_src=twsrc%5Etfw">October 27, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 14. Improving the Reconstruction of Disentangled Representation Learners via  Multi-Stage Modelling

Akash Srivastava, Yamini Bansal, Yukun Ding, Cole Hurwitz, Kai Xu, Bernhard Egger, Prasanna Sattigeri, Josh Tenenbaum, David D. Cox, Dan Gutfreund

- retweets: 36, favorites: 22 (10/28/2020 10:49:18)

- links: [abs](https://arxiv.org/abs/2010.13187) | [pdf](https://arxiv.org/pdf/2010.13187)
- [stat.ML](https://arxiv.org/list/stat.ML/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Current autoencoder-based disentangled representation learning methods achieve disentanglement by penalizing the (aggregate) posterior to encourage statistical independence of the latent factors. This approach introduces a trade-off between disentangled representation learning and reconstruction quality since the model does not have enough capacity to learn correlated latent variables that capture detail information present in most image data. To overcome this trade-off, we present a novel multi-stage modelling approach where the disentangled factors are first learned using a preexisting disentangled representation learning method (such as $\beta$-TCVAE); then, the low-quality reconstruction is improved with another deep generative model that is trained to model the missing correlated latent variables, adding detail information while maintaining conditioning on the previously learned disentangled factors. Taken together, our multi-stage modelling approach results in a single, coherent probabilistic model that is theoretically justified by the principal of D-separation and can be realized with a variety of model classes including likelihood-based models such as variational autoencoders, implicit models such as generative adversarial networks, and tractable models like normalizing flows or mixtures of Gaussians. We demonstrate that our multi-stage model has much higher reconstruction quality than current state-of-the-art methods with equivalent disentanglement performance across multiple standard benchmarks.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Improving the Reconstruction of Disentangled Representation Learners via Multi-Stage Modelling<br>pdf: <a href="https://t.co/dubu2XiYLf">https://t.co/dubu2XiYLf</a><br>abs: <a href="https://t.co/xtqmnXa4OI">https://t.co/xtqmnXa4OI</a> <a href="https://t.co/rpcI1d2xk1">pic.twitter.com/rpcI1d2xk1</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1320958521375182848?ref_src=twsrc%5Etfw">October 27, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 15. Contextualized Word Embeddings Encode Aspects of Human-Like Word Sense  Knowledge

Sathvik Nair, Mahesh Srinivasan, Stephan Meylan

- retweets: 30, favorites: 27 (10/28/2020 10:49:18)

- links: [abs](https://arxiv.org/abs/2010.13057) | [pdf](https://arxiv.org/pdf/2010.13057)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

Understanding context-dependent variation in word meanings is a key aspect of human language comprehension supported by the lexicon. Lexicographic resources (e.g., WordNet) capture only some of this context-dependent variation; for example, they often do not encode how closely senses, or discretized word meanings, are related to one another. Our work investigates whether recent advances in NLP, specifically contextualized word embeddings, capture human-like distinctions between English word senses, such as polysemy and homonymy. We collect data from a behavioral, web-based experiment, in which participants provide judgments of the relatedness of multiple WordNet senses of a word in a two-dimensional spatial arrangement task. We find that participants' judgments of the relatedness between senses are correlated with distances between senses in the BERT embedding space. Homonymous senses (e.g., bat as mammal vs. bat as sports equipment) are reliably more distant from one another in the embedding space than polysemous ones (e.g., chicken as animal vs. chicken as meat). Our findings point towards the potential utility of continuous-space representations of sense meanings.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Excited to officially announce my paper, Contextualized Word Embeddings Encode Aspects of Human-Like Word Sense Knowledge, for the Cognitive Aspects of the Lexicon Workshop at <a href="https://twitter.com/coling2020?ref_src=twsrc%5Etfw">@coling2020</a>. <a href="https://t.co/Ilu7lDSSLS">https://t.co/Ilu7lDSSLS</a></p>&mdash; Sathvik (@sathvikn4) <a href="https://twitter.com/sathvikn4/status/1321175553827692545?ref_src=twsrc%5Etfw">October 27, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 16. Performance Analysis of Scientific Computing Workloads on Trusted  Execution Environments

Ayaz Akram, Anna Giannakou, Venkatesh Akella, Jason Lowe-Power, Sean Peisert

- retweets: 26, favorites: 27 (10/28/2020 10:49:18)

- links: [abs](https://arxiv.org/abs/2010.13216) | [pdf](https://arxiv.org/pdf/2010.13216)
- [cs.DC](https://arxiv.org/list/cs.DC/recent) | [cs.AR](https://arxiv.org/list/cs.AR/recent) | [cs.CR](https://arxiv.org/list/cs.CR/recent)

Scientific computing sometimes involves computation on sensitive data. Depending on the data and the execution environment, the HPC (high-performance computing) user or data provider may require confidentiality and/or integrity guarantees. To study the applicability of hardware-based trusted execution environments (TEEs) to enable secure scientific computing, we deeply analyze the performance impact of AMD SEV and Intel SGX for diverse HPC benchmarks including traditional scientific computing, machine learning, graph analytics, and emerging scientific computing workloads. We observe three main findings: 1) SEV requires careful memory placement on large scale NUMA machines (1$\times$$-$3.4$\times$ slowdown without and 1$\times$$-$1.15$\times$ slowdown with NUMA aware placement), 2) virtualization$-$a prerequisite for SEV$-$results in performance degradation for workloads with irregular memory accesses and large working sets (1$\times$$-$4$\times$ slowdown compared to native execution for graph applications) and 3) SGX is inappropriate for HPC given its limited secure memory size and inflexible programming model (1.2$\times$$-$126$\times$ slowdown over unsecure execution). Finally, we discuss forthcoming new TEE designs and their potential impact on scientific computing.



