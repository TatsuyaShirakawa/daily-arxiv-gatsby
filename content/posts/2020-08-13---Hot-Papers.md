---
title: Hot Papers 2020-08-13
date: 2020-08-14T13:07:35.Z
template: "post"
draft: false
slug: "hot-papers-2020-08-13"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-08-13"
socialImage: "/media/flying-marine.jpg"

---

# 1. Sampling using $SU(N)$ gauge equivariant flows

Denis Boyda, Gurtej Kanwar, Sébastien Racanière, Danilo Jimenez Rezende, Michael S. Albergo, Kyle Cranmer, Daniel C. Hackett, Phiala E. Shanahan

- retweets: 255, favorites: 1291 (08/14/2020 13:07:35)

- links: [abs](https://arxiv.org/abs/2008.05456) | [pdf](https://arxiv.org/pdf/2008.05456)
- [hep-lat](https://arxiv.org/list/hep-lat/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

We develop a flow-based sampling algorithm for $SU(N)$ lattice gauge theories that is gauge-invariant by construction. Our key contribution is constructing a class of flows on an $SU(N)$ variable (or on a $U(N)$ variable by a simple alternative) that respect matrix conjugation symmetry. We apply this technique to sample distributions of single $SU(N)$ variables and to construct flow-based samplers for $SU(2)$ and $SU(3)$ lattice gauge theory in two dimensions.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Very excited to share our newest paper <br>combining machine learning &amp; physics. We develop normalizing flows that impose the elaborate of symmetry groups you find in fundamental particle physics. <br>It&#39;s a beautiful mix of math, ML, and physics<a href="https://t.co/1keqEEEYM3">https://t.co/1keqEEEYM3</a> <a href="https://t.co/1fblwhsutF">pic.twitter.com/1fblwhsutF</a></p>&mdash; Kyle Cranmer (@KyleCranmer) <a href="https://twitter.com/KyleCranmer/status/1293707130126704641?ref_src=twsrc%5Etfw">August 13, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">The Lie group SU(N) is very important in fundamental particle physics as a Gauge symmetry group. Building SU(N) Gauge-equivariant models is key to make ML useful in this field. It was not easy, but WE DID IT!<br>SU(N) gauge equivariant flows for LatticeQCD:<a href="https://t.co/kwCqBjGVpZ">https://t.co/kwCqBjGVpZ</a></p>&mdash; Danilo J. Rezende (@DaniloJRezende) <a href="https://twitter.com/DaniloJRezende/status/1293738960737509376?ref_src=twsrc%5Etfw">August 13, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Check out our latest paper where we extend equivariant normalizing flows to SU(N) for arbitrary N! So much great work from collaborators <a href="https://twitter.com/DaniloJRezende?ref_src=twsrc%5Etfw">@DaniloJRezende</a> <a href="https://twitter.com/sracaniere?ref_src=twsrc%5Etfw">@sracaniere</a> <a href="https://twitter.com/KyleCranmer?ref_src=twsrc%5Etfw">@KyleCranmer</a> and those not on twitter :) the arXiv link is here: <a href="https://t.co/geUCEtctzi">https://t.co/geUCEtctzi</a> <a href="https://t.co/N65Yd19kjQ">https://t.co/N65Yd19kjQ</a></p>&mdash; Michael Albergo (@msalbergo) <a href="https://twitter.com/msalbergo/status/1293709832676139014?ref_src=twsrc%5Etfw">August 13, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">It took several months to work through a series of challenges. Shout out to the team: <br>Denis Boyda, Gurtej Kanwar, <a href="https://twitter.com/sracaniere?ref_src=twsrc%5Etfw">@sracaniere</a>, <a href="https://twitter.com/DaniloJRezende?ref_src=twsrc%5Etfw">@DaniloJRezende</a>,<a href="https://twitter.com/msalbergo?ref_src=twsrc%5Etfw">@msalbergo</a>, <a href="https://twitter.com/KyleCranmer?ref_src=twsrc%5Etfw">@kylecranmer</a>, Daniel Hackett, Phiala Shanahan<a href="https://t.co/1keqEEEYM3">https://t.co/1keqEEEYM3</a> <a href="https://t.co/7O1dqsdRYs">pic.twitter.com/7O1dqsdRYs</a></p>&mdash; Kyle Cranmer (@KyleCranmer) <a href="https://twitter.com/KyleCranmer/status/1293707131707961350?ref_src=twsrc%5Etfw">August 13, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Audio- and Gaze-driven Facial Animation of Codec Avatars

Alexander Richard, Colin Lea, Shugao Ma, Juergen Gall, Fernando de la Torre, Yaser Sheikh

- retweets: 31, favorites: 118 (08/14/2020 13:07:36)

- links: [abs](https://arxiv.org/abs/2008.05023) | [pdf](https://arxiv.org/pdf/2008.05023)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Codec Avatars are a recent class of learned, photorealistic face models that accurately represent the geometry and texture of a person in 3D (i.e., for virtual reality), and are almost indistinguishable from video. In this paper we describe the first approach to animate these parametric models in real-time which could be deployed on commodity virtual reality hardware using audio and/or eye tracking. Our goal is to display expressive conversations between individuals that exhibit important social signals such as laughter and excitement solely from latent cues in our lossy input signals. To this end we collected over 5 hours of high frame rate 3D face scans across three participants including traditional neutral speech as well as expressive and conversational speech. We investigate a multimodal fusion approach that dynamically identifies which sensor encoding should animate which parts of the face at any time. See the supplemental video which demonstrates our ability to generate full face motion far beyond the typically neutral lip articulations seen in competing work: https://research.fb.com/videos/audio-and-gaze-driven-facial-animation-of-codec-avatars/

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Audio- and Gaze-driven Facial Animation of Codec Avatars<br>pdf: <a href="https://t.co/KYi8YeRbXc">https://t.co/KYi8YeRbXc</a><br>abs: <a href="https://t.co/t5EMTDzCeN">https://t.co/t5EMTDzCeN</a><br>project page: <a href="https://t.co/C0LAR6nYVW">https://t.co/C0LAR6nYVW</a> <a href="https://t.co/XSIqWPjUbu">pic.twitter.com/XSIqWPjUbu</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1293709209507262464?ref_src=twsrc%5Etfw">August 13, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Hypergraph reconstruction from network data

Jean-Gabriel Young, Giovanni Petri, Tiago P. Peixoto

- retweets: 22, favorites: 72 (08/14/2020 13:07:36)

- links: [abs](https://arxiv.org/abs/2008.04948) | [pdf](https://arxiv.org/pdf/2008.04948)
- [cs.SI](https://arxiv.org/list/cs.SI/recent) | [physics.soc-ph](https://arxiv.org/list/physics.soc-ph/recent) | [stat.AP](https://arxiv.org/list/stat.AP/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

Networks can describe the structure of a wide variety of complex systems by specifying how pairs of nodes interact. This choice of representation is flexible, but not necessarily appropriate when joint interactions between groups of nodes are needed to explain empirical phenomena. Networks remain the de facto standard, however, as relational datasets often fail to record higher-order interactions. To address this gap, we here introduce a Bayesian approach to reconstruct the higher-order interactions from pairwise network data. Our method is based on the principle of parsimony and does not reconstruct higher-order structures when there is scant statistical evidence. We demonstrate that our approach successfully uncovers higher-order interactions in synthetic and empirical network data.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">&quot;Hypergraph reconstruction from network data&quot;<br><br>New preprint from faculty member <a href="https://twitter.com/_jgyou?ref_src=twsrc%5Etfw">@_jgyou</a> w/<a href="https://twitter.com/lordgrilo?ref_src=twsrc%5Etfw">@lordgrilo</a><br>&amp; <a href="https://twitter.com/tiagopeixoto?ref_src=twsrc%5Etfw">@tiagopeixoto</a><a href="https://t.co/QAV8l9ekmn">https://t.co/QAV8l9ekmn</a> <a href="https://t.co/jfmsPrzbpw">pic.twitter.com/jfmsPrzbpw</a></p>&mdash; Vermont Complex Systems Center @ UVM (@uvmcomplexity) <a href="https://twitter.com/uvmcomplexity/status/1293900386105991169?ref_src=twsrc%5Etfw">August 13, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Learning to Caricature via Semantic Shape Transform

Wenqing Chu, Wei-Chih Hung, Yi-Hsuan Tsai, Yu-Ting Chang, Yijun Li, Deng Cai, Ming-Hsuan Yang

- retweets: 16, favorites: 74 (08/14/2020 13:07:37)

- links: [abs](https://arxiv.org/abs/2008.05090) | [pdf](https://arxiv.org/pdf/2008.05090)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Caricature is an artistic drawing created to abstract or exaggerate facial features of a person. Rendering visually pleasing caricatures is a difficult task that requires professional skills, and thus it is of great interest to design a method to automatically generate such drawings. To deal with large shape changes, we propose an algorithm based on a semantic shape transform to produce diverse and plausible shape exaggerations. Specifically, we predict pixel-wise semantic correspondences and perform image warping on the input photo to achieve dense shape transformation. We show that the proposed framework is able to render visually pleasing shape exaggerations while maintaining their facial structures. In addition, our model allows users to manipulate the shape via the semantic map. We demonstrate the effectiveness of our approach on a large photograph-caricature benchmark dataset with comparisons to the state-of-the-art methods.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Learning to Caricature via Semantic Shape Transform<br>pdf: <a href="https://t.co/BvMEMYDtAB">https://t.co/BvMEMYDtAB</a><br>abs: <a href="https://t.co/OgrE7Et0Bm">https://t.co/OgrE7Et0Bm</a><br>github: <a href="https://t.co/HwHuLGPV7q">https://t.co/HwHuLGPV7q</a> <a href="https://t.co/qmdPLfY6cc">pic.twitter.com/qmdPLfY6cc</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1293720170784002051?ref_src=twsrc%5Etfw">August 13, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Short Shor-style syndrome sequences

Nicolas Delfosse, Ben W. Reichardt

- retweets: 12, favorites: 69 (08/14/2020 13:07:37)

- links: [abs](https://arxiv.org/abs/2008.05051) | [pdf](https://arxiv.org/pdf/2008.05051)
- [quant-ph](https://arxiv.org/list/quant-ph/recent) | [cs.IT](https://arxiv.org/list/cs.IT/recent)

We optimize fault-tolerant quantum error correction to reduce the number of syndrome bit measurements. Speeding up error correction will also speed up an encoded quantum computation, and should reduce its effective error rate. We give both code-specific and general methods, using a variety of techniques and in a variety of settings. We design new quantum error-correcting codes specifically for efficient error correction, e.g., allowing single-shot error correction. For codes with multiple logical qubits, we give methods for combining error correction with partial logical measurements. There are tradeoffs in choosing a code and error-correction technique. While to date most work has concentrated on optimizing the syndrome-extraction procedure, we show that there are also substantial benefits to optimizing how the measured syndromes are chosen and used. As an example, we design single-shot measurement sequences for fault-tolerant quantum error correction with the 16-qubit extended Hamming code. Our scheme uses 10 syndrome bit measurements, compared to 40 measurements with the Shor scheme. We design single-shot logical measurements as well: any logical Z measurement can be made together with fault-tolerant error correction using only 11 measurements. For comparison, using the Shor scheme a basic implementation of such a non-destructive logical measurement uses 63 measurements. We also offer ten open problems, the solutions of which could lead to substantial improvements of fault-tolerant error correction.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Three reasons to read our new paper with <a href="https://twitter.com/breic?ref_src=twsrc%5Etfw">@breic</a>: <br>1. All the words of the title start with S.<br>2. It contains 10 open questions.<br>3. It is full of new techniques for quantum error correction. My favorite: single-shot logical measurements.<a href="https://twitter.com/hashtag/QuantumComputing?src=hash&amp;ref_src=twsrc%5Etfw">#QuantumComputing</a><a href="https://t.co/gEwQdE2FcO">https://t.co/gEwQdE2FcO</a></p>&mdash; Nicolas Delfosse (@nic_delfosse) <a href="https://twitter.com/nic_delfosse/status/1293723577779253248?ref_src=twsrc%5Etfw">August 13, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. Speaker Conditional WaveRNN: Towards Universal Neural Vocoder for Unseen  Speaker and Recording Conditions

Dipjyoti Paul, Yannis Pantazis, Yannis Stylianou

- retweets: 14, favorites: 52 (08/14/2020 13:07:37)

- links: [abs](https://arxiv.org/abs/2008.05289) | [pdf](https://arxiv.org/pdf/2008.05289)
- [eess.AS](https://arxiv.org/list/eess.AS/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.SD](https://arxiv.org/list/cs.SD/recent)

Recent advancements in deep learning led to human-level performance in single-speaker speech synthesis. However, there are still limitations in terms of speech quality when generalizing those systems into multiple-speaker models especially for unseen speakers and unseen recording qualities. For instance, conventional neural vocoders are adjusted to the training speaker and have poor generalization capabilities to unseen speakers. In this work, we propose a variant of WaveRNN, referred to as speaker conditional WaveRNN (SC-WaveRNN). We target towards the development of an efficient universal vocoder even for unseen speakers and recording conditions. In contrast to standard WaveRNN, SC-WaveRNN exploits additional information given in the form of speaker embeddings. Using publicly-available data for training, SC-WaveRNN achieves significantly better performance over baseline WaveRNN on both subjective and objective metrics. In MOS, SC-WaveRNN achieves an improvement of about 23% for seen speaker and seen recording condition and up to 95% for unseen speaker and unseen condition. Finally, we extend our work by implementing a multi-speaker text-to-speech (TTS) synthesis similar to zero-shot speaker adaptation. In terms of performance, our system has been preferred over the baseline TTS system by 60% over 15.5% and by 60.9% over 32.6%, for seen and unseen speakers, respectively.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Speaker Conditional WaveRNN: Towards Universal Neural Vocoder for Unseen Speaker and Recording Conditions<br>pdf: <a href="https://t.co/SwMMI2hSQV">https://t.co/SwMMI2hSQV</a><br>abs: <a href="https://t.co/vkiEUkIadz">https://t.co/vkiEUkIadz</a> <a href="https://t.co/K7B2HZ48xS">pic.twitter.com/K7B2HZ48xS</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1293748587801776128?ref_src=twsrc%5Etfw">August 13, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



