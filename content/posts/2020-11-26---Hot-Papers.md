---
title: Hot Papers 2020-11-26
date: 2020-11-27T09:54:49.Z
template: "post"
draft: false
slug: "hot-papers-2020-11-26"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-11-26"
socialImage: "/media/flying-marine.jpg"

---

# 1. Differentially Private Learning Needs Better Features (or Much More  Data)

Florian Tramèr, Dan Boneh

- retweets: 2120, favorites: 272 (11/27/2020 09:54:49)

- links: [abs](https://arxiv.org/abs/2011.11660) | [pdf](https://arxiv.org/pdf/2011.11660)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.CR](https://arxiv.org/list/cs.CR/recent) | [stat.ML](https://arxiv.org/list/stat.ML/recent)

We demonstrate that differentially private machine learning has not yet reached its "AlexNet moment" on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets. To exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain. Our work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Current algorithms for training neural nets with differential privacy greatly hurt model accuracy.<br><br>Can we do better? Yes!<br>With <a href="https://twitter.com/danboneh?ref_src=twsrc%5Etfw">@danboneh</a> we show how to get better private models by...not using deep learning!<br><br>Paper: <a href="https://t.co/5jMfcq2NXZ">https://t.co/5jMfcq2NXZ</a><br>Code: <a href="https://t.co/ZnudaQrZ9Q">https://t.co/ZnudaQrZ9Q</a> <a href="https://t.co/qFTpJeJ8WC">pic.twitter.com/qFTpJeJ8WC</a></p>&mdash; Florian Tramèr (@florian_tramer) <a href="https://twitter.com/florian_tramer/status/1331680382803034112?ref_src=twsrc%5Etfw">November 25, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Towards Playing Full MOBA Games with Deep Reinforcement Learning

Deheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia Chen, Zhao Liu, Fuhao Qiu, Hongsheng Yu, Yinyuting Yin, Bei Shi, Liang Wang, Tengfei Shi, Qiang Fu, Wei Yang, Lanxiao Huang, Wei Liu

- retweets: 368, favorites: 62 (11/27/2020 09:54:49)

- links: [abs](https://arxiv.org/abs/2011.12692) | [pdf](https://arxiv.org/pdf/2011.12692)
- [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

MOBA games, e.g., Honor of Kings, League of Legends, and Dota 2, pose grand challenges to AI systems such as multi-agent, enormous state-action space, complex action control, etc. Developing AI for playing MOBA games has raised much attention accordingly. However, existing work falls short in handling the raw game complexity caused by the explosion of agent combinations, i.e., lineups, when expanding the hero pool in case that OpenAI's Dota AI limits the play to a pool of only 17 heroes. As a result, full MOBA games without restrictions are far from being mastered by any existing AI system. In this paper, we propose a MOBA AI learning paradigm that methodologically enables playing full MOBA games with deep reinforcement learning. Specifically, we develop a combination of novel and existing learning techniques, including off-policy adaption, multi-head value estimation, curriculum self-play learning, policy distillation, and Monte-Carlo tree-search, in training and playing a large pool of heroes, meanwhile addressing the scalability issue skillfully. Tested on Honor of Kings, a popular MOBA game, we show how to build superhuman AI agents that can defeat top esports players. The superiority of our AI is demonstrated by the first large-scale performance test of MOBA AI agent in the literature.

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">中国企業のテンセントが深層強化学習を用いてMOBAゲーム「王者栄耀」で人間のトッププレイヤーを打ち負かした。MOBAを対象とした強化学習ベースの超人的AIプログラムにおいて、40体以上のキャラから選択できるものは初めて、と主張<a href="https://t.co/JH7i6Ac2N1">https://t.co/JH7i6Ac2N1</a></p>&mdash; 小猫遊りょう（たかにゃし・りょう） (@jaguring1) <a href="https://twitter.com/jaguring1/status/1332093057404047360?ref_src=twsrc%5Etfw">November 26, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">&quot;Towards Playing Full MOBA Games with Deep Reinforcement Learning,&quot; Ye et al. at Tencent: <a href="https://t.co/cMK0D9hAel">https://t.co/cMK0D9hAel</a><br><br>&quot;Tested on Honor of Kings, a popular MOBA game, we show how to build superhuman AI agents that can defeat top esports players.&quot;</p>&mdash; Miles Brundage (@Miles_Brundage) <a href="https://twitter.com/Miles_Brundage/status/1331786598787162113?ref_src=twsrc%5Etfw">November 26, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Space-time Neural Irradiance Fields for Free-Viewpoint Video

Wenqi Xian, Jia-Bin Huang, Johannes Kopf, Changil Kim

- retweets: 196, favorites: 111 (11/27/2020 09:54:49)

- links: [abs](https://arxiv.org/abs/2011.12950) | [pdf](https://arxiv.org/pdf/2011.12950)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

We present a method that learns a spatiotemporal neural irradiance field for dynamic scenes from a single video. Our learned representation enables free-viewpoint rendering of the input video. Our method builds upon recent advances in implicit representations. Learning a spatiotemporal irradiance field from a single video poses significant challenges because the video contains only one observation of the scene at any point in time. The 3D geometry of a scene can be legitimately represented in numerous ways since varying geometry (motion) can be explained with varying appearance and vice versa. We address this ambiguity by constraining the time-varying geometry of our dynamic scene representation using the scene depth estimated from video depth estimation methods, aggregating contents from individual frames into a single global representation. We provide an extensive quantitative evaluation and demonstrate compelling free-viewpoint rendering results.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Space-time Neural Irradiance Fields for Free-Viewpoint Video<br>pdf: <a href="https://t.co/xQV34eVI6u">https://t.co/xQV34eVI6u</a><br>abs: <a href="https://t.co/BXR2BBxstV">https://t.co/BXR2BBxstV</a><br>project page: <a href="https://t.co/6NajHGiIs6">https://t.co/6NajHGiIs6</a> <a href="https://t.co/wpxXHMnjHL">pic.twitter.com/wpxXHMnjHL</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1331795826167918595?ref_src=twsrc%5Etfw">November 26, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation

Zongze Wu, Dani Lischinski, Eli Shechtman

- retweets: 169, favorites: 64 (11/27/2020 09:54:50)

- links: [abs](https://arxiv.org/abs/2011.12799) | [pdf](https://arxiv.org/pdf/2011.12799)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.GR](https://arxiv.org/list/cs.GR/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

We explore and analyze the latent style space of StyleGAN2, a state-of-the-art architecture for image generation, using models pretrained on several different datasets. We first show that StyleSpace, the space of channel-wise style parameters, is significantly more disentangled than the other intermediate latent spaces explored by previous works. Next, we describe a method for discovering a large collection of style channels, each of which is shown to control a distinct visual attribute in a highly localized and disentangled manner. Third, we propose a simple method for identifying style channels that control a specific attribute, using a pretrained classifier or a small number of example images. Manipulation of visual attributes via these StyleSpace controls is shown to be better disentangled than via those proposed in previous works. To show this, we make use of a newly proposed Attribute Dependency metric. Finally, we demonstrate the applicability of StyleSpace controls to the manipulation of real images. Our findings pave the way to semantically meaningful and well-disentangled image manipulations via simple and intuitive interfaces.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation<br>pdf: <a href="https://t.co/K6ua267qRz">https://t.co/K6ua267qRz</a><br>abs: <a href="https://t.co/R0lSFZ9tPr">https://t.co/R0lSFZ9tPr</a> <a href="https://t.co/olYVKTr5By">pic.twitter.com/olYVKTr5By</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1331782938481012739?ref_src=twsrc%5Etfw">November 26, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Deep Physics-aware Inference of Cloth Deformation for Monocular Human  Performance Capture

Yue Li, Marc Habermann, Bernhard Thomaszewski, Stelian Coros, Thabo Beeler, Christian Theobalt

- retweets: 58, favorites: 44 (11/27/2020 09:54:50)

- links: [abs](https://arxiv.org/abs/2011.12866) | [pdf](https://arxiv.org/pdf/2011.12866)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Recent monocular human performance capture approaches have shown compelling dense tracking results of the full body from a single RGB camera. However, existing methods either do not estimate clothing at all or model cloth deformation with simple geometric priors instead of taking into account the underlying physical principles. This leads to noticeable artifacts in their reconstructions, such as baked-in wrinkles, implausible deformations that seemingly defy gravity, and intersections between cloth and body. To address these problems, we propose a person-specific, learning-based method that integrates a finite element-based simulation layer into the training process to provide for the first time physics supervision in the context of weakly-supervised deep monocular human performance capture. We show how integrating physics into the training process improves the learned cloth deformations, allows modeling clothing as a separate piece of geometry, and largely reduces cloth-body intersections. Relying only on weak 2D multi-view supervision during training, our approach leads to a significant improvement over current state-of-the-art methods and is thus a clear step towards realistic monocular capture of the entire deforming surface of a clothed human.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Deep Physics-aware Inference of Cloth Deformation for<br>Monocular Human Performance Capture<br>pdf: <a href="https://t.co/DMeDYb2gzF">https://t.co/DMeDYb2gzF</a><br>abs: <a href="https://t.co/hHS8R70tpy">https://t.co/hHS8R70tpy</a> <a href="https://t.co/Me4zGsCyjB">pic.twitter.com/Me4zGsCyjB</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1331793423762923521?ref_src=twsrc%5Etfw">November 26, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. DeRF: Decomposed Radiance Fields

Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li, Kwang Moo Yi, Andrea Tagliasacchi

- retweets: 56, favorites: 37 (11/27/2020 09:54:50)

- links: [abs](https://arxiv.org/abs/2011.12490) | [pdf](https://arxiv.org/pdf/2011.12490)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.GR](https://arxiv.org/list/cs.GR/recent)

With the advent of Neural Radiance Fields (NeRF), neural networks can now render novel views of a 3D scene with quality that fools the human eye. Yet, generating these images is very computationally intensive, limiting their applicability in practical scenarios. In this paper, we propose a technique based on spatial decomposition capable of mitigating this issue. Our key observation is that there are diminishing returns in employing larger (deeper and/or wider) networks. Hence, we propose to spatially decompose a scene and dedicate smaller networks for each decomposed part. When working together, these networks can render the whole scene. This allows us near-constant inference time regardless of the number of decomposed parts. Moreover, we show that a Voronoi spatial decomposition is preferable for this purpose, as it is provably compatible with the Painter's Algorithm for efficient and GPU-friendly rendering. Our experiments show that for real-world scenes, our method provides up to 3x more efficient inference than NeRF (with the same rendering quality), or an improvement of up to 1.0~dB in PSNR (for the same inference cost).

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">DeRF: Decomposed Radiance Fields<br>pdf: <a href="https://t.co/Gxnhcj0CNC">https://t.co/Gxnhcj0CNC</a><br>abs: <a href="https://t.co/DdL6tEslSn">https://t.co/DdL6tEslSn</a> <a href="https://t.co/svwuIqOQ8j">pic.twitter.com/svwuIqOQ8j</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1331781879939338240?ref_src=twsrc%5Etfw">November 26, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. Image Inpainting with Contextual Reconstruction Loss

Yu Zeng, Zhe Lin, Huchuan Lu, Vishal M. Patel

- retweets: 56, favorites: 21 (11/27/2020 09:54:50)

- links: [abs](https://arxiv.org/abs/2011.12836) | [pdf](https://arxiv.org/pdf/2011.12836)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.MM](https://arxiv.org/list/cs.MM/recent)

Convolutional neural networks (CNNs) have been observed to be inefficient in propagating information across distant spatial positions in images. Recent studies in image inpainting attempt to overcome this issue by explicitly searching reference regions throughout the entire image to fill the features from reference regions in the missing regions. This operation can be implemented as contextual attention layer (CA layer) \cite{yu2018generative}, which has been widely used in many deep learning-based methods. However, it brings significant computational overhead as it computes the pair-wise similarity of feature patches at every spatial position. Also, it often fails to find proper reference regions due to the lack of supervision in terms of the correspondence between missing regions and known regions. We propose a novel contextual reconstruction loss (CR loss) to solve these problems. First, a criterion of searching reference region is designed based on minimizing reconstruction and adversarial losses corresponding to the searched reference and the ground-truth image. Second, unlike previous approaches which integrate the computationally heavy patch searching and replacement operation in the inpainting model, CR loss encourages a vanilla CNN to simulate this behavior during training, thus no extra computations are required during inference. Experimental results demonstrate that the proposed inpainting model with the CR loss compares favourably against the state-of-the-arts in terms of quantitative and visual performance. Code is available at \url{https://github.com/zengxianyu/crfill}.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Image Inpainting with Contextual Reconstruction Loss<br>pdf: <a href="https://t.co/7s66J7qEqA">https://t.co/7s66J7qEqA</a><br>abs: <a href="https://t.co/VWrlLJhbPF">https://t.co/VWrlLJhbPF</a><br>github: <a href="https://t.co/em4kohvjda">https://t.co/em4kohvjda</a> <a href="https://t.co/lt3pjPQzCg">pic.twitter.com/lt3pjPQzCg</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1331792690091057156?ref_src=twsrc%5Etfw">November 26, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



