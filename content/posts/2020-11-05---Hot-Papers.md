---
title: Hot Papers 2020-11-05
date: 2020-11-06T09:52:01.Z
template: "post"
draft: false
slug: "hot-papers-2020-11-05"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-11-05"
socialImage: "/media/flying-marine.jpg"

---

# 1. Power of data in quantum machine learning

Hsin-Yuan Huang, Michael Broughton, Masoud Mohseni, Ryan Babbush, Sergio Boixo, Hartmut Neven, Jarrod R. McClean

- retweets: 1020, favorites: 173 (11/06/2020 09:52:01)

- links: [abs](https://arxiv.org/abs/2011.01938) | [pdf](https://arxiv.org/pdf/2011.01938)
- [quant-ph](https://arxiv.org/list/quant-ph/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

The use of quantum computing for machine learning is among the most exciting prospective applications of quantum technologies. At the crux of excitement is the potential for quantum computers to perform some computations exponentially faster than their classical counterparts. However, a machine learning task where some data is provided can be considerably different than more commonly studied computational tasks. In this work, we show that some problems that are classically hard to compute can be predicted easily with classical machines that learn from data. We find that classical machines can often compete or outperform existing quantum models even on data sets generated by quantum evolution, especially at large system sizes. Using rigorous prediction error bounds as a foundation, we develop a methodology for assessing the potential for quantum advantage in prediction on learning tasks. We show how the use of exponentially large quantum Hilbert space in existing quantum models can result in significantly inferior prediction performance compared to classical machines. To circumvent the observed setbacks, we propose an improvement by projecting all quantum states to an approximate classical representation. The projected quantum model provides a simple and rigorous quantum speed-up for a recently proposed learning problem in the fault-tolerant regime. For more near-term quantum models, the projected versions demonstrate a significant prediction advantage over some classical models on engineered data sets in one of the largest numerical tests for gate-based quantum machine learning to date, up to 30 qubits.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">It&#39;s gonna be tough to unravel even a fraction of the cool results in our recent paper in understanding quantum advantage in the presence of data - spearheaded by the amazing <a href="https://twitter.com/MoMoRobertHuang?ref_src=twsrc%5Etfw">@MoMoRobertHuang</a> but I&#39;ll try! (1/n)   <a href="https://t.co/y6yslQlNwK">https://t.co/y6yslQlNwK</a> <a href="https://t.co/qqW9AGXCXN">pic.twitter.com/qqW9AGXCXN</a></p>&mdash; Jarrod McClean (@JarrodMcclean) <a href="https://twitter.com/JarrodMcclean/status/1324218228298575873?ref_src=twsrc%5Etfw">November 5, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">From the practical side - we introduce a new projected quantum kernel approach with demonstrations of large quantum separations up to 30 qubits, utilizing <a href="https://twitter.com/TensorFlow?ref_src=twsrc%5Etfw">@TensorFlow</a>-Quantum for over 1 petaflops of compute power! Hat tip to Michael Broughton on that!(2/n) <a href="https://t.co/y6yslQlNwK">https://t.co/y6yslQlNwK</a> <a href="https://t.co/ljcFmbFFYv">pic.twitter.com/ljcFmbFFYv</a></p>&mdash; Jarrod McClean (@JarrodMcclean) <a href="https://twitter.com/JarrodMcclean/status/1324218231716933632?ref_src=twsrc%5Etfw">November 5, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Power of data in quantum machine learning.<a href="https://twitter.com/hashtag/AI?src=hash&amp;ref_src=twsrc%5Etfw">#AI</a> <a href="https://twitter.com/hashtag/DataScience?src=hash&amp;ref_src=twsrc%5Etfw">#DataScience</a> <a href="https://twitter.com/hashtag/BigData?src=hash&amp;ref_src=twsrc%5Etfw">#BigData</a> <a href="https://twitter.com/hashtag/Analytics?src=hash&amp;ref_src=twsrc%5Etfw">#Analytics</a> <a href="https://twitter.com/hashtag/Cloud?src=hash&amp;ref_src=twsrc%5Etfw">#Cloud</a> <a href="https://twitter.com/hashtag/IoT?src=hash&amp;ref_src=twsrc%5Etfw">#IoT</a> <a href="https://twitter.com/hashtag/Python?src=hash&amp;ref_src=twsrc%5Etfw">#Python</a> <a href="https://twitter.com/hashtag/RStats?src=hash&amp;ref_src=twsrc%5Etfw">#RStats</a> <a href="https://twitter.com/hashtag/IIoT?src=hash&amp;ref_src=twsrc%5Etfw">#IIoT</a> <a href="https://twitter.com/hashtag/JavaScript?src=hash&amp;ref_src=twsrc%5Etfw">#JavaScript</a> <a href="https://twitter.com/hashtag/ReactJS?src=hash&amp;ref_src=twsrc%5Etfw">#ReactJS</a> <a href="https://twitter.com/hashtag/Serverless?src=hash&amp;ref_src=twsrc%5Etfw">#Serverless</a> <a href="https://twitter.com/hashtag/Linux?src=hash&amp;ref_src=twsrc%5Etfw">#Linux</a> <a href="https://twitter.com/hashtag/100DaysOfCode?src=hash&amp;ref_src=twsrc%5Etfw">#100DaysOfCode</a> <a href="https://twitter.com/hashtag/Developers?src=hash&amp;ref_src=twsrc%5Etfw">#Developers</a> <a href="https://twitter.com/hashtag/Programming?src=hash&amp;ref_src=twsrc%5Etfw">#Programming</a> <a href="https://twitter.com/hashtag/Coding?src=hash&amp;ref_src=twsrc%5Etfw">#Coding</a> <a href="https://twitter.com/hashtag/MachineLearning?src=hash&amp;ref_src=twsrc%5Etfw">#MachineLearning</a> <a href="https://twitter.com/hashtag/DeepLearning?src=hash&amp;ref_src=twsrc%5Etfw">#DeepLearning</a> <a href="https://twitter.com/hashtag/ArtificialIntelligence?src=hash&amp;ref_src=twsrc%5Etfw">#ArtificialIntelligence</a><a href="https://t.co/qfCmZo10t8">https://t.co/qfCmZo10t8</a> <a href="https://t.co/buL429sSQJ">pic.twitter.com/buL429sSQJ</a></p>&mdash; Marcus Borba (@marcusborba) <a href="https://twitter.com/marcusborba/status/1324489996750016512?ref_src=twsrc%5Etfw">November 5, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Few-Shot Font Generation with Deep Metric Learning

Haruka Aoki, Koki Tsubota, Hikaru Ikuta, Kiyoharu Aizawa

- retweets: 308, favorites: 73 (11/06/2020 09:52:02)

- links: [abs](https://arxiv.org/abs/2011.02206) | [pdf](https://arxiv.org/pdf/2011.02206)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Designing fonts for languages with a large number of characters, such as Japanese and Chinese, is an extremely labor-intensive and time-consuming task. In this study, we addressed the problem of automatically generating Japanese typographic fonts from only a few font samples, where the synthesized glyphs are expected to have coherent characteristics, such as skeletons, contours, and serifs. Existing methods often fail to generate fine glyph images when the number of style reference glyphs is extremely limited. Herein, we proposed a simple but powerful framework for extracting better style features. This framework introduces deep metric learning to style encoders. We performed experiments using black-and-white and shape-distinctive font datasets and demonstrated the effectiveness of the proposed framework.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Few-Shot Font Generation with Deep Metric Learning<br>pdf: <a href="https://t.co/toJBQVvfMe">https://t.co/toJBQVvfMe</a><br>abs: <a href="https://t.co/AQ5ipQ7Bab">https://t.co/AQ5ipQ7Bab</a> <a href="https://t.co/x5M8U0rtgY">pic.twitter.com/x5M8U0rtgY</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1324175130117967872?ref_src=twsrc%5Etfw">November 5, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Rearrangement: A Challenge for Embodied AI

Dhruv Batra, Angel X. Chang, Sonia Chernova, Andrew J. Davison, Jia Deng, Vladlen Koltun, Sergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh Mottaghi, Manolis Savva, Hao Su

- retweets: 231, favorites: 101 (11/06/2020 09:52:02)

- links: [abs](https://arxiv.org/abs/2011.01975) | [pdf](https://arxiv.org/pdf/2011.01975)
- [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.RO](https://arxiv.org/list/cs.RO/recent)

We describe a framework for research and evaluation in Embodied AI. Our proposal is based on a canonical task: Rearrangement. A standard task can focus the development of new techniques and serve as a source of trained models that can be transferred to other settings. In the rearrangement task, the goal is to bring a given physical environment into a specified state. The goal state can be specified by object poses, by images, by a description in language, or by letting the agent experience the environment in the goal state. We characterize rearrangement scenarios along different axes and describe metrics for benchmarking rearrangement performance. To facilitate research and exploration, we present experimental testbeds of rearrangement scenarios in four different simulation environments. We anticipate that other datasets will be released and new simulation platforms will be built to support training of rearrangement agents and their deployment on physical systems.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Many tasks in robotics/AI involve *scene rearrangement*. How do we define a goal state or measure success? New environments with realistic simulation of perception and physics enable systematic research. We discuss in this major new collaborative report!<a href="https://t.co/DAqn5O5w6T">https://t.co/DAqn5O5w6T</a> <a href="https://t.co/7zDF73o14n">pic.twitter.com/7zDF73o14n</a></p>&mdash; Andrew Davison (@AjdDavison) <a href="https://twitter.com/AjdDavison/status/1324394320628228096?ref_src=twsrc%5Etfw">November 5, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Rearrangement: a new challenge for the Embodied AI community. This report is the result of 13 two-hour discussions over five months.<br><br>Paper: <a href="https://t.co/3XFht8K8l9">https://t.co/3XFht8K8l9</a> <a href="https://t.co/kCNdRJrtt8">pic.twitter.com/kCNdRJrtt8</a></p>&mdash; Roozbeh Mottaghi (@RoozbehMottaghi) <a href="https://twitter.com/RoozbehMottaghi/status/1324393907514273792?ref_src=twsrc%5Etfw">November 5, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. Deep Image Compositing

He Zhang, Jianming Zhang, Federico Perazzi, Zhe Lin, Vishal M. Patel

- retweets: 196, favorites: 79 (11/06/2020 09:52:03)

- links: [abs](https://arxiv.org/abs/2011.02146) | [pdf](https://arxiv.org/pdf/2011.02146)
- [cs.CV](https://arxiv.org/list/cs.CV/recent)

Image compositing is a task of combining regions from different images to compose a new image. A common use case is background replacement of portrait images. To obtain high quality composites, professionals typically manually perform multiple editing steps such as segmentation, matting and foreground color decontamination, which is very time consuming even with sophisticated photo editing tools. In this paper, we propose a new method which can automatically generate high-quality image compositing without any user input. Our method can be trained end-to-end to optimize exploitation of contextual and color information of both foreground and background images, where the compositing quality is considered in the optimization. Specifically, inspired by Laplacian pyramid blending, a dense-connected multi-stream fusion network is proposed to effectively fuse the information from the foreground and background images at different scales. In addition, we introduce a self-taught strategy to progressively train from easy to complex cases to mitigate the lack of training data. Experiments show that the proposed method can automatically generate high-quality composites and outperforms existing methods both qualitatively and quantitatively.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Deep Image Compositing<br>pdf: <a href="https://t.co/CBzDbgpda7">https://t.co/CBzDbgpda7</a><br>abs: <a href="https://t.co/NxUhmeJPZk">https://t.co/NxUhmeJPZk</a> <a href="https://t.co/pyUVTQsEuQ">pic.twitter.com/pyUVTQsEuQ</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1324177235289530372?ref_src=twsrc%5Etfw">November 5, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Ideal theory in AI ethics

Daniel Estrada

- retweets: 110, favorites: 56 (11/06/2020 09:52:03)

- links: [abs](https://arxiv.org/abs/2011.02279) | [pdf](https://arxiv.org/pdf/2011.02279)
- [cs.CY](https://arxiv.org/list/cs.CY/recent)

This paper addresses the ways AI ethics research operates on an ideology of ideal theory, in the sense discussed by Mills (2005) and recently applied to AI ethics by Fazelpour \& Lipton (2020). I address the structural and methodological conditions that attract AI ethics researchers to ideal theorizing, and the consequences this approach has for the quality and future of our research community. Finally, I discuss the possibilities for a nonideal future in AI ethics.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">My paper &quot;Ideal Theory in AI Ethics&quot; was accepted to the <a href="https://twitter.com/NeurIPSConf?ref_src=twsrc%5Etfw">@NeurIPSConf</a> Workshop on Navigating the Broader Impacts of AI Research! <a href="https://t.co/9AFZ5PRl49">https://t.co/9AFZ5PRl49</a><br><br>Feedback appreciated <a href="https://t.co/2I18sQplri">https://t.co/2I18sQplri</a> <a href="https://t.co/5KLcoDI0Ne">pic.twitter.com/5KLcoDI0Ne</a></p>&mdash; eRIPsa (@eripsa) <a href="https://twitter.com/eripsa/status/1324172286283448320?ref_src=twsrc%5Etfw">November 5, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. Surgical Data Science -- from Concepts to Clinical Translation

Lena Maier-Hein, Matthias Eisenmann, Duygu Sarikaya, Keno März, Toby Collins, Anand Malpani, Johannes Fallert, Hubertus Feussner, Stamatia Giannarou, Pietro Mascagni, Hirenkumar Nakawala, Adrian Park, Carla Pugh, Danail Stoyanov, Swaroop S. Vedula, Beat Peter Müller, Kevin Cleary, Gabor Fichtinger, Germain Forestier, Bernard Gibaud, Teodor Grantcharov, Makoto Hashizume, Hannes Kenngott, Ron Kikinis, Lars Mündermann, Nassir Navab, Sinan Onogur, Raphael Sznitman, Russell Taylor, Minu Dietlinde Tizabi, Martin Wagner, Gregory D. Hager, Thomas Neumuth, Nicolas Padoy, Pierre Jannin, Stefanie Speidel

- retweets: 76, favorites: 32 (11/06/2020 09:52:03)

- links: [abs](https://arxiv.org/abs/2011.02284) | [pdf](https://arxiv.org/pdf/2011.02284)
- [cs.CY](https://arxiv.org/list/cs.CY/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [eess.IV](https://arxiv.org/list/eess.IV/recent)

Recent developments in data science in general and machine learning in particular have transformed the way experts envision the future of surgery. Surgical data science is a new research field that aims to improve the quality of interventional healthcare through the capture, organization, analysis and modeling of data. While an increasing number of data-driven approaches and clinical applications have been studied in the fields of radiological and clinical data science, translational success stories are still lacking in surgery. In this publication, we shed light on the underlying reasons and provide a roadmap for future advances in the field. Based on an international workshop involving leading researchers in the field of surgical data science, we review current practice, key achievements and initiatives as well as available standards and tools for a number of topics relevant to the field, namely (1) technical infrastructure for data acquisition, storage and access in the presence of regulatory constraints, (2) data annotation and sharing and (3) data analytics. Drawing from this extensive review, we present current challenges for technology development and (4) describe a roadmap for faster clinical translation and exploitation of the full potential of surgical data science.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">31 institutions, one consensus paper. How can we convert the potential of <a href="https://twitter.com/hashtag/SurgicalDataScience?src=hash&amp;ref_src=twsrc%5Etfw">#SurgicalDataScience</a> into patient benefit?<a href="https://t.co/M0yCjmXC35">https://t.co/M0yCjmXC35</a><br><br>Some highlights:<br><br>1/ <a href="https://t.co/Tkb6CMZ5Wg">pic.twitter.com/Tkb6CMZ5Wg</a></p>&mdash; CAMI Lab (DKFZ) (@DKFZ_CAMI_lab) <a href="https://twitter.com/DKFZ_CAMI_lab/status/1324297773739581442?ref_src=twsrc%5Etfw">November 5, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 7. PheMT: A Phenomenon-wise Dataset for Machine Translation Robustness on  User-Generated Contents

Ryo Fujii, Masato Mita, Kaori Abe, Kazuaki Hanawa, Makoto Morishita, Jun Suzuki, Kentaro Inui

- retweets: 57, favorites: 16 (11/06/2020 09:52:03)

- links: [abs](https://arxiv.org/abs/2011.02121) | [pdf](https://arxiv.org/pdf/2011.02121)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

Neural Machine Translation (NMT) has shown drastic improvement in its quality when translating clean input, such as text from the news domain. However, existing studies suggest that NMT still struggles with certain kinds of input with considerable noise, such as User-Generated Contents (UGC) on the Internet. To make better use of NMT for cross-cultural communication, one of the most promising directions is to develop a model that correctly handles these expressions. Though its importance has been recognized, it is still not clear as to what creates the great gap in performance between the translation of clean input and that of UGC. To answer the question, we present a new dataset, PheMT, for evaluating the robustness of MT systems against specific linguistic phenomena in Japanese-English translation. Our experiments with the created dataset revealed that not only our in-house models but even widely used off-the-shelf systems are greatly disturbed by the presence of certain phenomena.




# 8. Learning 3D Dynamic Scene Representations for Robot Manipulation

Zhenjia Xu, Zhanpeng He, Jiajun Wu, Shuran Song

- retweets: 42, favorites: 26 (11/06/2020 09:52:03)

- links: [abs](https://arxiv.org/abs/2011.01968) | [pdf](https://arxiv.org/pdf/2011.01968)
- [cs.RO](https://arxiv.org/list/cs.RO/recent) | [cs.CV](https://arxiv.org/list/cs.CV/recent)

3D scene representation for robot manipulation should capture three key object properties: permanency -- objects that become occluded over time continue to exist; amodal completeness -- objects have 3D occupancy, even if only partial observations are available; spatiotemporal continuity -- the movement of each object is continuous over space and time. In this paper, we introduce 3D Dynamic Scene Representation (DSR), a 3D volumetric scene representation that simultaneously discovers, tracks, reconstructs objects, and predicts their dynamics while capturing all three properties. We further propose DSR-Net, which learns to aggregate visual observations over multiple interactions to gradually build and refine DSR. Our model achieves state-of-the-art performance in modeling 3D scene dynamics with DSR on both simulated and real data. Combined with model predictive control, DSR-Net enables accurate planning in downstream robotic manipulation tasks such as planar pushing. Video is available at https://youtu.be/GQjYG3nQJ80.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Learning 3D Dynamic Scene Representations for Robot Manipulation<br>pdf: <a href="https://t.co/vpEHtYm6SN">https://t.co/vpEHtYm6SN</a><br>abs: <a href="https://t.co/N1FOoAsOp0">https://t.co/N1FOoAsOp0</a><br>project page: <a href="https://t.co/VottzHhWjN">https://t.co/VottzHhWjN</a> <a href="https://t.co/6s4Gx6MBOr">pic.twitter.com/6s4Gx6MBOr</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1324185626850283521?ref_src=twsrc%5Etfw">November 5, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 9. Debiasing classifiers: is reality at variance with expectation?

Ashrya Agrawal, Florian Pfisterer, Bernd Bischl, Jiahao Chen, Srijan Sood, Sameena Shah, Francois Buet-Golfouse, Bilal A Mateen, Sebastian Vollmer

- retweets: 29, favorites: 32 (11/06/2020 09:52:03)

- links: [abs](https://arxiv.org/abs/2011.02407) | [pdf](https://arxiv.org/pdf/2011.02407)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.AI](https://arxiv.org/list/cs.AI/recent) | [cs.CY](https://arxiv.org/list/cs.CY/recent) | [econ.EM](https://arxiv.org/list/econ.EM/recent)

Many methods for debiasing classifiers have been proposed, but their effectiveness in practice remains unclear. We evaluate the performance of pre-processing and post-processing debiasers for improving fairness in random forest classifiers trained on a suite of data sets. Specifically, we study how these debiasers generalize with respect to both out-of-sample test error for computing fairness -- performance and fairness -- fairness trade-offs, and on the change in other fairness metrics that were not explicitly optimised. Our results demonstrate that out-of-sample performance on fairness and performance can vary substantially and unexpectedly. Moreover, the variance in estimation arises from class imbalances with respect to both the outcome and the protected classes. Our results highlight the importance of evaluating out-of-sample performance in practical usage.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">How well does bias remediation work in practice for ML models? New preprint featuring <a href="https://twitter.com/hashtag/JuliaLang?src=hash&amp;ref_src=twsrc%5Etfw">#JuliaLang</a> GSoC student <a href="https://twitter.com/Ashrya3?ref_src=twsrc%5Etfw">@Ashrya3</a> and my fantastic collaborators! &quot;Debiasing classifiers: is reality at variance with expectation?&quot; arXiv <a href="https://t.co/KlRvgZIXor">https://t.co/KlRvgZIXor</a> SSRN <a href="https://t.co/C04fpwDiSG">https://t.co/C04fpwDiSG</a> 🧵👇 <a href="https://t.co/VwhFJfuuDb">pic.twitter.com/VwhFJfuuDb</a></p>&mdash; 🍂 🦃🥧 Jiahao Chen 陳家豪 @ 🏡🗽 (@acidflask) <a href="https://twitter.com/acidflask/status/1324376277755797505?ref_src=twsrc%5Etfw">November 5, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



