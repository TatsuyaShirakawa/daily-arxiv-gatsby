---
title: Hot Papers 2020-09-11
date: 2020-09-12T09:21:47.Z
template: "post"
draft: false
slug: "hot-papers-2020-09-11"
category: "arXiv"
tags:
  - "arXiv"
  - "Twitter"
  - "Machine Learning"
  - "Computer Science"
description: "Hot papers 2020-09-11"
socialImage: "/media/flying-marine.jpg"

---

# 1. Modern Methods for Text Generation

Dimas Munoz Montesinos

- retweets: 19, favorites: 276 (09/12/2020 09:21:47)

- links: [abs](https://arxiv.org/abs/2009.04968) | [pdf](https://arxiv.org/pdf/2009.04968)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent)

Synthetic text generation is challenging and has limited success. Recently, a new architecture, called Transformers, allow machine learning models to understand better sequential data, such as translation or summarization. BERT and GPT-2, using Transformers in their cores, have shown a great performance in tasks such as text classification, translation and NLI tasks. In this article, we analyse both algorithms and compare their output quality in text generation tasks.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Here is a nice summary report of modern methods for text generation. <br><br>Very easy to read.<a href="https://t.co/Qf2h33Yzf7">https://t.co/Qf2h33Yzf7</a> <a href="https://t.co/cG1QJoEYy2">pic.twitter.com/cG1QJoEYy2</a></p>&mdash; elvis (@omarsar0) <a href="https://twitter.com/omarsar0/status/1304360284551696384?ref_src=twsrc%5Etfw">September 11, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 2. Understanding the Role of Individual Units in a Deep Neural Network

David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, Antonio Torralba

- retweets: 32, favorites: 101 (09/12/2020 09:21:48)

- links: [abs](https://arxiv.org/abs/2009.05041) | [pdf](https://arxiv.org/pdf/2009.05041)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.NE](https://arxiv.org/list/cs.NE/recent)

Deep neural networks excel at finding hierarchical representations that solve complex tasks over large data sets. How can we humans understand these learned representations? In this work, we present network dissection, an analytic framework to systematically identify the semantics of individual hidden units within image classification and image generation networks. First, we analyze a convolutional neural network (CNN) trained on scene classification and discover units that match a diverse set of object concepts. We find evidence that the network has learned many object classes that play crucial roles in classifying scene classes. Second, we use a similar analytic method to analyze a generative adversarial network (GAN) model trained to generate scenes. By analyzing changes made when small sets of units are activated or deactivated, we find that objects can be added and removed from the output scenes while adapting to the context. Finally, we apply our analytic framework to understanding adversarial attacks and to semantic image editing.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Understanding the Role of Individual Units in a Deep Neural Network<br>pdf: <a href="https://t.co/BZl1kDRfUd">https://t.co/BZl1kDRfUd</a><br>abs: <a href="https://t.co/mntNScqLfl">https://t.co/mntNScqLfl</a><br>project page: <a href="https://t.co/G2VP12WfvT">https://t.co/G2VP12WfvT</a><br>github: <a href="https://t.co/GIfyBPVzTz">https://t.co/GIfyBPVzTz</a> <a href="https://t.co/wMVcBlcUHl">pic.twitter.com/wMVcBlcUHl</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1304219490843066369?ref_src=twsrc%5Etfw">September 11, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 3. Brain2Word: Decoding Brain Activity for Language Generation

Nicolas Affolter, Beni Egressy, Damian Pascual, Roger Wattenhofer

- retweets: 24, favorites: 106 (09/12/2020 09:21:48)

- links: [abs](https://arxiv.org/abs/2009.04765) | [pdf](https://arxiv.org/pdf/2009.04765)
- [cs.CL](https://arxiv.org/list/cs.CL/recent) | [cs.LG](https://arxiv.org/list/cs.LG/recent) | [q-bio.NC](https://arxiv.org/list/q-bio.NC/recent)

Brain decoding, understood as the process of mapping brain activities to the stimuli that generated them, has been an active research area in the last years. In the case of language stimuli, recent studies have shown that it is possible to decode fMRI scans into an embedding of the word a subject is reading. However, such word embeddings are designed for natural language processing tasks rather than for brain decoding. Therefore, they limit our ability to recover the precise stimulus. In this work, we propose to directly classify an fMRI scan, mapping it to the corresponding word within a fixed vocabulary. Unlike existing work, we evaluate on scans from previously unseen subjects. We argue that this is a more realistic setup and we present a model that can decode fMRI data from unseen subjects. Our model achieves 5.22% Top-1 and 13.59% Top-5 accuracy in this challenging task, significantly outperforming all the considered competitive baselines. Furthermore, we use the decoded words to guide language generation with the GPT-2 model. This way, we advance the quest for a system that translates brain activities into coherent text.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Brain2Word: Decoding Brain Activity for Language Generation<br>pdf: <a href="https://t.co/Fh7a0QpeZG">https://t.co/Fh7a0QpeZG</a><br>abs: <a href="https://t.co/Xbc8XhZSTm">https://t.co/Xbc8XhZSTm</a> <a href="https://t.co/NhJKvkjNy2">pic.twitter.com/NhJKvkjNy2</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1304220472633102339?ref_src=twsrc%5Etfw">September 11, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 4. The Grievance Dictionary: Understanding Threatening Language Use

Isabelle van der Vegt, Maximilian Mozes, Bennett Kleinberg, Paul Gill

- retweets: 27, favorites: 83 (09/12/2020 09:21:49)

- links: [abs](https://arxiv.org/abs/2009.04798) | [pdf](https://arxiv.org/pdf/2009.04798)
- [cs.CL](https://arxiv.org/list/cs.CL/recent)

This paper introduces the Grievance Dictionary, a psycholinguistic dictionary which can be used to automatically understand language use in the context of grievance-fuelled violence threat assessment. We describe the development the dictionary, which was informed by suggestions from experienced threat assessment practitioners. These suggestions and subsequent human and computational word list generation resulted in a dictionary of 20,502 words annotated by 2,318 participants. The dictionary was validated by applying it to texts written by violent and non-violent individuals, showing strong evidence for a difference between populations in several dictionary categories. Further classification tasks showed promising performance, but future improvements are still needed. Finally, we provide instructions and suggestions for the use of the Grievance Dictionary by security professionals and (violence) researchers.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Our pre-print &quot;The Grievance Dictionary: Understanding Threatening Language Use&quot; is now available: <a href="https://t.co/AkSaEoj59P">https://t.co/AkSaEoj59P</a> We developed this open source tool for use in automated linguistic analysis, specifically in the violence and extremism (research) domain. cc: <a href="https://twitter.com/Grievance_ERC?ref_src=twsrc%5Etfw">@Grievance_ERC</a></p>&mdash; Isabelle (@Isabellevdv) <a href="https://twitter.com/Isabellevdv/status/1304326166573088769?ref_src=twsrc%5Etfw">September 11, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Our latest, led by <a href="https://twitter.com/Isabellevdv?ref_src=twsrc%5Etfw">@Isabellevdv</a> , is now out on pre-print - The Grievance Dictionary: Understanding Threatening Language Use - <a href="https://t.co/c4F0EcfO0R">https://t.co/c4F0EcfO0R</a> - Develops a psycholinguistic dictionary to automatically understand language use in the context of grievance-fuelled violence</p>&mdash; Paul Gill (@paulgill_ucl) <a href="https://twitter.com/paulgill_ucl/status/1304340979407876097?ref_src=twsrc%5Etfw">September 11, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 5. Pay Attention when Required

Swetha Mandava, Szymon Migacz, Alex Fit Florea

- retweets: 10, favorites: 54 (09/12/2020 09:21:49)

- links: [abs](https://arxiv.org/abs/2009.04534) | [pdf](https://arxiv.org/pdf/2009.04534)
- [cs.LG](https://arxiv.org/list/cs.LG/recent) | [cs.CL](https://arxiv.org/list/cs.CL/recent)

Transformer-based models consist of interleaved feed-forward blocks - that capture content meaning, and relatively more expensive self-attention blocks - that capture context meaning. In this paper, we explored trade-offs and ordering of the blocks to improve upon the current Transformer architecture and proposed PAR Transformer. It needs 35% lower compute time than Transformer-XL achieved by replacing ~63% of the self-attention blocks with feed-forward blocks, and retains the perplexity on WikiText-103 language modelling benchmark. We further validated our results on text8 and enwiki8 datasets, as well as on the BERT model.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Pushed my first paper to arXiv ðŸ¤“ <br><br>Introducing PAR Transformer that needs 35% lower compute time than Transformer-XL by replacing ~63% of the self-attention blocks with feed-forward blocks<a href="https://t.co/ZmbzsMrikS">https://t.co/ZmbzsMrikS</a> <a href="https://t.co/4gSfuD6xHA">pic.twitter.com/4gSfuD6xHA</a></p>&mdash; Swetha Mandava (@swethmandava) <a href="https://twitter.com/swethmandava/status/1304242194446835712?ref_src=twsrc%5Etfw">September 11, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




# 6. Fully Convolutional Graph Neural Networks for Parametric Virtual Try-On

Raquel Vidaurre, Igor Santesteban, Elena Garces, Dan Casas

- retweets: 13, favorites: 51 (09/12/2020 09:21:49)

- links: [abs](https://arxiv.org/abs/2009.04592) | [pdf](https://arxiv.org/pdf/2009.04592)
- [cs.CV](https://arxiv.org/list/cs.CV/recent) | [cs.GR](https://arxiv.org/list/cs.GR/recent)

We present a learning-based approach for virtual try-on applications based on a fully convolutional graph neural network. In contrast to existing data-driven models, which are trained for a specific garment or mesh topology, our fully convolutional model can cope with a large family of garments, represented as parametric predefined 2D panels with arbitrary mesh topology, including long dresses, shirts, and tight tops. Under the hood, our novel geometric deep learning approach learns to drape 3D garments by decoupling the three different sources of deformations that condition the fit of clothing: garment type, target body shape, and material. Specifically, we first learn a regressor that predicts the 3D drape of the input parametric garment when worn by a mean body shape. Then, after a mesh topology optimization step where we generate a sufficient level of detail for the input garment type, we further deform the mesh to reproduce deformations caused by the target body shape. Finally, we predict fine-scale details such as wrinkles that depend mostly on the garment material. We qualitatively and quantitatively demonstrate that our fully convolutional approach outperforms existing methods in terms of generalization capabilities and memory requirements, and therefore it opens the door to more general learning-based models for virtual try-on applications.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Fully Convolutional Graph Neural Networks for Parametric Virtual Try-On<br>pdf: <a href="https://t.co/mzDg8j5Z87">https://t.co/mzDg8j5Z87</a><br>abs: <a href="https://t.co/eQKu0NG32C">https://t.co/eQKu0NG32C</a><br>project page: <a href="https://t.co/TzeqmjYWEo">https://t.co/TzeqmjYWEo</a> <a href="https://t.co/FQU95vsWZl">pic.twitter.com/FQU95vsWZl</a></p>&mdash; AK (@ak92501) <a href="https://twitter.com/ak92501/status/1304231218431623172?ref_src=twsrc%5Etfw">September 11, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



